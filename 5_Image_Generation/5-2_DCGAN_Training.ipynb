{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-2 DCGAN の損失関数・学習・生成の実装\n",
    "\n",
    "## DCGAN の損失関数\n",
    "DCGAN の損失関数は通常のクラス分類と同様に定義できる．（正確には Jensen-Shannon ダイバージェンスの話になるがここでは省略）  \n",
    "入力される画像データが $x$ のとき，D の出力は $y = D(x)$ だが，出力 は前節で実装した D の出力にシグモイド関数がかかって，値が0から1に変換されているものとする．\n",
    "正しいラベルは G が生成した偽データをラベル0，教師データをラベル1とすると D の出力が正答かどうかは $y^l(1-y)^{1-l}$ で表される．\n",
    "$y^l(1-y)^{1-l}$ は正解ラベル $l$ と予測出力 $y$ の値が同じなら1，異なる間違った予測の場合は0になる．\n",
    "実際には $y$ は0から1の間の値をとり極端に0や1にならないため，$y^l(1-y)^{1-l}$ も0から1の値になる．\n",
    "\n",
    "この判定がミニバッチのデータ数 M 個分あるため，その同時確率は次で与えられる．\n",
    "$$\n",
    "    \\prod_{i = 1}^{M} y_i^{l_i}(1-y_i)^{1-l_i}\n",
    "$$\n",
    "これの対数を取ると次のようになる．\n",
    "$$\n",
    "    \\sum_{i = 1}^{M} [l_i log y_i + (1 - l_i) log (1 - y_i)]\n",
    "$$\n",
    "D はこの式（対数尤度）が最大となるように学習する．\n",
    "最大化は実装が難しいので $-1$ を掛けて最小化問題に帰着する．  \n",
    "この損失関数は torch.nn.BCEWithLogitsLoss() を使用して簡単に実装できる．（BCE は Binary Cross Entropy の略で2値分類の誤差関数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=20, image_size=64):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(nn.ConvTranspose2d(z_dim, image_size * 8, kernel_size=4, stride=1), \n",
    "                                    nn.BatchNorm2d(image_size * 8), \n",
    "                                    nn.ReLU(inplace=True))\n",
    "        self.layer2 = nn.Sequential(nn.ConvTranspose2d(image_size * 8, image_size * 4, kernel_size=4, stride=2, padding=1), \n",
    "                                    nn.BatchNorm2d(image_size * 4), \n",
    "                                    nn.ReLU(inplace=True))\n",
    "        self.layer3 = nn.Sequential(nn.ConvTranspose2d(image_size * 4, image_size * 2, kernel_size=4, stride=2, padding=1), \n",
    "                                    nn.BatchNorm2d(image_size * 2), \n",
    "                                    nn.ReLU(inplace=True))\n",
    "        self.layer4 = nn.Sequential(nn.ConvTranspose2d(image_size * 2, image_size, kernel_size=4, stride=2, padding=1), \n",
    "                                    nn.BatchNorm2d(image_size), \n",
    "                                    nn.ReLU(inplace=True))\n",
    "        self.last = nn.Sequential(nn.ConvTranspose2d(image_size, 1, kernel_size=4, stride=2, padding=1), \n",
    "                                  nn.Tanh())\n",
    "        \n",
    "    \n",
    "    def forward(self, z):\n",
    "        out = self.layer1(z)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.last(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, z_dim=20, image_size=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(1, image_size, kernel_size=4, stride=2, padding=1), \n",
    "                                    nn.LeakyReLU(0.1, inplace=True))\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(image_size, image_size * 2, kernel_size=4, stride=2, padding=1), \n",
    "                                    nn.LeakyReLU(0.1, inplace=True))\n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(image_size * 2, image_size * 4, kernel_size=4, stride=2, padding=1), \n",
    "                                    nn.LeakyReLU(0.1, inplace=True))\n",
    "        self.layer4 = nn.Sequential(nn.Conv2d(image_size * 4, image_size * 8, kernel_size=4, stride=2, padding=1), \n",
    "                                    nn.LeakyReLU(0.1, inplace=True))\n",
    "        self.last = nn.Conv2d(image_size * 8, 1, kernel_size=4, stride=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.last(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "G = Generator()\n",
    "D = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b4df679f9e21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# 真の画像を判定\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0md_out_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# 偽の画像を生成して判定\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "# D の誤差関数のイメージ実装\n",
    "# maximize log(D(x)) + log(1 - D(G(z)))\n",
    "# この段階では x が未定義なため動作はエラーになる\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 正解ラベルを作成\n",
    "mini_batch_size = 2\n",
    "label_real = torch.full((mini_batch_size,), 1)\n",
    "\n",
    "# 偽ラベルを作成\n",
    "label_fake = torch.full((mini_batch_size,), 0)\n",
    "\n",
    "# 誤差関数を定義\n",
    "criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "# 真の画像を判定\n",
    "d_out_real = D(x)\n",
    "\n",
    "# 偽の画像を生成して判定\n",
    "input_z = torch.randn(mini_batch_size, 20)\n",
    "input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\n",
    "fake_images = G(input_z)\n",
    "d_out_fake = D(fake_images)\n",
    "\n",
    "# 誤差を計算\n",
    "d_loss_real = criterion(d_out_real.view(-1), label_real)\n",
    "d_loss_fake = criterion(d_out_fake.view(-1), label_fake)\n",
    "d_loss = d_loss_real + d_loss_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G は D を騙したいので G で生成した画像に対して D の判定が失敗方向になれば良い．\n",
    "つまり D は G から生成された画像を正確に判定するために次式を最小化しようとしている．\n",
    "$$\n",
    "    -\\sum_{i = 1}^{M} [l_i log y_i + (1 - l_i) log (1 - y_i)]\n",
    "$$\n",
    "よって，G の損失関数は次のようになる．\n",
    "$$\n",
    "    \\sum_{i = 1}^{M} [l_i log y_i + (1 - l_i) log (1 - y_i)]\n",
    "$$\n",
    "ここで $l_i$ は偽画像は0なので第1項が消え，さらに $ 1 - l_i = 1, y = D(x)$ で，さらに G は $z_i$ から画像を生成するので，G のミニバッチでの損失関数は次で与えられる．\n",
    "$$\n",
    "    \\sum_{i = 1}^{M} log\\left(1-D(G(z_i))\\right)\n",
    "$$\n",
    "\n",
    "しかし，上式で与えられる損失関数では学習がうまくいかない．\n",
    "これは，初期段階の G が生成する画像は教師データと大きくかけ離れているため，未熟な D でもある程度正確な判定ができてしまうため，損失がほとんど0になってしまい学習が進めなくなるためである．  \n",
    "そこで，$D(G(z_i))$が1と判定されれば良いと考え，DCGAN では G の損失関数を次のように定義する．\n",
    "$$\n",
    "    -\\sum_{i = 1}^{M} log D(G(z_i))\n",
    "$$\n",
    "こうすることで，G がうまく D を騙せれば log の中身は1となり損失関数の値は0となる．\n",
    "一方で D に見抜かれると log の中身は0から1の値となり損失関数は正の大きな値をとり，学習が進むようになる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G の誤差関数のイメージ実装\n",
    "# maximize log(D(G(z)))\n",
    "# x が未定義なので動作はエラーになります\n",
    "\n",
    "# 偽の画像を生成して判定\n",
    "input_z = torch.randn(mini_batch_size, 20)\n",
    "input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\n",
    "fake_images = G(input_z)\n",
    "d_out_fake = D(fake_images)\n",
    "\n",
    "# 誤差を計算\n",
    "g_loss = criterion(d_out_fake.view(-1), label_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで，G の損失の計算に D での判定が入っている点に注目する．\n",
    "損失をバックプロパゲーションしたとき，まず D を通ってから G に到達する．\n",
    "もし，D の活性化関数に ReLU 関数を使うと，ReLU への入力が負であった場合に出力が0となってしまい，バックプロパゲーションもそこで止まってしまう．\n",
    "これでは G の学習が進まないため D の活性化関数に LeakyReLU を使って G まで損失がでんぱするようにしている．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
