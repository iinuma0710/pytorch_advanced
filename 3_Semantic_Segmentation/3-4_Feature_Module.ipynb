{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-4 Feature モジュールの解説と実装\n",
    "\n",
    "## Feature モジュールのサブネットワークの構成\n",
    "Feature モジュールの構成を以下に示す．\n",
    "\n",
    "<img src=\"../image/p149.png\">\n",
    "\n",
    "1つの FeatureMap_convolution と，それぞれ2つの residualBlockPSP，dilated 版 residualBlockPSP から構成される．\n",
    "4つ目のサブネットワークから出力されるテンソルは，AuxLoss モジュールに渡される．\n",
    "学習時には Decoder と AuxLoss 両方の損失値を用いて，より効率的にパラメータを学習させることが出来る．\n",
    "\n",
    "## サブネットワーク FeatureMap_convolution\n",
    "FeatureMap_Convolution の詳細なネットワークを次に示す．\n",
    "\n",
    "<img src=\"../image/p150.png\">\n",
    "\n",
    "入力画像は事前処理済みの 3 x 475 x 475，出力は 128 x 119 x119 である．\n",
    "ここでは，単純に畳み込み，バッチノーマライゼーション，マックスプーリングで画像の特徴量を抽出している．\n",
    "\n",
    "## FeatureMap_convolution の実装\n",
    "畳み込み層，バッチノーマライゼーション，ReLU をセットにした conv2dBatchNormRelu クラスを作成する．\n",
    "nn.ReLU(inplace=True) としているが，メモリ効率を重視して ReLU への入力をメモリに保存せず，そのまま出力を計算させている．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class conv2dDBatchNormRelu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, bias):\n",
    "        super(conv2DBatchNormRelu, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, bias=bias)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batchnorm(x)\n",
    "        outputs = self.relu(x)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "\n",
    "class FeatureMap_convolution(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureMap_convolution, self).__init__()\n",
    "        \n",
    "        # 畳み込み層1\n",
    "        in_channels, out_channels, kernel_size, stride, padding, dilation, bias = 3, 64, 3, 2, 1, 1, False\n",
    "        self.cbnr_1 = conv2DBatchNormRelu(in_channels, out_channels, kernel_size, stride, padding, dilation, bias)\n",
    "        # 畳み込み層2\n",
    "        in_channels, out_channels, kernel_size, stride, padding, dilation, bias = 64, 64, 3, 1, 1, 1, False\n",
    "        self.cbnr_2 = conv2DBatchNormRelu(in_channels, out_channels, kernel_size, stride, padding, dilation, bias)\n",
    "        # 畳み込み層3\n",
    "        in_channels, out_channels, kernel_size, stride, padding, dilation, bias = 64, 128, 3, 1, 1, 1, False\n",
    "        self.cbnr_3 = conv2DBatchNormRelu(in_channels, out_channels, kernel_size, stride, padding, dilation, bias)\n",
    "        \n",
    "        # Max Pooling 層\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cbnr_1(x)\n",
    "        x = self.cbnr_2(x)\n",
    "        x = self.cbnr_3(x)\n",
    "        outputs = self.maxpool(x)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResidualBlockPSP\n",
    "このモジュールでは ResNet の Residual Block 構造を用いる．\n",
    "以下に ResidualBlockPSP の構造を示す．\n",
    "\n",
    "<img src=\"../image/p152.png\">\n",
    "\n",
    "最初に bottleNeckPSP を経て bottleNeckIdentifyPSP を数回繰り返して最終的な出力を計算する．\n",
    "4回繰り返す ResidualBlockPSP では，それぞれ3，4，6，3回ずつ bottleNeckIdentifyPSP を繰り返す．\n",
    "この繰り返し回数は任意に設定できるが，ここでは ResNet50 に合わせている．\n",
    "実装では nn.Module ではなく nn.Sequential を継承している．\n",
    "nn.Sequential を継承することで forward 関数が予め実装されているため，ResidealBlockPSP の forward は明示的に定義しなくても良い．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlockPSP(nn.Sequential):\n",
    "    def __init__(self, n_blocks, in_channels, mid_channels, out_channels, stride, dilation):\n",
    "        super(ResidualBlockPSP, self).__init__()\n",
    "        \n",
    "        # bottleNeckPSP の用意\n",
    "        self.add_module(\"block1\", bottleNeckPSP(in_channels, mid_channels, out_channels, stride, dilation))\n",
    "        # bottleNeckIdentifyPSP の繰り返しの用意\n",
    "        for i in range(n_blocks - 1):\n",
    "            self.add_module(\"block\" + str(i + 2), bottleNeckIdentifyPSP(out_channels, mid_channels, stride, dilation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bottleNeckPSP と bottleNeckIdentifyPSP\n",
    "次に bottleNeckPSP と bottleNeckIdentifyPSP の構造を示す．\n",
    "\n",
    "<img src=\"../image/p154.png\">\n",
    "\n",
    "これらのネットワークでは入力が二股に別れて処理される．\n",
    "下側のルートをスキップ結合，またはショートカットコネクションやバイパスなどと呼ばれる．\n",
    "2種類のブロックの違いはスキップ結合に畳み込み層が入るか入らないかである．  \n",
    "スキップ結合は”劣化問題（degradation）”を緩和する役割がある．  \n",
    "深いネットワークでは浅いネットワークよりも訓練誤差が大きくなることがある．\n",
    "ブロックへの入力を $x$，上側のルートの出力を $F(x)$，ブロック全体の出力を $y$ とすると，$y = x + F(x)$ となる．\n",
    "もし，$F(x)$ が 0 となるとブロックの出力は，そのまま $x$ となる．\n",
    "すると，学習パラメータが全て 0 となっても前段の出力を後段にそのまま伝えることが出来るため，深いネットワークでも劣化問題を避けられる．\n",
    "すなわち，このブロックでは入力 $x$ をそのまま出力し，望ましい出力 $y$ との残差（residual）$y - x = F(x)$ を学ばせている．   \n",
    "また，各ブロックには dilation というパラメータが設定されている．\n",
    "通常の畳み込み層では dilation が1となっている．\n",
    "dilation が1でない値を使用する畳み込み層を Dilated Convolution と呼ぶ．\n",
    "これは，下図のように一定の間隔を空けて畳み込みを適用する手法である．\n",
    "この手法を適用することでより大局的な特徴量を抽出できる．\n",
    "\n",
    "<img src=\"../image/p155.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv2DBatchNorm(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, bias):\n",
    "        super(conv2DBatchNorm, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation ,bias=bias)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        outputs = self.batchnorm(x)\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "class bottleNeckPSP(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, kernel_size, stride, dilation):\n",
    "        super(bottleNeckPSP, self).__init__()\n",
    "        self.cbr_1 = conv2dBatchNormRelu(in_channels, mid_channels, kernel_size=1, \n",
    "                                         stride=1, padding=0, dilation=1, bias=False)\n",
    "        self.cbr_2 = conv2dBatchNormRelu(mid_channels, mid_channels, kernel_size=3, \n",
    "                                         stride=stride, padding=dilation, dilation=dilation, bias=False)\n",
    "        self.cb_3 = conv2DBatchNorm(mid_channels, out_channels, kernel_size=1,\n",
    "                                    stride=1, padding=0, dilation=1, bias=False)\n",
    "        self.cb_residual = conv2DBatchNorm(in_channels, out_channels, kernel_size=1,\n",
    "                                           stride=stride, padding=0, dilation=1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv = self.cb_3(self.cbr_2(self.cbr_1(x)))\n",
    "        residual = self.cb_residual(x)\n",
    "        return self.relu(conv + residual)\n",
    "    \n",
    "\n",
    "class bottleNeckIdentifyPSP(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, stride, dilation):\n",
    "        super(bottleNeckIdentifyPSP, self).__init__()\n",
    "        self.cbr_1 = conv2dBatchNormRelu(in_channels, mid_channels, kernel_size=1, \n",
    "                                         stride=1, padding=0, dilation=1, bias=False)\n",
    "        self.cbr_2 = conv2dBatchNormRelu(mid_channels, mid_channels, kernel_size=3, \n",
    "                                         stride=1, padding=dilation, dilation=dilation, bias=False)\n",
    "        self.cb_3 = conv2DBatchNorm(mid_channels, in_channels, kernel_size=1,\n",
    "                                    stride=1, padding=0, dilation=1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv = self.cb_3(self.cbr_2(self.cbr_1(x)))\n",
    "        residual = x\n",
    "        return self.relu(conv + residual)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
