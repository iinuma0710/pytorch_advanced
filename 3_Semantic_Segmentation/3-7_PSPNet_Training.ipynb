{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-7 ファインチューニングによる学習と検証に実施\n",
    "\n",
    "## データの準備\n",
    "学習済みモデル weights/pspnet50_ADE20K.pth を VOC2012 データセット向けにファインチューニングする．\n",
    "この学習済みモデルは，ADE20K と呼ばれるデータセットで学習したモデルである．\n",
    "\n",
    "## 学習・検証の実装\n",
    "はじめに DataLoader を作成する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataloader import make_datapath_list, DataTransform, VOCDataset\n",
    "\n",
    "# ファイルパスリスト作成\n",
    "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(rootpath=rootpath)\n",
    "\n",
    "# Dataset作成\n",
    "# (RGB)の色の平均値と標準偏差\n",
    "color_mean = (0.485, 0.456, 0.406)\n",
    "color_std = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", \n",
    "                           transform=DataTransform(input_size=475, color_mean=color_mean, color_std=color_std))\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", \n",
    "                         transform=DataTransform(input_size=475, color_mean=color_mean, color_std=color_std))\n",
    "\n",
    "# DataLoader作成\n",
    "batch_size = 8\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 辞書型変数にまとめる\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に ADE20K のネットワークモデルを用意する．\n",
    "出力クラス数は ADE20K データセットに合わせて150になっている点に留意する．\n",
    "この学習済みパラメータをロードし，最終的な出力層を21クラスとするために Decoder と AuxLoss モジュールの分類用の畳み込み層を付け替える．  \n",
    "付け替えた畳み込み層は Xavier の初期値で初期化する．\n",
    "Xavier の初期値は各畳み込み層において，その入力チャネル数が input_n のときに，その畳み込み層の結合パラメータの初期値に 1/sqrt(input_n) を標準偏差としたガウス分布に従う乱数を使用する手法である．\n",
    "この初期化手法は活性関数がシグモイド関数の場合に用いられる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ネットワーク設定完了：学習済みの重みをロードしました\n"
     ]
    }
   ],
   "source": [
    "from utils.pspnet import PSPNet\n",
    "\n",
    "# ファインチューニングでPSPNetを作成\n",
    "# ADE20Kデータセットの学習済みモデルを使用、ADE20Kはクラス数が150です\n",
    "net = PSPNet(n_classes=150)\n",
    "\n",
    "# ADE20K学習済みパラメータをロード\n",
    "state_dict = torch.load(\"./weights/pspnet50_ADE20K.pth\")\n",
    "net.load_state_dict(state_dict)\n",
    "\n",
    "# 分類用の畳み込み層を、出力数21のものにつけかえる\n",
    "n_classes = 21\n",
    "net.decode_feature.classification = nn.Conv2d(in_channels=512, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "net.aux.classification = nn.Conv2d(in_channels=256, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "# 付け替えた畳み込み層を初期化する。活性化関数がシグモイド関数なのでXavierを使用する。\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:  # バイアス項がある場合\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "\n",
    "net.decode_feature.classification.apply(weights_init)\n",
    "net.aux.classification.apply(weights_init)\n",
    "\n",
    "\n",
    "print('ネットワーク設定完了：学習済みの重みをロードしました')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に損失関数を実装する．\n",
    "ここでは，他クラス分類の損失関数であるクロスエントロピー誤差関数を用いる．\n",
    "メインの損失と AuxLoss に係数0.4をかけたものの和をトータルの損失とする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数の設定\n",
    "class PSPLoss(nn.Module):\n",
    "    ''' PSPNetの損失関数のクラス '''\n",
    "\n",
    "    def __init__(self, aux_weight=0.4):\n",
    "        super(PSPLoss, self).__init__()\n",
    "        self.aux_weight = aux_weight  # aux_lossの重み\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        損失関数の計算\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        outputs : PSPNetの出力(tuple)\n",
    "            (output=torch.Size([num_batch, 21, 475, 475]), output_aux=torch.Size([num_batch, 21, 475, 475]))\n",
    "        targets : [num_batch, 475, 4755]\n",
    "            正解のアノテーション情報\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : テンソル\n",
    "            損失の値\n",
    "        \"\"\"\n",
    "\n",
    "        loss = F.cross_entropy(outputs[0], targets, reduction='mean')\n",
    "        loss_aux = F.cross_entropy(outputs[1], targets, reduction='mean')\n",
    "\n",
    "        return loss + self.aux_weight * loss_aux\n",
    "\n",
    "\n",
    "criterion = PSPLoss(aux_weight=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## スケジューラーを利用した epoch ごとの学習率の変更\n",
    "ファインチューニングを行うため，入力層に近いモジュールは学習率を小さく，付け替えた Decoder モジュールと AuxLoss モジュールは学習率を大きく設定する．  \n",
    "ここでは，スケジューラを用いて epoch 数に応じて学習率を変化させる．\n",
    "スケジューラは ```scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch)``` で定義され，lamda_epoch の戻り値を各モジュールの学習率に掛けてそのエポックの学習率とする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファインチューニングなので、学習率は小さく\n",
    "optimizer = optim.SGD([\n",
    "    {'params': net.feature_conv.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_res_1.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_res_2.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_dilated_res_1.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_dilated_res_2.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.pyramid_pooling.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.decode_feature.parameters(), 'lr': 1e-2},\n",
    "    {'params': net.aux.parameters(), 'lr': 1e-2},\n",
    "], momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "\n",
    "# スケジューラーの設定\n",
    "def lambda_epoch(epoch):\n",
    "    max_epoch = 30\n",
    "    return math.pow((1 - epoch / max_epoch), 0.9)\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習を行う関数を定義する．基本的には第2章のものと同じであるが2点異なる箇所がある．  \n",
    "1点目はスケジューラを更新するための scheduler.step() が追加されている点である．  \n",
    "2点目は multiple minibatch を使用している点である．\n",
    "PSPNet ではバッチノーマライゼーションを用いるが，統計的に妥当なノーマライゼーションを行うためには，ある程度大きなバッチサイズが必要となる．\n",
    "しかし，GPU のメモリが限られている場合，あまり大きなバッチを使うことができない．\n",
    "そこで，複数の GPU を使用して大きなバッチサイズでノーマライゼーションを行う Synchronized Multi-GPU Batch Normalization と呼ばれる手法を用いる．これは PyTorch に実装されていないため，サードパーティ製のライブラリから読み込む．  \n",
    "ただし，ここでは複数の GPU を用意できないので，multiple minibatch という手法を用いる．\n",
    "これは，backward の計算を複数回行って，各パラメータに対して複数個のミニバッチから計算した勾配の合計を求め，複数回に1回だけパラメータを更新させる方法である．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs):\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # 画像の枚数\n",
    "    num_train_imgs = len(dataloaders_dict[\"train\"].dataset)\n",
    "    num_val_imgs = len(dataloaders_dict[\"val\"].dataset)\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "\n",
    "    # イテレーションカウンタをセット\n",
    "    iteration = 1\n",
    "    logs = []\n",
    "\n",
    "    # multiple minibatch\n",
    "    batch_multiplier = 3\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # 開始時刻を保存\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "        epoch_train_loss = 0.0  # epochの損失和\n",
    "        epoch_val_loss = 0.0  # epochの損失和\n",
    "\n",
    "        print('-------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "                scheduler.step()  # 最適化schedulerの更新\n",
    "                optimizer.zero_grad()\n",
    "                print('（train）')\n",
    "\n",
    "            else:\n",
    "                if((epoch+1) % 5 == 0):\n",
    "                    net.eval()   # モデルを検証モードに\n",
    "                    print('-------------')\n",
    "                    print('（val）')\n",
    "                else:\n",
    "                    # 検証は5回に1回だけ行う\n",
    "                    continue\n",
    "\n",
    "            # データローダーからminibatchずつ取り出すループ\n",
    "            count = 0  # multiple minibatch\n",
    "            for imges, anno_class_imges in dataloaders_dict[phase]:\n",
    "                # ミニバッチがサイズが1だと、バッチノーマライゼーションでエラーになるのでさける\n",
    "                if imges.size()[0] == 1:\n",
    "                    continue\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                imges = imges.to(device)\n",
    "                anno_class_imges = anno_class_imges.to(device)\n",
    "\n",
    "                \n",
    "                # multiple minibatchでのパラメータの更新\n",
    "                if (phase == 'train') and (count == 0):\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    count = batch_multiplier\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(imges)\n",
    "                    loss = criterion(\n",
    "                        outputs, anno_class_imges.long()) / batch_multiplier\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # 勾配の計算\n",
    "                        count -= 1  # multiple minibatch\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('イテレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
    "                                iteration, loss.item()/batch_size*batch_multiplier, duration))\n",
    "                            t_iter_start = time.time()\n",
    "\n",
    "                        epoch_train_loss += loss.item() * batch_multiplier\n",
    "                        iteration += 1\n",
    "\n",
    "                    # 検証時\n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item() * batch_multiplier\n",
    "\n",
    "        # epochのphaseごとのlossと正解率\n",
    "        t_epoch_finish = time.time()\n",
    "        print('-------------')\n",
    "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
    "            epoch+1, epoch_train_loss/num_train_imgs, epoch_val_loss/num_val_imgs))\n",
    "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        t_epoch_start = time.time()\n",
    "\n",
    "        # ログを保存\n",
    "        log_epoch = {'epoch': epoch+1, 'train_loss': epoch_train_loss /\n",
    "                     num_train_imgs, 'val_loss': epoch_val_loss/num_val_imgs}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv(\"log_output.csv\")\n",
    "\n",
    "    # 最後のネットワークを保存する\n",
    "    torch.save(net.state_dict(), 'weights/pspnet50_' +\n",
    "               str(epoch+1) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス： cuda:0\n",
      "-------------\n",
      "Epoch 1/30\n",
      "-------------\n",
      "（train）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuko/.python-venv/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 3.79 GiB total capacity; 2.05 GiB already allocated; 31.00 MiB free; 137.14 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6a2ef7f39e6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 学習・検証を実行する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-1caed0943c19>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;31m# 順伝搬（forward）計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                     loss = criterion(\n\u001b[1;32m     76\u001b[0m                         outputs, anno_class_imges.long()) / batch_multiplier\n",
      "\u001b[0;32m~/.python-venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch_advanced/3_Semantic_Segmentation/utils/pspnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moutput_aux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_dilated_res_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyramid_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.python-venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.python-venv/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.python-venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch_advanced/3_Semantic_Segmentation/utils/pspnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcb_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbr_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbr_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 58.00 MiB (GPU 0; 3.79 GiB total capacity; 2.05 GiB already allocated; 31.00 MiB free; 137.14 MiB cached)"
     ]
    }
   ],
   "source": [
    "# 学習・検証を実行する\n",
    "num_epochs = 30\n",
    "train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
