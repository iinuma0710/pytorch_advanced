{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-6 OpenPose の学習\n",
    "\n",
    "## 学習の注意点\n",
    "損失関数の定義と訓練用の Dataset に検証用データを使用する点に注意する．\n",
    "\n",
    "## DataLoader と Network の作成\n",
    "検証用の DataLoader を作成しないので，訓練用と検証用の DataLoader をまとめた辞書型変数 dataloaders_dict の val 側は None に設定する．\n",
    "なおミニバッチサイズは GPU メモリに載る範囲で最大に近い 32 と設定している．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "from utils.dataloader import make_datapath_list, DataTransform, COCOkeypointsDataset\n",
    "\n",
    "# COCO データセットのファイルパスリストを作成\n",
    "train_img_list, train_mask_list, val_img_list, val_mask_list, train_meta_list, val_meta_list = make_datapath_list(rootpath=\"./data/\")\n",
    "\n",
    "# Dataset の作成\n",
    "# train を val_list で作成しているため注意\n",
    "train_dataset = COCOkeypointsDataset(val_img_list, val_mask_list, val_meta_list, phase=\"train\", transform=DataTransform())\n",
    "\n",
    "# 今回は簡易な学習とし検証データは作成しない\n",
    "# val_dataset = CocokeypointsDataset(val_img_list, val_mask_list, val_meta_list, phase=\"val\", transform=DataTransform())\n",
    "\n",
    "# DataLoader 作成\n",
    "batch_size = 8\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_dataloader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 辞書型変数にまとめる\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": None}\n",
    "# dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.openpose_net import OpenPoseNet\n",
    "\n",
    "net = OpenPoseNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失関数の定義\n",
    "OpenPose の損失関数は heatmaps と PAFs のそれぞれについて正解アノテーションデータとの回帰の誤差になる．\n",
    "つまり，各ピクセルの値が教師データの値とどの程度近い値になるのか，ピクセルごとの値を回帰する．\n",
    "そのため，損失関数はセマンティックセグメンテーションとは異なるものとなる．  \n",
    "OpenPose の損失関数は回帰問題で一般に使われる平均二乗誤差とし，6つある各 Stage ごとに損失を計算する．\n",
    "ネットワークモデルの全体の誤差としては，各 Stage の全ての誤差を足し合わせたものとする．  \n",
    "人物が写っているが姿勢のアノテーションがない部分については損失を計算しない．\n",
    "そこで教師データのアノテーションと各 Stage で推定した内容のどちらにも mask（無視する部分の値が0，そうでない部分の値は1）をかけ算する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# 損失関数の設定\n",
    "class OpenPoseLoss(nn.Module):\n",
    "    ''' OpenPose の損失関数'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(OpenPoseLoss, self).__init__()\n",
    "        \n",
    "    \n",
    "    def forward(self, saved_for_loss, heatmap_target, heat_mask, paf_target, paf_mask):\n",
    "        \"\"\"\n",
    "        損失関数の計算\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        saved_for_loss : OpenPoseNet の出力 ( リスト )\n",
    "        heatmap_target : [num_batch, 19, 46, 46]\n",
    "            正解の部位のアノテーション情報\n",
    "        heatmap_mask : [num_batch, 19, 46, 46]\n",
    "            heatmap 画像の mask\n",
    "        paf_target : [num_batch, 38, 46, 46]\n",
    "            正解の PAF のアノテーション情報\n",
    "        paf_mask : [num_batch, 38, 46, 46]\n",
    "            PAF 画像の mask\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        loss : テンソル\n",
    "            損失の値\n",
    "        \"\"\"\n",
    "        \n",
    "        total_loss = 0\n",
    "        # ステージごとに計算\n",
    "        for j in range(6):\n",
    "            # PAFs と heatmaps においてマスクされている部分(paf_mask=0 など)は無視させる\n",
    "            # PAFs\n",
    "            pred1 = saved_for_loss[2 * j] * paf_mask\n",
    "            gt1 = paf_target.float() * paf_mask\n",
    "            \n",
    "            # heatmaps\n",
    "            pred2 = saved_for_loss[2 * j + 1] * heat_mask\n",
    "            gt2 = heatmap_target.float() * heat_mask\n",
    "            \n",
    "            total_loss += F.mse_loss(pred1, gt1, reduction=\"mean\") + F.mse_loss(pred2, gt2, reduction=\"mean\")\n",
    "            \n",
    "        return total_loss\n",
    "    \n",
    "criterion = OpenPoseLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習の実行\n",
    "本来であれば epoch ごとに徐々に学習率が小さくなるように設定するが，ここでは雰囲気の確認に留めるため簡単に設定する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=1e-2, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは検証フェーズを省略する．\n",
    "また，これまでとは異なり ```if imges.size()[0]== 1:``` という部分でミニバッチのサイズが 1 になっていないかをチェックしている．\n",
    "これは，PyTorch でバッチノーマライゼーションを行うときにミニバッチサイズが1だとエラーになるためである．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "    # GPU が使えることを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "    \n",
    "    # ネットワークを GPU へ\n",
    "    net.to(device)\n",
    "    \n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # 画像の枚数\n",
    "    num_train_imgs = len(dataloaders_dict[\"train\"].dataset)\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "\n",
    "    # イテレーションカウンタをセット\n",
    "    iteration = 1\n",
    "    \n",
    "    # 学習ループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # 開始時刻を保存\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "        epoch_train_loss = 0.0 # epoch の損失和\n",
    "        epoch_val_loss = 0.0 # epoch の損失和\n",
    "        \n",
    "        print('-------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "        \n",
    "        # epoch ごとの訓練と検証のループ\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                net.train()\n",
    "                optimizer.zero_grad()\n",
    "                print(' (train) ')\n",
    "                \n",
    "            # 今回は検証はスキップ\n",
    "            else:\n",
    "                continue\n",
    "                # net.eval()\n",
    "                # print('-------------')\n",
    "                # print('(val)')\n",
    "                \n",
    "            # データローダから minibatch ずつ取り出すループ\n",
    "            for images, heatmap_target, heat_mask, paf_target, paf_mask in dataloaders_dict[phase]:\n",
    "                # ミニバッチサイズが１だとエラーになる\n",
    "                if images.size()[0] == 1:\n",
    "                    continue\n",
    "                \n",
    "                # GPU が使えれば GPU を使う\n",
    "                images = images.to(device)\n",
    "                heatmap_target = heatmap_target.to(device)\n",
    "                heat_mask = heat_mask.to(device)\n",
    "                paf_target = paf_target.to(device)\n",
    "                paf_mask = paf_mask.to(device)\n",
    "                \n",
    "                # optimizer を初期化\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # foeward の計算\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    # (out6_1, out6_2) は使わないので _ で代替\n",
    "                    _, saved_for_loss = net(images)\n",
    "                    loss = criterion(saved_for_loss, heatmap_target, heat_mask, paf_target, paf_mask)\n",
    "                    \n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        if (iteration % 10 == 0): # 10iter に 1 度、loss を表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print( ' イテレーション {} || Loss: {:.4f} || 10iter:{:.4f} sec.'.format(iteration, loss.item()/batch_size, duration))\n",
    "                            t_iter_start = time.time()\n",
    "                        \n",
    "                        epoch_train_loss += loss.item()\n",
    "                        iteration += 1\n",
    "                    # 検証時\n",
    "                    # else:\n",
    "                        #epoch_val_loss += loss.item()\n",
    "                        \n",
    "                    # epochのphaseごとのlossと正解率\n",
    "                    t_epoch_finish = time.time()\n",
    "                    print('-------------')\n",
    "                    print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(epoch+1, epoch_train_loss/num_train_imgs, 0))\n",
    "                    print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "                    t_epoch_start = time.time()\n",
    "\n",
    "                    # 最後のネットワークを保存する\n",
    "                    torch.save(net.state_dict(), 'weights/openpose_net_' + str(epoch+1) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス： cuda:0\n",
      "-------------\n",
      "Epoch 1/2\n",
      "-------------\n",
      " (train) \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 3.79 GiB total capacity; 2.14 GiB already allocated; 19.31 MiB free; 17.30 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e8368a5f9284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 学習・検証を実行する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-c3462eede86f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(net, dataloaders_dict, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0;31m# (out6_1, out6_2) は使わないので _ で代替\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_for_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_for_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheatmap_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheat_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaf_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaf_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.python-venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch_advanced/4_Pose_Estimation/utils/openpose_net.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Stage5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mout5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout4_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout4_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mout5_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel5_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mout5_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel5_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 3.79 GiB total capacity; 2.14 GiB already allocated; 19.31 MiB free; 17.30 MiB cached)"
     ]
    }
   ],
   "source": [
    "# 学習・検証を実行する\n",
    "num_epochs = 2\n",
    "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
