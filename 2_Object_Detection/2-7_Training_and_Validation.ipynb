{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-7 学習と検証の実施\n",
    "\n",
    "## プログラムに実装\n",
    "前節までに実装した内容を用いて学習プログラムを実装する．\n",
    "流れは次の通り．\n",
    "\n",
    "1. DataLoader の実装\n",
    "1. ネットワークモデルの作成\n",
    "1. 損失関数の定義\n",
    "1. 最適化手法の設定\n",
    "1. 学習・検証の実施\n",
    "\n",
    "## DataLoader\n",
    "2.2，2.3節の実装を用いる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch import nn\n",
    "from utils.dataloader import od_collate_fn, make_VOC_dataloader\n",
    "from utils.dataset import make_datapath_list, VOCDataset, DataTransform, Anno_xml2list\n",
    "from utils.forward import SSD\n",
    "from utils.loss import MultiBoxLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイルパスのリストを作成\n",
    "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(rootpath)\n",
    "\n",
    "# Dataset を作成\n",
    "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair','cow', 'diningtable', \n",
    "               'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "color_mean = (104, 117, 123)  # BGR 色平均\n",
    "input_size = 300              # 画像の入力サイズは 300x300\n",
    "\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\",\n",
    "                           transform=DataTransform(input_size, color_mean),\n",
    "                           transform_anno=Anno_xml2list(voc_classes))\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\",\n",
    "                           transform=DataTransform(input_size, color_mean),\n",
    "                           transform_anno=Anno_xml2list(voc_classes))\n",
    "\n",
    "# データローダの作成\n",
    "batch_size = 8\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn)\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn)\n",
    "\n",
    "# 辞書型のオブジェクトにまとめる\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークモデルの作成\n",
    "2.4節で実装したネットワークを用いる．\n",
    "vgg モジュールの初期値には ImageNet で事前に学習した結合パラメータを用いる．\n",
    "ここでは，weights/vgg16_reducedfc.pth にダウンロード済みのものを用いる．\n",
    "それ以外のモジュールの初期値は He の初期値を用いる．\n",
    "He の初期値は，入力チャネル数 input_n に対して結合パラメータの初期値に，平均0，分散 2/input_n のガウス分布に従う乱数を使用する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス： cuda:0\n",
      "ネットワーク設定完了：学習済みの重みをロードしました\n"
     ]
    }
   ],
   "source": [
    "# SSD300 の設定\n",
    "ssd_cfg = {\n",
    "    'num_classes': 21, # 背景クラスを含めた合計クラス数\n",
    "    'input_size': 300, # 画像の入力サイズ\n",
    "    'bbox_aspect_num': [4, 6, 6, 6, 4, 4],      # 出力するデフォルトボックスのアスペクト比の種類\n",
    "    'feature_maps': [38, 19, 10, 5, 3, 1],      # 各 source の画像サイズ\n",
    "    'steps': [8, 16, 32, 64, 100, 300],         # デフォルトボックスの大きさを決める\n",
    "    'min_sizes': [30, 60, 111, 162, 213, 264],  # デフォルトボックスの大きさを決める\n",
    "    'max_sizes': [60, 111, 162, 213, 264, 315], # デフォルトボックスの大きさを決める\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]]\n",
    "}\n",
    "\n",
    "# SSD ネットワークモデル\n",
    "net = SSD(phase=\"train\", cfg=ssd_cfg)\n",
    "\n",
    "# SSD の結合パラメータの初期値を設定\n",
    "# vgg モジュールの初期値をロード\n",
    "vgg_weights = torch.load(\"./weights/vgg16_reducedfc.pth\")\n",
    "net.vgg.load_state_dict(vgg_weights)\n",
    "\n",
    "# SSD のその他のネットワークの重みは He の初期値で初期化\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight.data)\n",
    "        if m.bias is not None:  # バイアス項がある場合\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "# He の初期値を適用\n",
    "net.extras.apply(weights_init)\n",
    "net.loc.apply(weights_init)\n",
    "net.conf.apply(weights_init)\n",
    "\n",
    "# GPU が使えるかを確認\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"使用デバイス：\", device)\n",
    "print(\"ネットワーク設定完了：学習済みの重みをロードしました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失関数と最適化手法の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数の設定\n",
    "criterion = MultiBoxLoss(jaccard_threshold=0.5, neg_pos=3, device=device)\n",
    "# 最適化手法の設定\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習と検証の実施\n",
    "学習と検証を行う関数を定義する．\n",
    "10 epoch ごとに検証を行い，各 epoch ごとに学習と検証の loss の値を log_output.csv に保存する．\n",
    "ネットワークの結合パラメータも 10 epoch ごとに保存する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "    # GPU が使えるか確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "    \n",
    "    # ネットワークを GPU へ\n",
    "    net.to(device)\n",
    "    \n",
    "    # ネットワークがある程度固定であれば高速化\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # イテレーションのカウンタをセット\n",
    "    iteration = 1\n",
    "    epoch_train_loss = 0.0  # 学習時の epoch の損失和\n",
    "    epoch_val_loss = 0.0    # 検証時の epoch の損失和\n",
    "    logs = []\n",
    "    \n",
    "    # epoch のグループ\n",
    "    for epoch in range(num_epochs + 1):\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "        \n",
    "        print(\"-------------------\")\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print(\"-------------------\")\n",
    "        \n",
    "        # epoch ごとの訓練と検証のループ\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                net.train()  # モデルを訓練モードに設定\n",
    "                print(\"-------------------\")\n",
    "                print(\" (train) \")\n",
    "            else:\n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    net.eval()  # モデルを検証モードに設定\n",
    "                    print(\"-------------------\")\n",
    "                    print(\" (val) \")\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            # データローダから mini batch サイズずつ取り出すループ\n",
    "            for images, targets in dataloaders_dict[phase]:\n",
    "                # GPU が使用可能であれば GPU にデータを送る\n",
    "                images = images.to(device)\n",
    "                targets = [ann.to(device) for ann in targets] # リストの各要素のテンソルを GPU に送る\n",
    "                \n",
    "                # optimizer を初期化\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 学習の実行\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # 順伝播の計算\n",
    "                    outputs = net(images)\n",
    "                    \n",
    "                    # 損失の計算\n",
    "                    loss_l, loss_c = criterion(outputs, targets)\n",
    "                    loss = loss_l + loss_c\n",
    "                    \n",
    "                    # 訓練時には逆伝播も計算\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        # 勾配が大きくなりすぎると学習が不安定になるため最大で2.0となるように制限\n",
    "                        nn.utils.clip_grad_value_(net.parameters(), clip_value=2.0)\n",
    "                        # パラメータを更新\n",
    "                        optimizer.step()\n",
    "                        # 10 iter に1度 loss を表示\n",
    "                        if iteration % 10 == 0:\n",
    "                            t_iter_finish =time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('iteration {} || Loss: {:.4f} || 10iter:{:.4f} sec.'.format(iteration, loss.item(), duration))\n",
    "                            t_iter_start = time.time()\n",
    "                            \n",
    "                        epoch_train_loss += loss.item()\n",
    "                        iteration += 1\n",
    "                        \n",
    "                    # 検証時\n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item()\n",
    "                        \n",
    "        # epoch の phase ごとの loss と正解率\n",
    "        t_epoch_finish = time.time()\n",
    "        print(\"-------------------\")\n",
    "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} || Epoch_VAL_Loss:{:.4f}'.format(epoch + 1, epoch_train_loss, epoch_val_loss))\n",
    "        t_epoch_start = time.time()\n",
    "        \n",
    "        # ログの保存\n",
    "        log_epoch = {\"epoch\": epoch + 1, \"train_loss\": epoch_train_loss, \"val_loss\": epoch_val_loss}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv(\"log_output.csv\")\n",
    "        \n",
    "        # 損失値のリセット\n",
    "        epoch_train_loss = 0.0\n",
    "        epoch_val_loss = 0.0\n",
    "        \n",
    "        # 10 epoch ごとにネットワークを保存する\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save(net.state_dict(), 'weights/ssd300_' + str(epoch+1) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス： cuda:0\n",
      "-------------------\n",
      "Epoch 1/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 10 || Loss: 15.2293 || 10iter:11.3841 sec.\n",
      "iteration 20 || Loss: 13.6413 || 10iter:7.8792 sec.\n",
      "iteration 30 || Loss: 14.4254 || 10iter:7.7383 sec.\n",
      "iteration 40 || Loss: 9.7031 || 10iter:7.7602 sec.\n",
      "iteration 50 || Loss: 11.6924 || 10iter:7.7592 sec.\n",
      "iteration 60 || Loss: 10.1063 || 10iter:7.7779 sec.\n",
      "iteration 70 || Loss: 10.8501 || 10iter:7.8928 sec.\n",
      "iteration 80 || Loss: 7.9851 || 10iter:7.8083 sec.\n",
      "iteration 90 || Loss: 9.8848 || 10iter:8.0706 sec.\n",
      "iteration 100 || Loss: 9.9436 || 10iter:8.2000 sec.\n",
      "iteration 110 || Loss: 8.4661 || 10iter:7.9685 sec.\n",
      "iteration 120 || Loss: 8.2492 || 10iter:7.8529 sec.\n",
      "iteration 130 || Loss: 9.4068 || 10iter:8.0069 sec.\n",
      "iteration 140 || Loss: 7.9618 || 10iter:7.8789 sec.\n",
      "iteration 150 || Loss: 9.2625 || 10iter:7.8460 sec.\n",
      "iteration 160 || Loss: 10.8993 || 10iter:7.9483 sec.\n",
      "iteration 170 || Loss: 8.4994 || 10iter:8.1447 sec.\n",
      "iteration 180 || Loss: 9.6938 || 10iter:8.1053 sec.\n",
      "iteration 190 || Loss: 11.1307 || 10iter:7.9462 sec.\n",
      "iteration 200 || Loss: 10.8654 || 10iter:7.9619 sec.\n",
      "iteration 210 || Loss: 8.0246 || 10iter:8.0740 sec.\n",
      "iteration 220 || Loss: 8.4336 || 10iter:8.5145 sec.\n",
      "iteration 230 || Loss: 8.1277 || 10iter:7.9483 sec.\n",
      "iteration 240 || Loss: 8.8627 || 10iter:7.7977 sec.\n",
      "iteration 250 || Loss: 9.8859 || 10iter:7.8912 sec.\n",
      "iteration 260 || Loss: 12.1407 || 10iter:7.9301 sec.\n",
      "iteration 270 || Loss: 10.6362 || 10iter:8.0230 sec.\n",
      "iteration 280 || Loss: 9.8551 || 10iter:7.9374 sec.\n",
      "iteration 290 || Loss: 10.3039 || 10iter:8.0764 sec.\n",
      "iteration 300 || Loss: 7.5039 || 10iter:7.9105 sec.\n",
      "iteration 310 || Loss: 10.0010 || 10iter:7.9705 sec.\n",
      "iteration 320 || Loss: 10.2445 || 10iter:7.9792 sec.\n",
      "iteration 330 || Loss: 8.6753 || 10iter:8.0399 sec.\n",
      "iteration 340 || Loss: 7.2307 || 10iter:7.8602 sec.\n",
      "iteration 350 || Loss: 10.4094 || 10iter:7.8849 sec.\n",
      "iteration 360 || Loss: 9.8269 || 10iter:7.9403 sec.\n",
      "iteration 370 || Loss: 10.1577 || 10iter:7.7975 sec.\n",
      "iteration 380 || Loss: 11.1000 || 10iter:8.0178 sec.\n",
      "iteration 390 || Loss: 12.0281 || 10iter:8.0191 sec.\n",
      "iteration 400 || Loss: 9.1428 || 10iter:7.8921 sec.\n",
      "iteration 410 || Loss: 10.1330 || 10iter:8.1203 sec.\n",
      "iteration 420 || Loss: 7.4521 || 10iter:8.0977 sec.\n",
      "iteration 430 || Loss: 9.5869 || 10iter:8.0509 sec.\n",
      "iteration 440 || Loss: 8.4574 || 10iter:8.0716 sec.\n",
      "iteration 450 || Loss: 9.5378 || 10iter:8.0407 sec.\n",
      "iteration 460 || Loss: 8.0962 || 10iter:8.1144 sec.\n",
      "iteration 470 || Loss: 7.0243 || 10iter:8.1414 sec.\n",
      "iteration 480 || Loss: 9.1375 || 10iter:7.9689 sec.\n",
      "iteration 490 || Loss: 9.1375 || 10iter:7.9997 sec.\n",
      "iteration 500 || Loss: 8.2180 || 10iter:7.9663 sec.\n",
      "iteration 510 || Loss: 7.8288 || 10iter:8.0667 sec.\n",
      "iteration 520 || Loss: 7.8946 || 10iter:7.9519 sec.\n",
      "iteration 530 || Loss: 7.2065 || 10iter:8.1307 sec.\n",
      "iteration 540 || Loss: 16.8449 || 10iter:7.9744 sec.\n",
      "iteration 550 || Loss: 8.1566 || 10iter:7.8687 sec.\n",
      "iteration 560 || Loss: 8.4973 || 10iter:7.9967 sec.\n",
      "iteration 570 || Loss: 7.5158 || 10iter:8.0303 sec.\n",
      "iteration 580 || Loss: 6.8095 || 10iter:8.0092 sec.\n",
      "iteration 590 || Loss: 13.6047 || 10iter:8.1671 sec.\n",
      "iteration 600 || Loss: 7.7157 || 10iter:7.9611 sec.\n",
      "iteration 610 || Loss: 8.3549 || 10iter:8.1721 sec.\n",
      "iteration 620 || Loss: 9.9541 || 10iter:8.0219 sec.\n",
      "iteration 630 || Loss: 10.0474 || 10iter:7.9452 sec.\n",
      "iteration 640 || Loss: 7.9199 || 10iter:7.9434 sec.\n",
      "iteration 650 || Loss: 8.0108 || 10iter:7.9862 sec.\n",
      "iteration 660 || Loss: 9.2867 || 10iter:7.8559 sec.\n",
      "iteration 670 || Loss: 7.1986 || 10iter:7.8996 sec.\n",
      "iteration 680 || Loss: 8.9679 || 10iter:7.8328 sec.\n",
      "iteration 690 || Loss: 13.1065 || 10iter:7.8855 sec.\n",
      "iteration 700 || Loss: 7.4433 || 10iter:7.8796 sec.\n",
      "iteration 710 || Loss: 8.5508 || 10iter:7.9830 sec.\n",
      "-------------------\n",
      "epoch 1 || Epoch_TRAIN_Loss:7011.5184 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 2/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 720 || Loss: 10.5463 || 10iter:3.8340 sec.\n",
      "iteration 730 || Loss: 7.9046 || 10iter:7.8709 sec.\n",
      "iteration 740 || Loss: 7.3472 || 10iter:7.8891 sec.\n",
      "iteration 750 || Loss: 10.8048 || 10iter:7.9469 sec.\n",
      "iteration 760 || Loss: 7.2664 || 10iter:7.8073 sec.\n",
      "iteration 770 || Loss: 9.7487 || 10iter:7.8518 sec.\n",
      "iteration 780 || Loss: 8.8360 || 10iter:7.8245 sec.\n",
      "iteration 790 || Loss: 9.6439 || 10iter:7.8700 sec.\n",
      "iteration 800 || Loss: 8.7053 || 10iter:7.8831 sec.\n",
      "iteration 810 || Loss: 9.4868 || 10iter:7.9215 sec.\n",
      "iteration 820 || Loss: 9.8616 || 10iter:7.8131 sec.\n",
      "iteration 830 || Loss: 9.1819 || 10iter:7.8895 sec.\n",
      "iteration 840 || Loss: 9.7133 || 10iter:7.7518 sec.\n",
      "iteration 850 || Loss: 9.8329 || 10iter:7.9074 sec.\n",
      "iteration 860 || Loss: 7.3147 || 10iter:7.9236 sec.\n",
      "iteration 870 || Loss: 9.7364 || 10iter:7.8102 sec.\n",
      "iteration 880 || Loss: 10.3929 || 10iter:7.8333 sec.\n",
      "iteration 890 || Loss: 7.1445 || 10iter:7.8842 sec.\n",
      "iteration 900 || Loss: 7.1950 || 10iter:7.7686 sec.\n",
      "iteration 910 || Loss: 7.6553 || 10iter:7.8614 sec.\n",
      "iteration 920 || Loss: 9.0012 || 10iter:7.9034 sec.\n",
      "iteration 930 || Loss: 11.3428 || 10iter:7.8082 sec.\n",
      "iteration 940 || Loss: 8.0707 || 10iter:7.9528 sec.\n",
      "iteration 950 || Loss: 9.2286 || 10iter:7.9391 sec.\n",
      "iteration 960 || Loss: 7.6730 || 10iter:7.8849 sec.\n",
      "iteration 970 || Loss: 7.1743 || 10iter:7.8915 sec.\n",
      "iteration 980 || Loss: 10.3552 || 10iter:7.9087 sec.\n",
      "iteration 990 || Loss: 10.0594 || 10iter:7.8006 sec.\n",
      "iteration 1000 || Loss: 8.1282 || 10iter:7.8494 sec.\n",
      "iteration 1010 || Loss: 7.9917 || 10iter:7.9104 sec.\n",
      "iteration 1020 || Loss: 7.8922 || 10iter:7.9043 sec.\n",
      "iteration 1030 || Loss: 7.5686 || 10iter:7.8788 sec.\n",
      "iteration 1040 || Loss: 6.6709 || 10iter:7.9373 sec.\n",
      "iteration 1050 || Loss: 8.8434 || 10iter:7.8616 sec.\n",
      "iteration 1060 || Loss: 8.9511 || 10iter:7.8184 sec.\n",
      "iteration 1070 || Loss: 11.8758 || 10iter:8.0298 sec.\n",
      "iteration 1080 || Loss: 8.3757 || 10iter:7.8005 sec.\n",
      "iteration 1090 || Loss: 6.8221 || 10iter:7.8712 sec.\n",
      "iteration 1100 || Loss: 8.7992 || 10iter:7.9093 sec.\n",
      "iteration 1110 || Loss: 7.5695 || 10iter:7.8491 sec.\n",
      "iteration 1120 || Loss: 9.6781 || 10iter:8.0335 sec.\n",
      "iteration 1130 || Loss: 8.7190 || 10iter:7.8703 sec.\n",
      "iteration 1140 || Loss: 11.1488 || 10iter:7.9218 sec.\n",
      "iteration 1150 || Loss: 10.3679 || 10iter:7.8755 sec.\n",
      "iteration 1160 || Loss: 11.6363 || 10iter:7.8535 sec.\n",
      "iteration 1170 || Loss: 8.7591 || 10iter:7.8488 sec.\n",
      "iteration 1180 || Loss: 10.8961 || 10iter:7.8847 sec.\n",
      "iteration 1190 || Loss: 7.8870 || 10iter:7.9979 sec.\n",
      "iteration 1200 || Loss: 10.8649 || 10iter:7.8844 sec.\n",
      "iteration 1210 || Loss: 8.8593 || 10iter:7.8808 sec.\n",
      "iteration 1220 || Loss: 13.7148 || 10iter:7.8729 sec.\n",
      "iteration 1230 || Loss: 9.9347 || 10iter:7.9012 sec.\n",
      "iteration 1240 || Loss: 9.9873 || 10iter:7.8525 sec.\n",
      "iteration 1250 || Loss: 8.3859 || 10iter:7.8616 sec.\n",
      "iteration 1260 || Loss: 8.0695 || 10iter:7.8370 sec.\n",
      "iteration 1270 || Loss: 9.0106 || 10iter:7.8677 sec.\n",
      "iteration 1280 || Loss: 7.2890 || 10iter:8.1985 sec.\n",
      "iteration 1290 || Loss: 8.6186 || 10iter:8.1840 sec.\n",
      "iteration 1300 || Loss: 14.9849 || 10iter:7.9982 sec.\n",
      "iteration 1310 || Loss: 9.4047 || 10iter:7.9785 sec.\n",
      "iteration 1320 || Loss: 7.3873 || 10iter:7.9594 sec.\n",
      "iteration 1330 || Loss: 7.3487 || 10iter:7.8292 sec.\n",
      "iteration 1340 || Loss: 12.2025 || 10iter:7.9940 sec.\n",
      "iteration 1350 || Loss: 11.8396 || 10iter:8.0030 sec.\n",
      "iteration 1360 || Loss: 8.5340 || 10iter:8.1376 sec.\n",
      "iteration 1370 || Loss: 7.4169 || 10iter:7.8613 sec.\n",
      "iteration 1380 || Loss: 10.5199 || 10iter:7.8970 sec.\n",
      "iteration 1390 || Loss: 7.1347 || 10iter:7.9228 sec.\n",
      "iteration 1400 || Loss: 7.5894 || 10iter:7.7583 sec.\n",
      "iteration 1410 || Loss: 8.3679 || 10iter:7.8974 sec.\n",
      "iteration 1420 || Loss: 7.8210 || 10iter:7.9171 sec.\n",
      "iteration 1430 || Loss: 9.4008 || 10iter:7.9713 sec.\n",
      "-------------------\n",
      "epoch 2 || Epoch_TRAIN_Loss:6402.6031 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 3/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 1440 || Loss: 8.8872 || 10iter:8.0881 sec.\n",
      "iteration 1450 || Loss: 9.4819 || 10iter:8.1121 sec.\n",
      "iteration 1460 || Loss: 7.6348 || 10iter:8.0729 sec.\n",
      "iteration 1470 || Loss: 8.0473 || 10iter:8.1619 sec.\n",
      "iteration 1480 || Loss: 9.4064 || 10iter:7.9970 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1490 || Loss: 6.5225 || 10iter:8.0989 sec.\n",
      "iteration 1500 || Loss: 7.7073 || 10iter:7.9287 sec.\n",
      "iteration 1510 || Loss: 7.0649 || 10iter:8.2696 sec.\n",
      "iteration 1520 || Loss: 6.3736 || 10iter:8.0390 sec.\n",
      "iteration 1530 || Loss: 6.4012 || 10iter:8.2679 sec.\n",
      "iteration 1540 || Loss: 8.5400 || 10iter:8.0381 sec.\n",
      "iteration 1550 || Loss: 8.2234 || 10iter:7.9244 sec.\n",
      "iteration 1560 || Loss: 8.3384 || 10iter:7.8214 sec.\n",
      "iteration 1570 || Loss: 8.7888 || 10iter:7.9256 sec.\n",
      "iteration 1580 || Loss: 8.1114 || 10iter:7.8121 sec.\n",
      "iteration 1590 || Loss: 6.9493 || 10iter:7.9816 sec.\n",
      "iteration 1600 || Loss: 8.8552 || 10iter:7.8491 sec.\n",
      "iteration 1610 || Loss: 6.4098 || 10iter:7.7929 sec.\n",
      "iteration 1620 || Loss: 10.6343 || 10iter:7.8728 sec.\n",
      "iteration 1630 || Loss: 9.3367 || 10iter:7.9640 sec.\n",
      "iteration 1640 || Loss: 7.4395 || 10iter:7.9645 sec.\n",
      "iteration 1650 || Loss: 9.3289 || 10iter:7.9667 sec.\n",
      "iteration 1660 || Loss: 9.3559 || 10iter:7.9214 sec.\n",
      "iteration 1670 || Loss: 9.8283 || 10iter:7.8816 sec.\n",
      "iteration 1680 || Loss: 11.8260 || 10iter:7.7578 sec.\n",
      "iteration 1690 || Loss: 9.5640 || 10iter:8.1406 sec.\n",
      "iteration 1700 || Loss: 14.1318 || 10iter:8.0479 sec.\n",
      "iteration 1710 || Loss: 7.0966 || 10iter:7.8226 sec.\n",
      "iteration 1720 || Loss: 7.8954 || 10iter:7.9425 sec.\n",
      "iteration 1730 || Loss: 7.2386 || 10iter:7.9259 sec.\n",
      "iteration 1740 || Loss: 9.6394 || 10iter:8.7650 sec.\n",
      "iteration 1750 || Loss: 11.3515 || 10iter:8.8141 sec.\n",
      "iteration 1760 || Loss: 16.0458 || 10iter:8.6356 sec.\n",
      "iteration 1770 || Loss: 8.7039 || 10iter:8.7594 sec.\n",
      "iteration 1780 || Loss: 8.5797 || 10iter:8.2332 sec.\n",
      "iteration 1790 || Loss: 11.4055 || 10iter:8.3765 sec.\n",
      "iteration 1800 || Loss: 6.3592 || 10iter:8.3566 sec.\n",
      "iteration 1810 || Loss: 6.8971 || 10iter:7.9823 sec.\n",
      "iteration 1820 || Loss: 12.0879 || 10iter:7.8569 sec.\n",
      "iteration 1830 || Loss: 10.4311 || 10iter:7.9331 sec.\n",
      "iteration 1840 || Loss: 6.5415 || 10iter:7.8851 sec.\n",
      "iteration 1850 || Loss: 10.4701 || 10iter:7.9243 sec.\n",
      "iteration 1860 || Loss: 8.2366 || 10iter:7.8753 sec.\n",
      "iteration 1870 || Loss: 7.0196 || 10iter:7.9735 sec.\n",
      "iteration 1880 || Loss: 8.5040 || 10iter:7.8478 sec.\n",
      "iteration 1890 || Loss: 8.0493 || 10iter:7.8581 sec.\n",
      "iteration 1900 || Loss: 7.6913 || 10iter:7.7863 sec.\n",
      "iteration 1910 || Loss: 9.1978 || 10iter:7.8040 sec.\n",
      "iteration 1920 || Loss: 6.7262 || 10iter:7.8451 sec.\n",
      "iteration 1930 || Loss: 7.7019 || 10iter:7.9161 sec.\n",
      "iteration 1940 || Loss: 8.3959 || 10iter:7.9373 sec.\n",
      "iteration 1950 || Loss: 10.2734 || 10iter:7.9225 sec.\n",
      "iteration 1960 || Loss: 6.9255 || 10iter:7.7568 sec.\n",
      "iteration 1970 || Loss: 8.4000 || 10iter:7.7594 sec.\n",
      "iteration 1980 || Loss: 11.8083 || 10iter:7.8785 sec.\n",
      "iteration 1990 || Loss: 6.8405 || 10iter:7.8675 sec.\n",
      "iteration 2000 || Loss: 6.9194 || 10iter:7.7678 sec.\n",
      "iteration 2010 || Loss: 8.5483 || 10iter:7.8128 sec.\n",
      "iteration 2020 || Loss: 12.5999 || 10iter:7.9771 sec.\n",
      "iteration 2030 || Loss: 10.2067 || 10iter:7.7749 sec.\n",
      "iteration 2040 || Loss: 7.5772 || 10iter:7.8620 sec.\n",
      "iteration 2050 || Loss: 7.0271 || 10iter:7.9386 sec.\n",
      "iteration 2060 || Loss: 11.8392 || 10iter:7.9460 sec.\n",
      "iteration 2070 || Loss: 9.1255 || 10iter:7.9209 sec.\n",
      "iteration 2080 || Loss: 11.1662 || 10iter:8.0083 sec.\n",
      "iteration 2090 || Loss: 6.7068 || 10iter:7.9435 sec.\n",
      "iteration 2100 || Loss: 11.5968 || 10iter:7.8193 sec.\n",
      "iteration 2110 || Loss: 9.0594 || 10iter:7.8645 sec.\n",
      "iteration 2120 || Loss: 9.3821 || 10iter:7.8526 sec.\n",
      "iteration 2130 || Loss: 10.9532 || 10iter:7.8486 sec.\n",
      "iteration 2140 || Loss: 6.5916 || 10iter:7.8396 sec.\n",
      "-------------------\n",
      "epoch 3 || Epoch_TRAIN_Loss:6374.7362 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 4/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 2150 || Loss: 9.0501 || 10iter:3.7996 sec.\n",
      "iteration 2160 || Loss: 6.9811 || 10iter:7.9591 sec.\n",
      "iteration 2170 || Loss: 7.3449 || 10iter:7.8709 sec.\n",
      "iteration 2180 || Loss: 6.7092 || 10iter:7.9276 sec.\n",
      "iteration 2190 || Loss: 9.1834 || 10iter:7.8564 sec.\n",
      "iteration 2200 || Loss: 10.0095 || 10iter:7.8255 sec.\n",
      "iteration 2210 || Loss: 9.2520 || 10iter:7.9015 sec.\n",
      "iteration 2220 || Loss: 6.3145 || 10iter:7.8609 sec.\n",
      "iteration 2230 || Loss: 6.3127 || 10iter:7.8878 sec.\n",
      "iteration 2240 || Loss: 10.0617 || 10iter:7.9748 sec.\n",
      "iteration 2250 || Loss: 8.1961 || 10iter:7.9513 sec.\n",
      "iteration 2260 || Loss: 7.8083 || 10iter:7.8518 sec.\n",
      "iteration 2270 || Loss: 9.8700 || 10iter:7.9319 sec.\n",
      "iteration 2280 || Loss: 8.1136 || 10iter:7.6925 sec.\n",
      "iteration 2290 || Loss: 8.4029 || 10iter:7.9213 sec.\n",
      "iteration 2300 || Loss: 6.4589 || 10iter:7.7129 sec.\n",
      "iteration 2310 || Loss: 8.6822 || 10iter:7.7741 sec.\n",
      "iteration 2320 || Loss: 9.2820 || 10iter:7.9371 sec.\n",
      "iteration 2330 || Loss: 8.9013 || 10iter:7.8544 sec.\n",
      "iteration 2340 || Loss: 10.5222 || 10iter:7.8868 sec.\n",
      "iteration 2350 || Loss: 6.4125 || 10iter:7.9189 sec.\n",
      "iteration 2360 || Loss: 6.9176 || 10iter:7.8177 sec.\n",
      "iteration 2370 || Loss: 8.4793 || 10iter:7.7199 sec.\n",
      "iteration 2380 || Loss: 10.4087 || 10iter:7.8540 sec.\n",
      "iteration 2390 || Loss: 7.7284 || 10iter:7.8697 sec.\n",
      "iteration 2400 || Loss: 8.3681 || 10iter:7.9588 sec.\n",
      "iteration 2410 || Loss: 9.0909 || 10iter:7.7949 sec.\n",
      "iteration 2420 || Loss: 10.1522 || 10iter:7.9071 sec.\n",
      "iteration 2430 || Loss: 8.2723 || 10iter:7.9055 sec.\n",
      "iteration 2440 || Loss: 11.3091 || 10iter:7.8269 sec.\n",
      "iteration 2450 || Loss: 10.7607 || 10iter:7.9446 sec.\n",
      "iteration 2460 || Loss: 7.1257 || 10iter:7.9457 sec.\n",
      "iteration 2470 || Loss: 7.1922 || 10iter:8.0178 sec.\n",
      "iteration 2480 || Loss: 12.5372 || 10iter:8.0869 sec.\n",
      "iteration 2490 || Loss: 7.4611 || 10iter:7.8061 sec.\n",
      "iteration 2500 || Loss: 7.5694 || 10iter:7.8325 sec.\n",
      "iteration 2510 || Loss: 10.0333 || 10iter:7.8187 sec.\n",
      "iteration 2520 || Loss: 8.4082 || 10iter:7.8176 sec.\n",
      "iteration 2530 || Loss: 7.4496 || 10iter:7.7885 sec.\n",
      "iteration 2540 || Loss: 7.6229 || 10iter:7.8859 sec.\n",
      "iteration 2550 || Loss: 10.3456 || 10iter:7.8559 sec.\n",
      "iteration 2560 || Loss: 8.1060 || 10iter:7.9175 sec.\n",
      "iteration 2570 || Loss: 10.3964 || 10iter:7.9728 sec.\n",
      "iteration 2580 || Loss: 6.5265 || 10iter:7.9749 sec.\n",
      "iteration 2590 || Loss: 7.5988 || 10iter:7.9424 sec.\n",
      "iteration 2600 || Loss: 10.1521 || 10iter:7.7817 sec.\n",
      "iteration 2610 || Loss: 9.3460 || 10iter:7.8244 sec.\n",
      "iteration 2620 || Loss: 9.3799 || 10iter:7.8161 sec.\n",
      "iteration 2630 || Loss: 9.7560 || 10iter:7.9140 sec.\n",
      "iteration 2640 || Loss: 8.4789 || 10iter:7.8807 sec.\n",
      "iteration 2650 || Loss: 7.2051 || 10iter:7.9305 sec.\n",
      "iteration 2660 || Loss: 9.1790 || 10iter:7.8987 sec.\n",
      "iteration 2670 || Loss: 8.1230 || 10iter:7.8697 sec.\n",
      "iteration 2680 || Loss: 11.1211 || 10iter:7.8448 sec.\n",
      "iteration 2690 || Loss: 7.6346 || 10iter:7.8830 sec.\n",
      "iteration 2700 || Loss: 7.2573 || 10iter:7.8288 sec.\n",
      "iteration 2710 || Loss: 9.7424 || 10iter:7.8635 sec.\n",
      "iteration 2720 || Loss: 9.0004 || 10iter:7.8290 sec.\n",
      "iteration 2730 || Loss: 8.0143 || 10iter:7.7960 sec.\n",
      "iteration 2740 || Loss: 7.2801 || 10iter:7.9301 sec.\n",
      "iteration 2750 || Loss: 7.6371 || 10iter:7.7419 sec.\n",
      "iteration 2760 || Loss: 6.5419 || 10iter:7.8999 sec.\n",
      "iteration 2770 || Loss: 7.1321 || 10iter:7.9339 sec.\n",
      "iteration 2780 || Loss: 8.0706 || 10iter:7.9731 sec.\n",
      "iteration 2790 || Loss: 7.3465 || 10iter:8.2012 sec.\n",
      "iteration 2800 || Loss: 6.6729 || 10iter:8.0162 sec.\n",
      "iteration 2810 || Loss: 7.0354 || 10iter:7.8582 sec.\n",
      "iteration 2820 || Loss: 7.6908 || 10iter:7.8225 sec.\n",
      "iteration 2830 || Loss: 7.7350 || 10iter:7.8551 sec.\n",
      "iteration 2840 || Loss: 6.7342 || 10iter:7.8840 sec.\n",
      "iteration 2850 || Loss: 7.9387 || 10iter:7.8977 sec.\n",
      "iteration 2860 || Loss: 9.8630 || 10iter:7.6839 sec.\n",
      "-------------------\n",
      "epoch 4 || Epoch_TRAIN_Loss:6087.8875 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 5/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 2870 || Loss: 6.8369 || 10iter:7.8712 sec.\n",
      "iteration 2880 || Loss: 7.1221 || 10iter:7.8292 sec.\n",
      "iteration 2890 || Loss: 11.7486 || 10iter:7.8948 sec.\n",
      "iteration 2900 || Loss: 7.3331 || 10iter:7.8938 sec.\n",
      "iteration 2910 || Loss: 9.5023 || 10iter:7.9399 sec.\n",
      "iteration 2920 || Loss: 7.2076 || 10iter:7.9292 sec.\n",
      "iteration 2930 || Loss: 6.9664 || 10iter:7.9041 sec.\n",
      "iteration 2940 || Loss: 8.0673 || 10iter:7.9532 sec.\n",
      "iteration 2950 || Loss: 7.7467 || 10iter:7.8584 sec.\n",
      "iteration 2960 || Loss: 7.9304 || 10iter:7.9969 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2970 || Loss: 6.7068 || 10iter:7.9372 sec.\n",
      "iteration 2980 || Loss: 8.8598 || 10iter:8.0227 sec.\n",
      "iteration 2990 || Loss: 8.4985 || 10iter:7.9660 sec.\n",
      "iteration 3000 || Loss: 6.6875 || 10iter:7.9178 sec.\n",
      "iteration 3010 || Loss: 7.6920 || 10iter:7.8602 sec.\n",
      "iteration 3020 || Loss: 7.0075 || 10iter:7.8251 sec.\n",
      "iteration 3030 || Loss: 9.7056 || 10iter:7.9332 sec.\n",
      "iteration 3040 || Loss: 6.4420 || 10iter:7.8441 sec.\n",
      "iteration 3050 || Loss: 8.9670 || 10iter:7.8265 sec.\n",
      "iteration 3060 || Loss: 5.1964 || 10iter:7.8961 sec.\n",
      "iteration 3070 || Loss: 9.4408 || 10iter:7.9173 sec.\n",
      "iteration 3080 || Loss: 7.6450 || 10iter:7.8456 sec.\n",
      "iteration 3090 || Loss: 10.1844 || 10iter:7.8211 sec.\n",
      "iteration 3100 || Loss: 6.3175 || 10iter:7.8938 sec.\n",
      "iteration 3110 || Loss: 9.1922 || 10iter:7.8135 sec.\n",
      "iteration 3120 || Loss: 7.8163 || 10iter:7.8980 sec.\n",
      "iteration 3130 || Loss: 6.9840 || 10iter:7.7785 sec.\n",
      "iteration 3140 || Loss: 6.4152 || 10iter:7.8841 sec.\n",
      "iteration 3150 || Loss: 7.1860 || 10iter:7.8319 sec.\n",
      "iteration 3160 || Loss: 9.8239 || 10iter:7.9966 sec.\n",
      "iteration 3170 || Loss: 7.6803 || 10iter:7.7808 sec.\n",
      "iteration 3180 || Loss: 8.6619 || 10iter:7.8291 sec.\n",
      "iteration 3190 || Loss: 7.7269 || 10iter:7.8232 sec.\n",
      "iteration 3200 || Loss: 5.8157 || 10iter:7.8322 sec.\n",
      "iteration 3210 || Loss: 10.2497 || 10iter:7.8592 sec.\n",
      "iteration 3220 || Loss: 8.1147 || 10iter:7.8418 sec.\n",
      "iteration 3230 || Loss: 6.5110 || 10iter:7.8234 sec.\n",
      "iteration 3240 || Loss: 7.6660 || 10iter:7.9910 sec.\n",
      "iteration 3250 || Loss: 6.3567 || 10iter:7.8359 sec.\n",
      "iteration 3260 || Loss: 11.1005 || 10iter:7.9582 sec.\n",
      "iteration 3270 || Loss: 7.2843 || 10iter:7.8521 sec.\n",
      "iteration 3280 || Loss: 8.1990 || 10iter:7.7878 sec.\n",
      "iteration 3290 || Loss: 8.9012 || 10iter:7.8808 sec.\n",
      "iteration 3300 || Loss: 10.0244 || 10iter:7.9033 sec.\n",
      "iteration 3310 || Loss: 7.1013 || 10iter:7.8107 sec.\n",
      "iteration 3320 || Loss: 5.9616 || 10iter:7.8311 sec.\n",
      "iteration 3330 || Loss: 7.0907 || 10iter:7.8468 sec.\n",
      "iteration 3340 || Loss: 5.9419 || 10iter:7.8242 sec.\n",
      "iteration 3350 || Loss: 6.2575 || 10iter:7.7767 sec.\n",
      "iteration 3360 || Loss: 7.5350 || 10iter:7.9162 sec.\n",
      "iteration 3370 || Loss: 7.7720 || 10iter:7.7927 sec.\n",
      "iteration 3380 || Loss: 9.7675 || 10iter:7.8723 sec.\n",
      "iteration 3390 || Loss: 9.6880 || 10iter:7.8578 sec.\n",
      "iteration 3400 || Loss: 7.8024 || 10iter:7.9685 sec.\n",
      "iteration 3410 || Loss: 7.5078 || 10iter:7.8518 sec.\n",
      "iteration 3420 || Loss: 6.0495 || 10iter:7.8481 sec.\n",
      "iteration 3430 || Loss: 10.0913 || 10iter:7.8260 sec.\n",
      "iteration 3440 || Loss: 6.8369 || 10iter:7.8711 sec.\n",
      "iteration 3450 || Loss: 6.3642 || 10iter:7.8085 sec.\n",
      "iteration 3460 || Loss: 8.4813 || 10iter:7.8659 sec.\n",
      "iteration 3470 || Loss: 7.0397 || 10iter:7.8741 sec.\n",
      "iteration 3480 || Loss: 6.9842 || 10iter:7.8260 sec.\n",
      "iteration 3490 || Loss: 6.8078 || 10iter:7.9216 sec.\n",
      "iteration 3500 || Loss: 11.0875 || 10iter:7.8898 sec.\n",
      "iteration 3510 || Loss: 9.1810 || 10iter:7.8037 sec.\n",
      "iteration 3520 || Loss: 6.2778 || 10iter:7.9079 sec.\n",
      "iteration 3530 || Loss: 9.1756 || 10iter:7.8258 sec.\n",
      "iteration 3540 || Loss: 8.9994 || 10iter:7.9248 sec.\n",
      "iteration 3550 || Loss: 10.2438 || 10iter:7.9761 sec.\n",
      "iteration 3560 || Loss: 11.0345 || 10iter:7.8638 sec.\n",
      "iteration 3570 || Loss: 6.1583 || 10iter:7.8454 sec.\n",
      "-------------------\n",
      "epoch 5 || Epoch_TRAIN_Loss:5747.4032 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 6/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 3580 || Loss: 9.1521 || 10iter:3.7061 sec.\n",
      "iteration 3590 || Loss: 5.4735 || 10iter:7.9323 sec.\n",
      "iteration 3600 || Loss: 7.8120 || 10iter:7.8956 sec.\n",
      "iteration 3610 || Loss: 6.5805 || 10iter:7.7732 sec.\n",
      "iteration 3620 || Loss: 8.3465 || 10iter:7.7460 sec.\n",
      "iteration 3630 || Loss: 8.8585 || 10iter:8.0036 sec.\n",
      "iteration 3640 || Loss: 7.6166 || 10iter:7.8526 sec.\n",
      "iteration 3650 || Loss: 11.9072 || 10iter:7.8939 sec.\n",
      "iteration 3660 || Loss: 6.8816 || 10iter:7.8023 sec.\n",
      "iteration 3670 || Loss: 7.4239 || 10iter:7.8406 sec.\n",
      "iteration 3680 || Loss: 7.7561 || 10iter:7.8295 sec.\n",
      "iteration 3690 || Loss: 7.5541 || 10iter:7.8569 sec.\n",
      "iteration 3700 || Loss: 5.7839 || 10iter:8.0149 sec.\n",
      "iteration 3710 || Loss: 6.5818 || 10iter:7.9081 sec.\n",
      "iteration 3720 || Loss: 7.4748 || 10iter:7.8598 sec.\n",
      "iteration 3730 || Loss: 6.7085 || 10iter:7.8872 sec.\n",
      "iteration 3740 || Loss: 8.0412 || 10iter:7.8408 sec.\n",
      "iteration 3750 || Loss: 6.0749 || 10iter:7.7738 sec.\n",
      "iteration 3760 || Loss: 7.5102 || 10iter:7.8520 sec.\n",
      "iteration 3770 || Loss: 6.5925 || 10iter:7.9350 sec.\n",
      "iteration 3780 || Loss: 9.0466 || 10iter:7.8644 sec.\n",
      "iteration 3790 || Loss: 6.4310 || 10iter:7.7847 sec.\n",
      "iteration 3800 || Loss: 8.4277 || 10iter:7.8040 sec.\n",
      "iteration 3810 || Loss: 7.5056 || 10iter:7.8503 sec.\n",
      "iteration 3820 || Loss: 11.5592 || 10iter:7.9009 sec.\n",
      "iteration 3830 || Loss: 5.9880 || 10iter:7.9007 sec.\n",
      "iteration 3840 || Loss: 6.1086 || 10iter:7.8182 sec.\n",
      "iteration 3850 || Loss: 11.5798 || 10iter:8.0322 sec.\n",
      "iteration 3860 || Loss: 6.1319 || 10iter:7.7668 sec.\n",
      "iteration 3870 || Loss: 5.9075 || 10iter:7.7889 sec.\n",
      "iteration 3880 || Loss: 7.8934 || 10iter:7.8563 sec.\n",
      "iteration 3890 || Loss: 9.6868 || 10iter:7.8215 sec.\n",
      "iteration 3900 || Loss: 8.2399 || 10iter:7.7973 sec.\n",
      "iteration 3910 || Loss: 7.1806 || 10iter:7.9425 sec.\n",
      "iteration 3920 || Loss: 7.4046 || 10iter:7.7948 sec.\n",
      "iteration 3930 || Loss: 8.3510 || 10iter:7.7833 sec.\n",
      "iteration 3940 || Loss: 6.7500 || 10iter:7.8669 sec.\n",
      "iteration 3950 || Loss: 8.1232 || 10iter:7.9375 sec.\n",
      "iteration 3960 || Loss: 6.3799 || 10iter:7.8376 sec.\n",
      "iteration 3970 || Loss: 6.9344 || 10iter:7.7917 sec.\n",
      "iteration 3980 || Loss: 8.9163 || 10iter:7.9718 sec.\n",
      "iteration 3990 || Loss: 12.6024 || 10iter:7.9196 sec.\n",
      "iteration 4000 || Loss: 7.9583 || 10iter:7.8246 sec.\n",
      "iteration 4010 || Loss: 7.0042 || 10iter:7.8087 sec.\n",
      "iteration 4020 || Loss: 6.2396 || 10iter:7.7379 sec.\n",
      "iteration 4030 || Loss: 10.0418 || 10iter:7.9534 sec.\n",
      "iteration 4040 || Loss: 6.0647 || 10iter:7.8521 sec.\n",
      "iteration 4050 || Loss: 5.7942 || 10iter:7.8898 sec.\n",
      "iteration 4060 || Loss: 8.5435 || 10iter:7.8338 sec.\n",
      "iteration 4070 || Loss: 8.5688 || 10iter:7.8587 sec.\n",
      "iteration 4080 || Loss: 8.4230 || 10iter:7.9518 sec.\n",
      "iteration 4090 || Loss: 8.1497 || 10iter:7.8949 sec.\n",
      "iteration 4100 || Loss: 5.5200 || 10iter:7.8480 sec.\n",
      "iteration 4110 || Loss: 9.0897 || 10iter:7.8351 sec.\n",
      "iteration 4120 || Loss: 5.8186 || 10iter:7.8891 sec.\n",
      "iteration 4130 || Loss: 5.9888 || 10iter:7.8290 sec.\n",
      "iteration 4140 || Loss: 8.8673 || 10iter:7.7394 sec.\n",
      "iteration 4150 || Loss: 12.6851 || 10iter:7.8704 sec.\n",
      "iteration 4160 || Loss: 6.4630 || 10iter:7.8168 sec.\n",
      "iteration 4170 || Loss: 7.2904 || 10iter:7.8822 sec.\n",
      "iteration 4180 || Loss: 10.1196 || 10iter:7.9242 sec.\n",
      "iteration 4190 || Loss: 8.5734 || 10iter:7.7702 sec.\n",
      "iteration 4200 || Loss: 7.8588 || 10iter:7.9465 sec.\n",
      "iteration 4210 || Loss: 7.9753 || 10iter:7.9486 sec.\n",
      "iteration 4220 || Loss: 9.5391 || 10iter:7.8224 sec.\n",
      "iteration 4230 || Loss: 7.8958 || 10iter:7.8557 sec.\n",
      "iteration 4240 || Loss: 7.8373 || 10iter:7.8268 sec.\n",
      "iteration 4250 || Loss: 11.1433 || 10iter:7.8336 sec.\n",
      "iteration 4260 || Loss: 9.3898 || 10iter:7.8432 sec.\n",
      "iteration 4270 || Loss: 10.3698 || 10iter:7.8846 sec.\n",
      "iteration 4280 || Loss: 5.3751 || 10iter:7.7690 sec.\n",
      "iteration 4290 || Loss: 7.0155 || 10iter:7.6954 sec.\n",
      "-------------------\n",
      "epoch 6 || Epoch_TRAIN_Loss:5686.3952 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 7/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 4300 || Loss: 10.1706 || 10iter:7.8540 sec.\n",
      "iteration 4310 || Loss: 8.7034 || 10iter:7.7997 sec.\n",
      "iteration 4320 || Loss: 7.2259 || 10iter:7.8638 sec.\n",
      "iteration 4330 || Loss: 7.1172 || 10iter:7.8940 sec.\n",
      "iteration 4340 || Loss: 6.2277 || 10iter:7.9674 sec.\n",
      "iteration 4350 || Loss: 9.2840 || 10iter:8.0042 sec.\n",
      "iteration 4360 || Loss: 6.8559 || 10iter:7.8582 sec.\n",
      "iteration 4370 || Loss: 7.7557 || 10iter:7.8800 sec.\n",
      "iteration 4380 || Loss: 9.1040 || 10iter:7.8110 sec.\n",
      "iteration 4390 || Loss: 6.8384 || 10iter:7.9569 sec.\n",
      "iteration 4400 || Loss: 7.7346 || 10iter:7.9579 sec.\n",
      "iteration 4410 || Loss: 7.8780 || 10iter:7.7681 sec.\n",
      "iteration 4420 || Loss: 8.0017 || 10iter:7.7572 sec.\n",
      "iteration 4430 || Loss: 6.8355 || 10iter:7.8249 sec.\n",
      "iteration 4440 || Loss: 7.4264 || 10iter:7.8137 sec.\n",
      "iteration 4450 || Loss: 6.8151 || 10iter:7.7616 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4460 || Loss: 6.0655 || 10iter:7.9440 sec.\n",
      "iteration 4470 || Loss: 7.5246 || 10iter:7.9462 sec.\n",
      "iteration 4480 || Loss: 7.0708 || 10iter:7.8630 sec.\n",
      "iteration 4490 || Loss: 8.2861 || 10iter:7.9180 sec.\n",
      "iteration 4500 || Loss: 8.7318 || 10iter:7.8754 sec.\n",
      "iteration 4510 || Loss: 7.2961 || 10iter:7.7836 sec.\n",
      "iteration 4520 || Loss: 7.0793 || 10iter:7.7525 sec.\n",
      "iteration 4530 || Loss: 7.1745 || 10iter:7.8311 sec.\n",
      "iteration 4540 || Loss: 6.6808 || 10iter:7.8763 sec.\n",
      "iteration 4550 || Loss: 6.1350 || 10iter:7.8630 sec.\n",
      "iteration 4560 || Loss: 6.9599 || 10iter:7.9714 sec.\n",
      "iteration 4570 || Loss: 5.9796 || 10iter:7.8952 sec.\n",
      "iteration 4580 || Loss: 6.0352 || 10iter:7.8581 sec.\n",
      "iteration 4590 || Loss: 6.7193 || 10iter:7.8452 sec.\n",
      "iteration 4600 || Loss: 6.4083 || 10iter:7.9866 sec.\n",
      "iteration 4610 || Loss: 6.5827 || 10iter:7.8003 sec.\n",
      "iteration 4620 || Loss: 6.7247 || 10iter:7.8533 sec.\n",
      "iteration 4630 || Loss: 6.0411 || 10iter:7.8736 sec.\n",
      "iteration 4640 || Loss: 7.6450 || 10iter:7.9029 sec.\n",
      "iteration 4650 || Loss: 6.7049 || 10iter:7.7975 sec.\n",
      "iteration 4660 || Loss: 8.4192 || 10iter:7.8309 sec.\n",
      "iteration 4670 || Loss: 6.5541 || 10iter:7.8540 sec.\n",
      "iteration 4680 || Loss: 10.7971 || 10iter:7.9329 sec.\n",
      "iteration 4690 || Loss: 6.9318 || 10iter:7.8314 sec.\n",
      "iteration 4700 || Loss: 8.4115 || 10iter:7.7320 sec.\n",
      "iteration 4710 || Loss: 5.7906 || 10iter:7.8799 sec.\n",
      "iteration 4720 || Loss: 6.9626 || 10iter:7.9880 sec.\n",
      "iteration 4730 || Loss: 6.0079 || 10iter:7.8132 sec.\n",
      "iteration 4740 || Loss: 7.2377 || 10iter:7.8117 sec.\n",
      "iteration 4750 || Loss: 8.3186 || 10iter:7.8137 sec.\n",
      "iteration 4760 || Loss: 6.1594 || 10iter:7.8365 sec.\n",
      "iteration 4770 || Loss: 5.6160 || 10iter:7.8576 sec.\n",
      "iteration 4780 || Loss: 6.9243 || 10iter:7.7575 sec.\n",
      "iteration 4790 || Loss: 6.1227 || 10iter:7.7860 sec.\n",
      "iteration 4800 || Loss: 5.7891 || 10iter:7.7976 sec.\n",
      "iteration 4810 || Loss: 6.7283 || 10iter:7.8285 sec.\n",
      "iteration 4820 || Loss: 5.7804 || 10iter:7.7946 sec.\n",
      "iteration 4830 || Loss: 7.2813 || 10iter:7.9226 sec.\n",
      "iteration 4840 || Loss: 7.2238 || 10iter:7.9734 sec.\n",
      "iteration 4850 || Loss: 7.4023 || 10iter:7.8908 sec.\n",
      "iteration 4860 || Loss: 6.0784 || 10iter:7.8473 sec.\n",
      "iteration 4870 || Loss: 7.5085 || 10iter:7.9759 sec.\n",
      "iteration 4880 || Loss: 6.2071 || 10iter:7.8465 sec.\n",
      "iteration 4890 || Loss: 7.6199 || 10iter:7.9235 sec.\n",
      "iteration 4900 || Loss: 6.6222 || 10iter:7.9629 sec.\n",
      "iteration 4910 || Loss: 7.8239 || 10iter:7.8486 sec.\n",
      "iteration 4920 || Loss: 7.1989 || 10iter:7.7866 sec.\n",
      "iteration 4930 || Loss: 7.0754 || 10iter:7.8038 sec.\n",
      "iteration 4940 || Loss: 7.7522 || 10iter:7.7785 sec.\n",
      "iteration 4950 || Loss: 8.0923 || 10iter:7.8853 sec.\n",
      "iteration 4960 || Loss: 6.8487 || 10iter:7.8227 sec.\n",
      "iteration 4970 || Loss: 6.2977 || 10iter:7.9292 sec.\n",
      "iteration 4980 || Loss: 7.5922 || 10iter:8.0060 sec.\n",
      "iteration 4990 || Loss: 4.7580 || 10iter:7.9969 sec.\n",
      "iteration 5000 || Loss: 6.2522 || 10iter:7.9893 sec.\n",
      "-------------------\n",
      "epoch 7 || Epoch_TRAIN_Loss:5228.8125 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 8/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 5010 || Loss: 7.2688 || 10iter:3.8474 sec.\n",
      "iteration 5020 || Loss: 7.9855 || 10iter:7.9539 sec.\n",
      "iteration 5030 || Loss: 7.5097 || 10iter:7.8560 sec.\n",
      "iteration 5040 || Loss: 6.7537 || 10iter:7.8007 sec.\n",
      "iteration 5050 || Loss: 5.7047 || 10iter:7.7907 sec.\n",
      "iteration 5060 || Loss: 6.1843 || 10iter:7.8436 sec.\n",
      "iteration 5070 || Loss: 7.8285 || 10iter:7.9317 sec.\n",
      "iteration 5080 || Loss: 6.7139 || 10iter:7.8847 sec.\n",
      "iteration 5090 || Loss: 5.7014 || 10iter:7.8288 sec.\n",
      "iteration 5100 || Loss: 7.8989 || 10iter:7.9529 sec.\n",
      "iteration 5110 || Loss: 8.3754 || 10iter:7.8566 sec.\n",
      "iteration 5120 || Loss: 9.3678 || 10iter:7.8630 sec.\n",
      "iteration 5130 || Loss: 6.2922 || 10iter:7.7511 sec.\n",
      "iteration 5140 || Loss: 8.8604 || 10iter:7.8079 sec.\n",
      "iteration 5150 || Loss: 6.9000 || 10iter:7.9425 sec.\n",
      "iteration 5160 || Loss: 5.9346 || 10iter:7.8503 sec.\n",
      "iteration 5170 || Loss: 9.0842 || 10iter:7.8196 sec.\n",
      "iteration 5180 || Loss: 8.3349 || 10iter:7.8548 sec.\n",
      "iteration 5190 || Loss: 8.6098 || 10iter:7.8878 sec.\n",
      "iteration 5200 || Loss: 8.4162 || 10iter:7.7839 sec.\n",
      "iteration 5210 || Loss: 7.5228 || 10iter:8.0162 sec.\n",
      "iteration 5220 || Loss: 7.9065 || 10iter:7.8241 sec.\n",
      "iteration 5230 || Loss: 7.3352 || 10iter:7.7243 sec.\n",
      "iteration 5240 || Loss: 7.8684 || 10iter:7.7583 sec.\n",
      "iteration 5250 || Loss: 7.5732 || 10iter:7.8909 sec.\n",
      "iteration 5260 || Loss: 5.8049 || 10iter:7.8646 sec.\n",
      "iteration 5270 || Loss: 6.5240 || 10iter:7.9219 sec.\n",
      "iteration 5280 || Loss: 6.1446 || 10iter:7.8912 sec.\n",
      "iteration 5290 || Loss: 6.3022 || 10iter:7.9603 sec.\n",
      "iteration 5300 || Loss: 6.1411 || 10iter:7.8170 sec.\n",
      "iteration 5310 || Loss: 6.9355 || 10iter:7.8925 sec.\n",
      "iteration 5320 || Loss: 5.4633 || 10iter:7.8735 sec.\n",
      "iteration 5330 || Loss: 7.4661 || 10iter:7.8414 sec.\n",
      "iteration 5340 || Loss: 6.8129 || 10iter:7.9284 sec.\n",
      "iteration 5350 || Loss: 7.2292 || 10iter:7.8883 sec.\n",
      "iteration 5360 || Loss: 6.7280 || 10iter:7.8539 sec.\n",
      "iteration 5370 || Loss: 7.2560 || 10iter:7.8878 sec.\n",
      "iteration 5380 || Loss: 7.7905 || 10iter:7.8840 sec.\n",
      "iteration 5390 || Loss: 5.9933 || 10iter:7.7256 sec.\n",
      "iteration 5400 || Loss: 6.5639 || 10iter:7.8332 sec.\n",
      "iteration 5410 || Loss: 7.8760 || 10iter:7.7758 sec.\n",
      "iteration 5420 || Loss: 8.0252 || 10iter:7.8996 sec.\n",
      "iteration 5430 || Loss: 7.2960 || 10iter:7.9925 sec.\n",
      "iteration 5440 || Loss: 8.7289 || 10iter:7.8873 sec.\n",
      "iteration 5450 || Loss: 6.4793 || 10iter:7.7675 sec.\n",
      "iteration 5460 || Loss: 6.4347 || 10iter:7.8260 sec.\n",
      "iteration 5470 || Loss: 7.0365 || 10iter:7.9327 sec.\n",
      "iteration 5480 || Loss: 6.8136 || 10iter:7.8827 sec.\n",
      "iteration 5490 || Loss: 6.5589 || 10iter:7.8707 sec.\n",
      "iteration 5500 || Loss: 6.3356 || 10iter:7.8909 sec.\n",
      "iteration 5510 || Loss: 8.1637 || 10iter:7.8869 sec.\n",
      "iteration 5520 || Loss: 6.3590 || 10iter:7.8226 sec.\n",
      "iteration 5530 || Loss: 6.7249 || 10iter:7.8447 sec.\n",
      "iteration 5540 || Loss: 5.9557 || 10iter:7.9142 sec.\n",
      "iteration 5550 || Loss: 6.8360 || 10iter:7.8312 sec.\n",
      "iteration 5560 || Loss: 6.0660 || 10iter:7.8377 sec.\n",
      "iteration 5570 || Loss: 6.6090 || 10iter:7.9020 sec.\n",
      "iteration 5580 || Loss: 10.6082 || 10iter:7.7963 sec.\n",
      "iteration 5590 || Loss: 7.4681 || 10iter:7.8394 sec.\n",
      "iteration 5600 || Loss: 5.5680 || 10iter:7.9176 sec.\n",
      "iteration 5610 || Loss: 7.1855 || 10iter:7.8816 sec.\n",
      "iteration 5620 || Loss: 5.9115 || 10iter:7.8981 sec.\n",
      "iteration 5630 || Loss: 6.9565 || 10iter:7.9324 sec.\n",
      "iteration 5640 || Loss: 6.5888 || 10iter:8.0126 sec.\n",
      "iteration 5650 || Loss: 6.6346 || 10iter:7.8352 sec.\n",
      "iteration 5660 || Loss: 6.9338 || 10iter:7.7718 sec.\n",
      "iteration 5670 || Loss: 7.2780 || 10iter:7.7739 sec.\n",
      "iteration 5680 || Loss: 5.1460 || 10iter:7.9144 sec.\n",
      "iteration 5690 || Loss: 6.6847 || 10iter:7.8624 sec.\n",
      "iteration 5700 || Loss: 6.7663 || 10iter:7.9345 sec.\n",
      "iteration 5710 || Loss: 7.4452 || 10iter:7.8330 sec.\n",
      "iteration 5720 || Loss: 5.3275 || 10iter:7.7059 sec.\n",
      "-------------------\n",
      "epoch 8 || Epoch_TRAIN_Loss:5051.7289 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 9/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 5730 || Loss: 8.5524 || 10iter:7.8106 sec.\n",
      "iteration 5740 || Loss: 7.4290 || 10iter:7.9202 sec.\n",
      "iteration 5750 || Loss: 6.8977 || 10iter:7.9063 sec.\n",
      "iteration 5760 || Loss: 6.4947 || 10iter:7.7810 sec.\n",
      "iteration 5770 || Loss: 6.7965 || 10iter:7.8278 sec.\n",
      "iteration 5780 || Loss: 7.0766 || 10iter:7.8913 sec.\n",
      "iteration 5790 || Loss: 7.0036 || 10iter:7.9213 sec.\n",
      "iteration 5800 || Loss: 7.1052 || 10iter:7.8955 sec.\n",
      "iteration 5810 || Loss: 6.9909 || 10iter:7.8356 sec.\n",
      "iteration 5820 || Loss: 8.4121 || 10iter:7.8699 sec.\n",
      "iteration 5830 || Loss: 6.9886 || 10iter:7.8893 sec.\n",
      "iteration 5840 || Loss: 7.5835 || 10iter:7.9437 sec.\n",
      "iteration 5850 || Loss: 6.2347 || 10iter:7.7857 sec.\n",
      "iteration 5860 || Loss: 6.2929 || 10iter:7.7971 sec.\n",
      "iteration 5870 || Loss: 6.6084 || 10iter:7.8669 sec.\n",
      "iteration 5880 || Loss: 5.6222 || 10iter:7.9288 sec.\n",
      "iteration 5890 || Loss: 5.8894 || 10iter:7.8966 sec.\n",
      "iteration 5900 || Loss: 6.2665 || 10iter:7.8001 sec.\n",
      "iteration 5910 || Loss: 6.8592 || 10iter:7.8697 sec.\n",
      "iteration 5920 || Loss: 6.7306 || 10iter:7.8901 sec.\n",
      "iteration 5930 || Loss: 6.7459 || 10iter:7.8314 sec.\n",
      "iteration 5940 || Loss: 6.9545 || 10iter:7.8982 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5950 || Loss: 6.6091 || 10iter:7.8000 sec.\n",
      "iteration 5960 || Loss: 6.3295 || 10iter:7.8808 sec.\n",
      "iteration 5970 || Loss: 6.2572 || 10iter:7.9762 sec.\n",
      "iteration 5980 || Loss: 5.5530 || 10iter:7.8583 sec.\n",
      "iteration 5990 || Loss: 6.0298 || 10iter:7.8700 sec.\n",
      "iteration 6000 || Loss: 7.1839 || 10iter:7.8505 sec.\n",
      "iteration 6010 || Loss: 5.3397 || 10iter:7.8919 sec.\n",
      "iteration 6020 || Loss: 7.2483 || 10iter:7.8508 sec.\n",
      "iteration 6030 || Loss: 5.5321 || 10iter:7.8559 sec.\n",
      "iteration 6040 || Loss: 6.4940 || 10iter:7.7316 sec.\n",
      "iteration 6050 || Loss: 7.8727 || 10iter:7.8447 sec.\n",
      "iteration 6060 || Loss: 5.4328 || 10iter:7.9060 sec.\n",
      "iteration 6070 || Loss: 6.0718 || 10iter:7.7242 sec.\n",
      "iteration 6080 || Loss: 6.2748 || 10iter:7.7754 sec.\n",
      "iteration 6090 || Loss: 5.6395 || 10iter:7.8277 sec.\n",
      "iteration 6100 || Loss: 5.1661 || 10iter:7.9305 sec.\n",
      "iteration 6110 || Loss: 4.8291 || 10iter:7.8224 sec.\n",
      "iteration 6120 || Loss: 5.2806 || 10iter:7.8562 sec.\n",
      "iteration 6130 || Loss: 7.6413 || 10iter:7.8207 sec.\n",
      "iteration 6140 || Loss: 6.9721 || 10iter:7.7664 sec.\n",
      "iteration 6150 || Loss: 6.1895 || 10iter:7.8332 sec.\n",
      "iteration 6160 || Loss: 6.9723 || 10iter:7.8289 sec.\n",
      "iteration 6170 || Loss: 6.3182 || 10iter:7.9094 sec.\n",
      "iteration 6180 || Loss: 6.2910 || 10iter:7.9170 sec.\n",
      "iteration 6190 || Loss: 5.4879 || 10iter:7.8242 sec.\n",
      "iteration 6200 || Loss: 6.2511 || 10iter:7.9092 sec.\n",
      "iteration 6210 || Loss: 6.3011 || 10iter:7.9414 sec.\n",
      "iteration 6220 || Loss: 6.8826 || 10iter:7.9639 sec.\n",
      "iteration 6230 || Loss: 8.4346 || 10iter:7.8787 sec.\n",
      "iteration 6240 || Loss: 7.5431 || 10iter:7.8087 sec.\n",
      "iteration 6250 || Loss: 6.6138 || 10iter:7.7356 sec.\n",
      "iteration 6260 || Loss: 7.6607 || 10iter:7.8617 sec.\n",
      "iteration 6270 || Loss: 7.1284 || 10iter:7.8366 sec.\n",
      "iteration 6280 || Loss: 6.5721 || 10iter:7.8235 sec.\n",
      "iteration 6290 || Loss: 6.0914 || 10iter:7.8906 sec.\n",
      "iteration 6300 || Loss: 6.5728 || 10iter:7.8550 sec.\n",
      "iteration 6310 || Loss: 6.2623 || 10iter:7.7522 sec.\n",
      "iteration 6320 || Loss: 6.1756 || 10iter:7.8292 sec.\n",
      "iteration 6330 || Loss: 6.6256 || 10iter:7.9175 sec.\n",
      "iteration 6340 || Loss: 6.7170 || 10iter:7.8961 sec.\n",
      "iteration 6350 || Loss: 6.7825 || 10iter:7.7996 sec.\n",
      "iteration 6360 || Loss: 8.8715 || 10iter:7.8411 sec.\n",
      "iteration 6370 || Loss: 4.7392 || 10iter:7.9470 sec.\n",
      "iteration 6380 || Loss: 7.0624 || 10iter:7.7797 sec.\n",
      "iteration 6390 || Loss: 6.2813 || 10iter:7.7899 sec.\n",
      "iteration 6400 || Loss: 8.6588 || 10iter:8.0113 sec.\n",
      "iteration 6410 || Loss: 6.2222 || 10iter:7.8181 sec.\n",
      "iteration 6420 || Loss: 7.6643 || 10iter:7.8568 sec.\n",
      "iteration 6430 || Loss: 6.1850 || 10iter:7.8383 sec.\n",
      "-------------------\n",
      "epoch 9 || Epoch_TRAIN_Loss:4773.4739 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 10/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 6440 || Loss: 6.1492 || 10iter:3.7924 sec.\n",
      "iteration 6450 || Loss: 5.1418 || 10iter:7.8714 sec.\n",
      "iteration 6460 || Loss: 7.6161 || 10iter:7.8750 sec.\n",
      "iteration 6470 || Loss: 6.7484 || 10iter:7.7905 sec.\n",
      "iteration 6480 || Loss: 7.0355 || 10iter:7.8456 sec.\n",
      "iteration 6490 || Loss: 6.8644 || 10iter:7.8696 sec.\n",
      "iteration 6500 || Loss: 7.0682 || 10iter:7.8141 sec.\n",
      "iteration 6510 || Loss: 6.2848 || 10iter:7.8020 sec.\n",
      "iteration 6520 || Loss: 6.8941 || 10iter:7.8629 sec.\n",
      "iteration 6530 || Loss: 5.8523 || 10iter:7.8594 sec.\n",
      "iteration 6540 || Loss: 7.4187 || 10iter:7.9668 sec.\n",
      "iteration 6550 || Loss: 5.8423 || 10iter:7.9286 sec.\n",
      "iteration 6560 || Loss: 6.1283 || 10iter:7.9051 sec.\n",
      "iteration 6570 || Loss: 7.0762 || 10iter:7.9511 sec.\n",
      "iteration 6580 || Loss: 6.0709 || 10iter:7.8978 sec.\n",
      "iteration 6590 || Loss: 6.8990 || 10iter:7.9747 sec.\n",
      "iteration 6600 || Loss: 5.9501 || 10iter:7.9879 sec.\n",
      "iteration 6610 || Loss: 6.5710 || 10iter:7.9464 sec.\n",
      "iteration 6620 || Loss: 6.8157 || 10iter:7.9788 sec.\n",
      "iteration 6630 || Loss: 6.9494 || 10iter:7.8810 sec.\n",
      "iteration 6640 || Loss: 6.5135 || 10iter:7.9232 sec.\n",
      "iteration 6650 || Loss: 7.1997 || 10iter:7.8586 sec.\n",
      "iteration 6660 || Loss: 5.4381 || 10iter:7.9049 sec.\n",
      "iteration 6670 || Loss: 7.4458 || 10iter:8.0100 sec.\n",
      "iteration 6680 || Loss: 6.2152 || 10iter:7.7862 sec.\n",
      "iteration 6690 || Loss: 7.4133 || 10iter:7.8253 sec.\n",
      "iteration 6700 || Loss: 5.1821 || 10iter:7.8472 sec.\n",
      "iteration 6710 || Loss: 6.2574 || 10iter:7.8945 sec.\n",
      "iteration 6720 || Loss: 8.5643 || 10iter:7.9851 sec.\n",
      "iteration 6730 || Loss: 6.0040 || 10iter:7.8013 sec.\n",
      "iteration 6740 || Loss: 7.5455 || 10iter:7.9299 sec.\n",
      "iteration 6750 || Loss: 5.4415 || 10iter:7.8042 sec.\n",
      "iteration 6760 || Loss: 7.5220 || 10iter:7.8500 sec.\n",
      "iteration 6770 || Loss: 6.4557 || 10iter:7.9014 sec.\n",
      "iteration 6780 || Loss: 5.1477 || 10iter:7.8813 sec.\n",
      "iteration 6790 || Loss: 6.6084 || 10iter:8.0913 sec.\n",
      "iteration 6800 || Loss: 7.7851 || 10iter:7.9162 sec.\n",
      "iteration 6810 || Loss: 5.8730 || 10iter:7.9227 sec.\n",
      "iteration 6820 || Loss: 6.8949 || 10iter:7.8694 sec.\n",
      "iteration 6830 || Loss: 6.3931 || 10iter:7.9204 sec.\n",
      "iteration 6840 || Loss: 7.2803 || 10iter:7.8562 sec.\n",
      "iteration 6850 || Loss: 6.5483 || 10iter:7.9169 sec.\n",
      "iteration 6860 || Loss: 9.1113 || 10iter:7.8334 sec.\n",
      "iteration 6870 || Loss: 4.5466 || 10iter:7.7739 sec.\n",
      "iteration 6880 || Loss: 6.1054 || 10iter:7.8139 sec.\n",
      "iteration 6890 || Loss: 5.4184 || 10iter:7.8297 sec.\n",
      "iteration 6900 || Loss: 7.5406 || 10iter:7.8921 sec.\n",
      "iteration 6910 || Loss: 7.5147 || 10iter:7.9262 sec.\n",
      "iteration 6920 || Loss: 5.8672 || 10iter:7.8605 sec.\n",
      "iteration 6930 || Loss: 5.8840 || 10iter:7.9073 sec.\n",
      "iteration 6940 || Loss: 6.7293 || 10iter:7.7758 sec.\n",
      "iteration 6950 || Loss: 5.1794 || 10iter:7.8481 sec.\n",
      "iteration 6960 || Loss: 6.6426 || 10iter:7.8883 sec.\n",
      "iteration 6970 || Loss: 8.0587 || 10iter:8.0346 sec.\n",
      "iteration 6980 || Loss: 7.4173 || 10iter:7.8199 sec.\n",
      "iteration 6990 || Loss: 6.5796 || 10iter:7.9715 sec.\n",
      "iteration 7000 || Loss: 7.0721 || 10iter:7.9033 sec.\n",
      "iteration 7010 || Loss: 5.8386 || 10iter:7.9251 sec.\n",
      "iteration 7020 || Loss: 5.8557 || 10iter:7.9555 sec.\n",
      "iteration 7030 || Loss: 6.5051 || 10iter:7.8295 sec.\n",
      "iteration 7040 || Loss: 6.0354 || 10iter:7.8517 sec.\n",
      "iteration 7050 || Loss: 8.0335 || 10iter:7.8365 sec.\n",
      "iteration 7060 || Loss: 4.9946 || 10iter:7.8957 sec.\n",
      "iteration 7070 || Loss: 5.5518 || 10iter:7.7821 sec.\n",
      "iteration 7080 || Loss: 6.3829 || 10iter:7.9205 sec.\n",
      "iteration 7090 || Loss: 5.5328 || 10iter:7.8441 sec.\n",
      "iteration 7100 || Loss: 6.1492 || 10iter:7.8706 sec.\n",
      "iteration 7110 || Loss: 6.3953 || 10iter:7.8567 sec.\n",
      "iteration 7120 || Loss: 6.5270 || 10iter:7.8786 sec.\n",
      "iteration 7130 || Loss: 5.1419 || 10iter:7.8814 sec.\n",
      "iteration 7140 || Loss: 5.8281 || 10iter:7.8233 sec.\n",
      "iteration 7150 || Loss: 6.2531 || 10iter:7.5926 sec.\n",
      "-------------------\n",
      " (val) \n",
      "-------------------\n",
      "epoch 10 || Epoch_TRAIN_Loss:4614.3417 || Epoch_VAL_Loss:4550.4060\n",
      "-------------------\n",
      "Epoch 11/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 7160 || Loss: 5.4096 || 10iter:7.8336 sec.\n",
      "iteration 7170 || Loss: 7.2802 || 10iter:7.8752 sec.\n",
      "iteration 7180 || Loss: 6.5103 || 10iter:7.9039 sec.\n",
      "iteration 7190 || Loss: 5.4716 || 10iter:7.7977 sec.\n",
      "iteration 7200 || Loss: 6.0329 || 10iter:7.9186 sec.\n",
      "iteration 7210 || Loss: 5.6384 || 10iter:7.8532 sec.\n",
      "iteration 7220 || Loss: 6.0187 || 10iter:7.9466 sec.\n",
      "iteration 7230 || Loss: 6.5650 || 10iter:7.9557 sec.\n",
      "iteration 7240 || Loss: 6.7393 || 10iter:7.8427 sec.\n",
      "iteration 7250 || Loss: 5.5975 || 10iter:7.8576 sec.\n",
      "iteration 7260 || Loss: 6.5853 || 10iter:7.7990 sec.\n",
      "iteration 7270 || Loss: 5.6571 || 10iter:7.8737 sec.\n",
      "iteration 7280 || Loss: 6.2248 || 10iter:7.9489 sec.\n",
      "iteration 7290 || Loss: 6.8999 || 10iter:7.9073 sec.\n",
      "iteration 7300 || Loss: 5.9183 || 10iter:7.9356 sec.\n",
      "iteration 7310 || Loss: 6.4047 || 10iter:7.8596 sec.\n",
      "iteration 7320 || Loss: 5.6212 || 10iter:7.8154 sec.\n",
      "iteration 7330 || Loss: 7.0761 || 10iter:7.9472 sec.\n",
      "iteration 7340 || Loss: 7.0063 || 10iter:7.7734 sec.\n",
      "iteration 7350 || Loss: 5.8991 || 10iter:7.9142 sec.\n",
      "iteration 7360 || Loss: 5.8707 || 10iter:7.7908 sec.\n",
      "iteration 7370 || Loss: 5.3861 || 10iter:7.8023 sec.\n",
      "iteration 7380 || Loss: 5.2644 || 10iter:7.9177 sec.\n",
      "iteration 7390 || Loss: 4.7696 || 10iter:7.8976 sec.\n",
      "iteration 7400 || Loss: 5.3332 || 10iter:7.9041 sec.\n",
      "iteration 7410 || Loss: 6.0229 || 10iter:7.9341 sec.\n",
      "iteration 7420 || Loss: 6.5283 || 10iter:7.9089 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7430 || Loss: 5.6898 || 10iter:7.8829 sec.\n",
      "iteration 7440 || Loss: 5.9564 || 10iter:7.9191 sec.\n",
      "iteration 7450 || Loss: 5.7991 || 10iter:7.8485 sec.\n",
      "iteration 7460 || Loss: 5.6913 || 10iter:7.9787 sec.\n",
      "iteration 7470 || Loss: 4.9518 || 10iter:7.8531 sec.\n",
      "iteration 7480 || Loss: 5.5692 || 10iter:7.7846 sec.\n",
      "iteration 7490 || Loss: 5.8146 || 10iter:7.8136 sec.\n",
      "iteration 7500 || Loss: 7.2918 || 10iter:7.7594 sec.\n",
      "iteration 7510 || Loss: 5.3724 || 10iter:7.9430 sec.\n",
      "iteration 7520 || Loss: 5.8719 || 10iter:7.9501 sec.\n",
      "iteration 7530 || Loss: 7.1959 || 10iter:7.9535 sec.\n",
      "iteration 7540 || Loss: 6.2414 || 10iter:7.9816 sec.\n",
      "iteration 7550 || Loss: 5.9936 || 10iter:7.9251 sec.\n",
      "iteration 7560 || Loss: 5.7473 || 10iter:7.8961 sec.\n",
      "iteration 7570 || Loss: 6.6906 || 10iter:7.8989 sec.\n",
      "iteration 7580 || Loss: 6.4399 || 10iter:7.9877 sec.\n",
      "iteration 7590 || Loss: 5.3701 || 10iter:7.9196 sec.\n",
      "iteration 7600 || Loss: 6.8769 || 10iter:7.8842 sec.\n",
      "iteration 7610 || Loss: 5.3424 || 10iter:7.9392 sec.\n",
      "iteration 7620 || Loss: 7.0512 || 10iter:7.8871 sec.\n",
      "iteration 7630 || Loss: 7.5417 || 10iter:7.9163 sec.\n",
      "iteration 7640 || Loss: 5.4532 || 10iter:7.8673 sec.\n",
      "iteration 7650 || Loss: 5.9421 || 10iter:7.8880 sec.\n",
      "iteration 7660 || Loss: 6.3605 || 10iter:7.7793 sec.\n",
      "iteration 7670 || Loss: 6.1566 || 10iter:7.8208 sec.\n",
      "iteration 7680 || Loss: 5.8384 || 10iter:7.8947 sec.\n",
      "iteration 7690 || Loss: 4.5777 || 10iter:7.9043 sec.\n",
      "iteration 7700 || Loss: 5.6475 || 10iter:7.8721 sec.\n",
      "iteration 7710 || Loss: 6.6176 || 10iter:7.9605 sec.\n",
      "iteration 7720 || Loss: 4.9234 || 10iter:7.8408 sec.\n",
      "iteration 7730 || Loss: 6.6982 || 10iter:7.9418 sec.\n",
      "iteration 7740 || Loss: 5.3783 || 10iter:7.8454 sec.\n",
      "iteration 7750 || Loss: 7.2840 || 10iter:7.7942 sec.\n",
      "iteration 7760 || Loss: 6.1782 || 10iter:7.7965 sec.\n",
      "iteration 7770 || Loss: 6.4730 || 10iter:7.8953 sec.\n",
      "iteration 7780 || Loss: 6.1801 || 10iter:7.7909 sec.\n",
      "iteration 7790 || Loss: 6.8720 || 10iter:7.8861 sec.\n",
      "iteration 7800 || Loss: 6.3047 || 10iter:7.7619 sec.\n",
      "iteration 7810 || Loss: 5.7827 || 10iter:7.8734 sec.\n",
      "iteration 7820 || Loss: 6.0917 || 10iter:7.8212 sec.\n",
      "iteration 7830 || Loss: 6.0434 || 10iter:7.7741 sec.\n",
      "iteration 7840 || Loss: 6.5259 || 10iter:7.8582 sec.\n",
      "iteration 7850 || Loss: 6.1583 || 10iter:7.7936 sec.\n",
      "iteration 7860 || Loss: 5.6666 || 10iter:7.8338 sec.\n",
      "-------------------\n",
      "epoch 11 || Epoch_TRAIN_Loss:4407.3715 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 12/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 7870 || Loss: 6.2594 || 10iter:3.7432 sec.\n",
      "iteration 7880 || Loss: 6.2568 || 10iter:7.8557 sec.\n",
      "iteration 7890 || Loss: 5.8649 || 10iter:7.8438 sec.\n",
      "iteration 7900 || Loss: 6.0431 || 10iter:7.8158 sec.\n",
      "iteration 7910 || Loss: 6.0935 || 10iter:7.9142 sec.\n",
      "iteration 7920 || Loss: 6.8514 || 10iter:7.7820 sec.\n",
      "iteration 7930 || Loss: 5.6748 || 10iter:7.9069 sec.\n",
      "iteration 7940 || Loss: 7.4868 || 10iter:7.8734 sec.\n",
      "iteration 7950 || Loss: 5.5837 || 10iter:8.0328 sec.\n",
      "iteration 7960 || Loss: 6.8249 || 10iter:7.9727 sec.\n",
      "iteration 7970 || Loss: 6.7830 || 10iter:7.9553 sec.\n",
      "iteration 7980 || Loss: 5.5565 || 10iter:7.9739 sec.\n",
      "iteration 7990 || Loss: 5.2269 || 10iter:7.7604 sec.\n",
      "iteration 8000 || Loss: 6.9846 || 10iter:7.9022 sec.\n",
      "iteration 8010 || Loss: 7.1604 || 10iter:7.8232 sec.\n",
      "iteration 8020 || Loss: 6.1268 || 10iter:7.8878 sec.\n",
      "iteration 8030 || Loss: 6.3685 || 10iter:7.9363 sec.\n",
      "iteration 8040 || Loss: 6.3777 || 10iter:7.8758 sec.\n",
      "iteration 8050 || Loss: 5.5553 || 10iter:7.8157 sec.\n",
      "iteration 8060 || Loss: 5.8264 || 10iter:7.8685 sec.\n",
      "iteration 8070 || Loss: 5.8210 || 10iter:7.9452 sec.\n",
      "iteration 8080 || Loss: 7.0487 || 10iter:7.9197 sec.\n",
      "iteration 8090 || Loss: 5.3932 || 10iter:7.8980 sec.\n",
      "iteration 8100 || Loss: 6.7780 || 10iter:7.8473 sec.\n",
      "iteration 8110 || Loss: 6.4841 || 10iter:8.0890 sec.\n",
      "iteration 8120 || Loss: 8.5252 || 10iter:7.8286 sec.\n",
      "iteration 8130 || Loss: 6.0057 || 10iter:7.9056 sec.\n",
      "iteration 8140 || Loss: 5.8006 || 10iter:7.8510 sec.\n",
      "iteration 8150 || Loss: 6.5275 || 10iter:7.8340 sec.\n",
      "iteration 8160 || Loss: 6.6298 || 10iter:7.8510 sec.\n",
      "iteration 8170 || Loss: 5.2907 || 10iter:7.9305 sec.\n",
      "iteration 8180 || Loss: 6.3913 || 10iter:7.8806 sec.\n",
      "iteration 8190 || Loss: 6.1607 || 10iter:7.9378 sec.\n",
      "iteration 8200 || Loss: 5.8303 || 10iter:7.8047 sec.\n",
      "iteration 8210 || Loss: 5.8058 || 10iter:7.8400 sec.\n",
      "iteration 8220 || Loss: 6.7931 || 10iter:7.9162 sec.\n",
      "iteration 8230 || Loss: 6.2473 || 10iter:7.9731 sec.\n",
      "iteration 8240 || Loss: 6.3820 || 10iter:7.8586 sec.\n",
      "iteration 8250 || Loss: 5.5224 || 10iter:7.8512 sec.\n",
      "iteration 8260 || Loss: 5.5435 || 10iter:7.8811 sec.\n",
      "iteration 8270 || Loss: 5.8240 || 10iter:7.8408 sec.\n",
      "iteration 8280 || Loss: 5.7280 || 10iter:7.8631 sec.\n",
      "iteration 8290 || Loss: 5.5890 || 10iter:7.8650 sec.\n",
      "iteration 8300 || Loss: 6.5440 || 10iter:7.9083 sec.\n",
      "iteration 8310 || Loss: 5.9083 || 10iter:7.8780 sec.\n",
      "iteration 8320 || Loss: 5.9791 || 10iter:7.7979 sec.\n",
      "iteration 8330 || Loss: 4.8548 || 10iter:7.8504 sec.\n",
      "iteration 8340 || Loss: 5.5372 || 10iter:7.8685 sec.\n",
      "iteration 8350 || Loss: 6.3165 || 10iter:7.9897 sec.\n",
      "iteration 8360 || Loss: 5.9229 || 10iter:7.8334 sec.\n",
      "iteration 8370 || Loss: 6.1139 || 10iter:7.9014 sec.\n",
      "iteration 8380 || Loss: 5.6305 || 10iter:7.9610 sec.\n",
      "iteration 8390 || Loss: 6.9259 || 10iter:7.8662 sec.\n",
      "iteration 8400 || Loss: 5.6587 || 10iter:7.9197 sec.\n",
      "iteration 8410 || Loss: 6.0435 || 10iter:7.8529 sec.\n",
      "iteration 8420 || Loss: 5.4730 || 10iter:7.9533 sec.\n",
      "iteration 8430 || Loss: 6.0852 || 10iter:7.7419 sec.\n",
      "iteration 8440 || Loss: 4.4859 || 10iter:7.7890 sec.\n",
      "iteration 8450 || Loss: 6.1393 || 10iter:7.7471 sec.\n",
      "iteration 8460 || Loss: 6.2621 || 10iter:7.8824 sec.\n",
      "iteration 8470 || Loss: 7.0806 || 10iter:7.9484 sec.\n",
      "iteration 8480 || Loss: 6.1921 || 10iter:7.9082 sec.\n",
      "iteration 8490 || Loss: 5.6363 || 10iter:7.7638 sec.\n",
      "iteration 8500 || Loss: 7.1188 || 10iter:7.7518 sec.\n",
      "iteration 8510 || Loss: 5.8994 || 10iter:7.8533 sec.\n",
      "iteration 8520 || Loss: 5.7946 || 10iter:7.8768 sec.\n",
      "iteration 8530 || Loss: 6.8794 || 10iter:7.8147 sec.\n",
      "iteration 8540 || Loss: 7.3303 || 10iter:7.9562 sec.\n",
      "iteration 8550 || Loss: 4.7678 || 10iter:7.8532 sec.\n",
      "iteration 8560 || Loss: 4.7622 || 10iter:7.8492 sec.\n",
      "iteration 8570 || Loss: 5.1705 || 10iter:7.9372 sec.\n",
      "iteration 8580 || Loss: 7.1700 || 10iter:7.8223 sec.\n",
      "-------------------\n",
      "epoch 12 || Epoch_TRAIN_Loss:4304.9135 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 13/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 8590 || Loss: 5.8240 || 10iter:7.7741 sec.\n",
      "iteration 8600 || Loss: 5.8218 || 10iter:8.0098 sec.\n",
      "iteration 8610 || Loss: 6.2851 || 10iter:7.9646 sec.\n",
      "iteration 8620 || Loss: 6.0854 || 10iter:7.8350 sec.\n",
      "iteration 8630 || Loss: 5.2617 || 10iter:7.8703 sec.\n",
      "iteration 8640 || Loss: 6.3281 || 10iter:7.8911 sec.\n",
      "iteration 8650 || Loss: 6.5371 || 10iter:7.8389 sec.\n",
      "iteration 8660 || Loss: 6.5657 || 10iter:7.9196 sec.\n",
      "iteration 8670 || Loss: 5.5912 || 10iter:7.7832 sec.\n",
      "iteration 8680 || Loss: 5.9881 || 10iter:7.7843 sec.\n",
      "iteration 8690 || Loss: 4.5569 || 10iter:7.7764 sec.\n",
      "iteration 8700 || Loss: 5.4114 || 10iter:7.7381 sec.\n",
      "iteration 8710 || Loss: 5.3246 || 10iter:8.0036 sec.\n",
      "iteration 8720 || Loss: 6.9811 || 10iter:7.8715 sec.\n",
      "iteration 8730 || Loss: 5.5271 || 10iter:7.9546 sec.\n",
      "iteration 8740 || Loss: 5.8946 || 10iter:7.9251 sec.\n",
      "iteration 8750 || Loss: 5.4146 || 10iter:7.8263 sec.\n",
      "iteration 8760 || Loss: 4.2136 || 10iter:7.8374 sec.\n",
      "iteration 8770 || Loss: 5.8534 || 10iter:7.9100 sec.\n",
      "iteration 8780 || Loss: 5.3091 || 10iter:7.7853 sec.\n",
      "iteration 8790 || Loss: 4.2821 || 10iter:7.8116 sec.\n",
      "iteration 8800 || Loss: 5.5059 || 10iter:7.8394 sec.\n",
      "iteration 8810 || Loss: 6.3630 || 10iter:7.8902 sec.\n",
      "iteration 8820 || Loss: 6.7969 || 10iter:7.7875 sec.\n",
      "iteration 8830 || Loss: 5.6339 || 10iter:7.8567 sec.\n",
      "iteration 8840 || Loss: 7.3825 || 10iter:7.8839 sec.\n",
      "iteration 8850 || Loss: 5.6909 || 10iter:7.7877 sec.\n",
      "iteration 8860 || Loss: 6.3359 || 10iter:7.9030 sec.\n",
      "iteration 8870 || Loss: 5.2319 || 10iter:7.8624 sec.\n",
      "iteration 8880 || Loss: 5.3746 || 10iter:7.9745 sec.\n",
      "iteration 8890 || Loss: 5.9143 || 10iter:7.9085 sec.\n",
      "iteration 8900 || Loss: 5.3847 || 10iter:7.9022 sec.\n",
      "iteration 8910 || Loss: 7.1212 || 10iter:7.7459 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8920 || Loss: 4.7686 || 10iter:7.8349 sec.\n",
      "iteration 8930 || Loss: 5.8658 || 10iter:7.9068 sec.\n",
      "iteration 8940 || Loss: 5.8775 || 10iter:7.8379 sec.\n",
      "iteration 8950 || Loss: 6.3655 || 10iter:7.8975 sec.\n",
      "iteration 8960 || Loss: 4.3361 || 10iter:7.9611 sec.\n",
      "iteration 8970 || Loss: 6.7494 || 10iter:8.0214 sec.\n",
      "iteration 8980 || Loss: 6.0154 || 10iter:7.9646 sec.\n",
      "iteration 8990 || Loss: 7.0313 || 10iter:7.7731 sec.\n",
      "iteration 9000 || Loss: 4.8700 || 10iter:7.8002 sec.\n",
      "iteration 9010 || Loss: 6.7328 || 10iter:7.9028 sec.\n",
      "iteration 9020 || Loss: 6.2732 || 10iter:7.8599 sec.\n",
      "iteration 9030 || Loss: 8.0023 || 10iter:7.8644 sec.\n",
      "iteration 9040 || Loss: 5.7469 || 10iter:7.9703 sec.\n",
      "iteration 9050 || Loss: 6.2861 || 10iter:7.8972 sec.\n",
      "iteration 9060 || Loss: 6.0497 || 10iter:7.8365 sec.\n",
      "iteration 9070 || Loss: 6.2982 || 10iter:7.7494 sec.\n",
      "iteration 9080 || Loss: 6.1102 || 10iter:7.8248 sec.\n",
      "iteration 9090 || Loss: 5.5262 || 10iter:7.9625 sec.\n",
      "iteration 9100 || Loss: 6.0190 || 10iter:7.8532 sec.\n",
      "iteration 9110 || Loss: 4.9938 || 10iter:7.9311 sec.\n",
      "iteration 9120 || Loss: 6.1033 || 10iter:7.7747 sec.\n",
      "iteration 9130 || Loss: 5.3160 || 10iter:7.8767 sec.\n",
      "iteration 9140 || Loss: 6.8016 || 10iter:7.8874 sec.\n",
      "iteration 9150 || Loss: 5.8215 || 10iter:7.8355 sec.\n",
      "iteration 9160 || Loss: 5.4910 || 10iter:7.9544 sec.\n",
      "iteration 9170 || Loss: 5.6389 || 10iter:7.8290 sec.\n",
      "iteration 9180 || Loss: 5.8128 || 10iter:7.7916 sec.\n",
      "iteration 9190 || Loss: 6.2765 || 10iter:7.8368 sec.\n",
      "iteration 9200 || Loss: 5.9607 || 10iter:7.8513 sec.\n",
      "iteration 9210 || Loss: 6.7174 || 10iter:7.9481 sec.\n",
      "iteration 9220 || Loss: 5.1719 || 10iter:7.8149 sec.\n",
      "iteration 9230 || Loss: 6.6000 || 10iter:7.8550 sec.\n",
      "iteration 9240 || Loss: 5.3039 || 10iter:7.8232 sec.\n",
      "iteration 9250 || Loss: 5.6709 || 10iter:7.8575 sec.\n",
      "iteration 9260 || Loss: 5.5686 || 10iter:7.8838 sec.\n",
      "iteration 9270 || Loss: 5.8889 || 10iter:7.7477 sec.\n",
      "iteration 9280 || Loss: 5.9120 || 10iter:7.7890 sec.\n",
      "iteration 9290 || Loss: 5.1943 || 10iter:7.8532 sec.\n",
      "-------------------\n",
      "epoch 13 || Epoch_TRAIN_Loss:4206.2129 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 14/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 9300 || Loss: 6.4188 || 10iter:3.7199 sec.\n",
      "iteration 9310 || Loss: 6.1525 || 10iter:7.8660 sec.\n",
      "iteration 9320 || Loss: 6.3439 || 10iter:7.7755 sec.\n",
      "iteration 9330 || Loss: 5.4032 || 10iter:7.8204 sec.\n",
      "iteration 9340 || Loss: 6.0110 || 10iter:7.9580 sec.\n",
      "iteration 9350 || Loss: 6.0927 || 10iter:7.8577 sec.\n",
      "iteration 9360 || Loss: 5.4319 || 10iter:7.9389 sec.\n",
      "iteration 9370 || Loss: 4.5272 || 10iter:7.8044 sec.\n",
      "iteration 9380 || Loss: 4.7794 || 10iter:7.7637 sec.\n",
      "iteration 9390 || Loss: 5.0494 || 10iter:7.8478 sec.\n",
      "iteration 9400 || Loss: 6.4137 || 10iter:7.9972 sec.\n",
      "iteration 9410 || Loss: 5.6542 || 10iter:7.8836 sec.\n",
      "iteration 9420 || Loss: 5.8161 || 10iter:7.9398 sec.\n",
      "iteration 9430 || Loss: 5.6166 || 10iter:7.9420 sec.\n",
      "iteration 9440 || Loss: 5.7944 || 10iter:7.8231 sec.\n",
      "iteration 9450 || Loss: 6.4122 || 10iter:7.8303 sec.\n",
      "iteration 9460 || Loss: 6.2411 || 10iter:7.8225 sec.\n",
      "iteration 9470 || Loss: 6.3013 || 10iter:7.9561 sec.\n",
      "iteration 9480 || Loss: 5.7182 || 10iter:7.8781 sec.\n",
      "iteration 9490 || Loss: 6.3062 || 10iter:7.9011 sec.\n",
      "iteration 9500 || Loss: 5.0031 || 10iter:7.8065 sec.\n",
      "iteration 9510 || Loss: 5.8947 || 10iter:7.9349 sec.\n",
      "iteration 9520 || Loss: 5.5410 || 10iter:7.8569 sec.\n",
      "iteration 9530 || Loss: 6.2955 || 10iter:7.9091 sec.\n",
      "iteration 9540 || Loss: 5.4147 || 10iter:7.8243 sec.\n",
      "iteration 9550 || Loss: 6.1952 || 10iter:7.8381 sec.\n",
      "iteration 9560 || Loss: 6.2178 || 10iter:7.8247 sec.\n",
      "iteration 9570 || Loss: 5.2489 || 10iter:7.7629 sec.\n",
      "iteration 9580 || Loss: 6.2713 || 10iter:7.8492 sec.\n",
      "iteration 9590 || Loss: 4.5547 || 10iter:7.9705 sec.\n",
      "iteration 9600 || Loss: 5.0101 || 10iter:7.9271 sec.\n",
      "iteration 9610 || Loss: 5.4696 || 10iter:7.8564 sec.\n",
      "iteration 9620 || Loss: 5.3289 || 10iter:7.8556 sec.\n",
      "iteration 9630 || Loss: 5.6007 || 10iter:7.9550 sec.\n",
      "iteration 9640 || Loss: 4.8688 || 10iter:7.8148 sec.\n",
      "iteration 9650 || Loss: 5.9084 || 10iter:7.9159 sec.\n",
      "iteration 9660 || Loss: 6.2272 || 10iter:7.9246 sec.\n",
      "iteration 9670 || Loss: 6.2164 || 10iter:7.8815 sec.\n",
      "iteration 9680 || Loss: 5.3895 || 10iter:7.8719 sec.\n",
      "iteration 9690 || Loss: 5.4521 || 10iter:7.8809 sec.\n",
      "iteration 9700 || Loss: 5.2921 || 10iter:7.9149 sec.\n",
      "iteration 9710 || Loss: 7.4029 || 10iter:7.8664 sec.\n",
      "iteration 9720 || Loss: 5.5490 || 10iter:7.9508 sec.\n",
      "iteration 9730 || Loss: 6.1178 || 10iter:7.9407 sec.\n",
      "iteration 9740 || Loss: 5.9813 || 10iter:7.8721 sec.\n",
      "iteration 9750 || Loss: 5.5493 || 10iter:7.8959 sec.\n",
      "iteration 9760 || Loss: 6.6691 || 10iter:7.8572 sec.\n",
      "iteration 9770 || Loss: 6.2191 || 10iter:7.8688 sec.\n",
      "iteration 9780 || Loss: 4.6895 || 10iter:7.8786 sec.\n",
      "iteration 9790 || Loss: 5.0069 || 10iter:7.9202 sec.\n",
      "iteration 9800 || Loss: 4.7093 || 10iter:7.9049 sec.\n",
      "iteration 9810 || Loss: 6.1148 || 10iter:7.8696 sec.\n",
      "iteration 9820 || Loss: 5.1720 || 10iter:8.0117 sec.\n",
      "iteration 9830 || Loss: 5.6149 || 10iter:7.8505 sec.\n",
      "iteration 9840 || Loss: 6.4349 || 10iter:7.9825 sec.\n",
      "iteration 9850 || Loss: 7.7896 || 10iter:7.9811 sec.\n",
      "iteration 9860 || Loss: 5.3479 || 10iter:7.8599 sec.\n",
      "iteration 9870 || Loss: 6.0853 || 10iter:7.9462 sec.\n",
      "iteration 9880 || Loss: 6.0619 || 10iter:7.9819 sec.\n",
      "iteration 9890 || Loss: 6.4521 || 10iter:7.8915 sec.\n",
      "iteration 9900 || Loss: 5.1828 || 10iter:7.8902 sec.\n",
      "iteration 9910 || Loss: 6.0645 || 10iter:7.8924 sec.\n",
      "iteration 9920 || Loss: 5.7645 || 10iter:7.9297 sec.\n",
      "iteration 9930 || Loss: 5.8507 || 10iter:7.9843 sec.\n",
      "iteration 9940 || Loss: 6.5990 || 10iter:7.9228 sec.\n",
      "iteration 9950 || Loss: 6.2072 || 10iter:7.8132 sec.\n",
      "iteration 9960 || Loss: 5.3345 || 10iter:7.8756 sec.\n",
      "iteration 9970 || Loss: 6.1564 || 10iter:7.9005 sec.\n",
      "iteration 9980 || Loss: 6.2025 || 10iter:7.8131 sec.\n",
      "iteration 9990 || Loss: 4.2845 || 10iter:7.8997 sec.\n",
      "iteration 10000 || Loss: 5.7403 || 10iter:7.9455 sec.\n",
      "iteration 10010 || Loss: 6.9807 || 10iter:7.7994 sec.\n",
      "-------------------\n",
      "epoch 14 || Epoch_TRAIN_Loss:4099.7994 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 15/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 10020 || Loss: 6.1816 || 10iter:7.8553 sec.\n",
      "iteration 10030 || Loss: 5.5954 || 10iter:7.8523 sec.\n",
      "iteration 10040 || Loss: 6.9949 || 10iter:7.9918 sec.\n",
      "iteration 10050 || Loss: 5.3935 || 10iter:7.8407 sec.\n",
      "iteration 10060 || Loss: 6.7674 || 10iter:7.9722 sec.\n",
      "iteration 10070 || Loss: 5.6223 || 10iter:8.0217 sec.\n",
      "iteration 10080 || Loss: 4.7534 || 10iter:7.9474 sec.\n",
      "iteration 10090 || Loss: 5.7149 || 10iter:7.9855 sec.\n",
      "iteration 10100 || Loss: 5.4574 || 10iter:7.8720 sec.\n",
      "iteration 10110 || Loss: 5.4270 || 10iter:7.7903 sec.\n",
      "iteration 10120 || Loss: 6.6172 || 10iter:7.8941 sec.\n",
      "iteration 10130 || Loss: 5.1349 || 10iter:7.8591 sec.\n",
      "iteration 10140 || Loss: 6.3414 || 10iter:7.8403 sec.\n",
      "iteration 10150 || Loss: 4.7521 || 10iter:7.8975 sec.\n",
      "iteration 10160 || Loss: 5.0213 || 10iter:7.9934 sec.\n",
      "iteration 10170 || Loss: 6.1168 || 10iter:7.9413 sec.\n",
      "iteration 10180 || Loss: 4.6398 || 10iter:7.8390 sec.\n",
      "iteration 10190 || Loss: 6.1517 || 10iter:7.8577 sec.\n",
      "iteration 10200 || Loss: 4.8663 || 10iter:7.9426 sec.\n",
      "iteration 10210 || Loss: 4.6579 || 10iter:7.8454 sec.\n",
      "iteration 10220 || Loss: 5.3969 || 10iter:7.8257 sec.\n",
      "iteration 10230 || Loss: 5.7085 || 10iter:7.7518 sec.\n",
      "iteration 10240 || Loss: 4.4257 || 10iter:7.8999 sec.\n",
      "iteration 10250 || Loss: 5.5423 || 10iter:7.7850 sec.\n",
      "iteration 10260 || Loss: 5.1843 || 10iter:7.8263 sec.\n",
      "iteration 10270 || Loss: 5.4163 || 10iter:7.8440 sec.\n",
      "iteration 10280 || Loss: 5.7980 || 10iter:7.8693 sec.\n",
      "iteration 10290 || Loss: 6.1429 || 10iter:7.8990 sec.\n",
      "iteration 10300 || Loss: 5.1936 || 10iter:7.7757 sec.\n",
      "iteration 10310 || Loss: 5.9631 || 10iter:7.9260 sec.\n",
      "iteration 10320 || Loss: 5.6231 || 10iter:7.8630 sec.\n",
      "iteration 10330 || Loss: 6.3709 || 10iter:7.9869 sec.\n",
      "iteration 10340 || Loss: 4.7977 || 10iter:7.8317 sec.\n",
      "iteration 10350 || Loss: 3.9872 || 10iter:7.8605 sec.\n",
      "iteration 10360 || Loss: 5.4067 || 10iter:7.9452 sec.\n",
      "iteration 10370 || Loss: 5.2350 || 10iter:7.9249 sec.\n",
      "iteration 10380 || Loss: 4.9054 || 10iter:7.8291 sec.\n",
      "iteration 10390 || Loss: 5.5154 || 10iter:7.8669 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10400 || Loss: 5.8411 || 10iter:7.9178 sec.\n",
      "iteration 10410 || Loss: 6.0003 || 10iter:7.7887 sec.\n",
      "iteration 10420 || Loss: 5.6101 || 10iter:7.9565 sec.\n",
      "iteration 10430 || Loss: 5.3410 || 10iter:7.8057 sec.\n",
      "iteration 10440 || Loss: 4.5810 || 10iter:7.8737 sec.\n",
      "iteration 10450 || Loss: 6.1474 || 10iter:7.8924 sec.\n",
      "iteration 10460 || Loss: 4.7376 || 10iter:7.9575 sec.\n",
      "iteration 10470 || Loss: 7.8178 || 10iter:7.8351 sec.\n",
      "iteration 10480 || Loss: 5.6750 || 10iter:7.8086 sec.\n",
      "iteration 10490 || Loss: 5.2591 || 10iter:7.7707 sec.\n",
      "iteration 10500 || Loss: 5.0169 || 10iter:7.8794 sec.\n",
      "iteration 10510 || Loss: 4.8718 || 10iter:7.8127 sec.\n",
      "iteration 10520 || Loss: 4.7549 || 10iter:8.0444 sec.\n",
      "iteration 10530 || Loss: 5.6662 || 10iter:7.9557 sec.\n",
      "iteration 10540 || Loss: 5.0258 || 10iter:7.9056 sec.\n",
      "iteration 10550 || Loss: 4.6254 || 10iter:7.9490 sec.\n",
      "iteration 10560 || Loss: 5.9840 || 10iter:7.8090 sec.\n",
      "iteration 10570 || Loss: 4.4983 || 10iter:7.9177 sec.\n",
      "iteration 10580 || Loss: 5.9609 || 10iter:7.9086 sec.\n",
      "iteration 10590 || Loss: 5.8397 || 10iter:7.8619 sec.\n",
      "iteration 10600 || Loss: 5.5182 || 10iter:7.8931 sec.\n",
      "iteration 10610 || Loss: 5.9272 || 10iter:7.8101 sec.\n",
      "iteration 10620 || Loss: 5.4482 || 10iter:7.8087 sec.\n",
      "iteration 10630 || Loss: 4.2477 || 10iter:7.9158 sec.\n",
      "iteration 10640 || Loss: 5.7577 || 10iter:7.8445 sec.\n",
      "iteration 10650 || Loss: 4.9464 || 10iter:7.9770 sec.\n",
      "iteration 10660 || Loss: 4.8681 || 10iter:7.8715 sec.\n",
      "iteration 10670 || Loss: 5.9609 || 10iter:7.8342 sec.\n",
      "iteration 10680 || Loss: 6.7904 || 10iter:7.8063 sec.\n",
      "iteration 10690 || Loss: 6.2689 || 10iter:7.8367 sec.\n",
      "iteration 10700 || Loss: 4.3122 || 10iter:7.8380 sec.\n",
      "iteration 10710 || Loss: 5.6238 || 10iter:7.9932 sec.\n",
      "iteration 10720 || Loss: 4.7001 || 10iter:7.8370 sec.\n",
      "-------------------\n",
      "epoch 15 || Epoch_TRAIN_Loss:4007.2942 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 16/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 10730 || Loss: 5.0797 || 10iter:3.7701 sec.\n",
      "iteration 10740 || Loss: 6.0820 || 10iter:7.7337 sec.\n",
      "iteration 10750 || Loss: 5.4210 || 10iter:7.8842 sec.\n",
      "iteration 10760 || Loss: 4.7059 || 10iter:7.7991 sec.\n",
      "iteration 10770 || Loss: 4.7727 || 10iter:7.9087 sec.\n",
      "iteration 10780 || Loss: 5.3277 || 10iter:7.8800 sec.\n",
      "iteration 10790 || Loss: 5.9023 || 10iter:7.9392 sec.\n",
      "iteration 10800 || Loss: 4.7029 || 10iter:7.8972 sec.\n",
      "iteration 10810 || Loss: 6.4703 || 10iter:7.9346 sec.\n",
      "iteration 10820 || Loss: 5.6479 || 10iter:7.8886 sec.\n",
      "iteration 10830 || Loss: 5.9384 || 10iter:7.7739 sec.\n",
      "iteration 10840 || Loss: 5.1255 || 10iter:7.9576 sec.\n",
      "iteration 10850 || Loss: 5.0963 || 10iter:7.8420 sec.\n",
      "iteration 10860 || Loss: 5.6310 || 10iter:7.9575 sec.\n",
      "iteration 10870 || Loss: 5.7811 || 10iter:7.8215 sec.\n",
      "iteration 10880 || Loss: 4.5643 || 10iter:7.8713 sec.\n",
      "iteration 10890 || Loss: 5.3502 || 10iter:7.8328 sec.\n",
      "iteration 10900 || Loss: 6.4598 || 10iter:7.8645 sec.\n",
      "iteration 10910 || Loss: 6.1736 || 10iter:7.8803 sec.\n",
      "iteration 10920 || Loss: 4.6376 || 10iter:7.8711 sec.\n",
      "iteration 10930 || Loss: 6.0569 || 10iter:8.0351 sec.\n",
      "iteration 10940 || Loss: 7.1377 || 10iter:7.9092 sec.\n",
      "iteration 10950 || Loss: 4.5260 || 10iter:7.8483 sec.\n",
      "iteration 10960 || Loss: 5.5075 || 10iter:7.8134 sec.\n",
      "iteration 10970 || Loss: 5.7128 || 10iter:8.0157 sec.\n",
      "iteration 10980 || Loss: 5.9787 || 10iter:7.8431 sec.\n",
      "iteration 10990 || Loss: 4.8467 || 10iter:7.8406 sec.\n",
      "iteration 11000 || Loss: 5.4175 || 10iter:7.8685 sec.\n",
      "iteration 11010 || Loss: 5.1962 || 10iter:7.9457 sec.\n",
      "iteration 11020 || Loss: 4.9450 || 10iter:7.8136 sec.\n",
      "iteration 11030 || Loss: 4.6897 || 10iter:7.7738 sec.\n",
      "iteration 11040 || Loss: 5.5891 || 10iter:7.8522 sec.\n",
      "iteration 11050 || Loss: 6.8448 || 10iter:7.7656 sec.\n",
      "iteration 11060 || Loss: 5.5943 || 10iter:7.8292 sec.\n",
      "iteration 11070 || Loss: 5.1380 || 10iter:7.9289 sec.\n",
      "iteration 11080 || Loss: 6.2684 || 10iter:7.8920 sec.\n",
      "iteration 11090 || Loss: 6.7205 || 10iter:7.7758 sec.\n",
      "iteration 11100 || Loss: 5.6508 || 10iter:7.8742 sec.\n",
      "iteration 11110 || Loss: 5.7310 || 10iter:7.8733 sec.\n",
      "iteration 11120 || Loss: 6.1401 || 10iter:7.8519 sec.\n",
      "iteration 11130 || Loss: 5.9772 || 10iter:7.8925 sec.\n",
      "iteration 11140 || Loss: 5.3331 || 10iter:7.9271 sec.\n",
      "iteration 11150 || Loss: 6.0411 || 10iter:7.8383 sec.\n",
      "iteration 11160 || Loss: 5.9936 || 10iter:7.7569 sec.\n",
      "iteration 11170 || Loss: 5.5642 || 10iter:7.8371 sec.\n",
      "iteration 11180 || Loss: 5.8398 || 10iter:7.8830 sec.\n",
      "iteration 11190 || Loss: 6.0253 || 10iter:7.9233 sec.\n",
      "iteration 11200 || Loss: 4.5832 || 10iter:7.8154 sec.\n",
      "iteration 11210 || Loss: 5.5380 || 10iter:7.9173 sec.\n",
      "iteration 11220 || Loss: 4.7983 || 10iter:7.8021 sec.\n",
      "iteration 11230 || Loss: 5.3149 || 10iter:7.8980 sec.\n",
      "iteration 11240 || Loss: 4.2950 || 10iter:7.8878 sec.\n",
      "iteration 11250 || Loss: 4.7586 || 10iter:7.8754 sec.\n",
      "iteration 11260 || Loss: 4.6706 || 10iter:7.9717 sec.\n",
      "iteration 11270 || Loss: 4.9528 || 10iter:7.8656 sec.\n",
      "iteration 11280 || Loss: 5.3533 || 10iter:7.8255 sec.\n",
      "iteration 11290 || Loss: 5.7028 || 10iter:7.9661 sec.\n",
      "iteration 11300 || Loss: 4.9144 || 10iter:7.8742 sec.\n",
      "iteration 11310 || Loss: 5.1956 || 10iter:7.9284 sec.\n",
      "iteration 11320 || Loss: 4.5324 || 10iter:7.8351 sec.\n",
      "iteration 11330 || Loss: 6.1897 || 10iter:7.8612 sec.\n",
      "iteration 11340 || Loss: 5.3403 || 10iter:7.8753 sec.\n",
      "iteration 11350 || Loss: 4.3007 || 10iter:7.9613 sec.\n",
      "iteration 11360 || Loss: 6.2565 || 10iter:7.8543 sec.\n",
      "iteration 11370 || Loss: 5.6681 || 10iter:7.9096 sec.\n",
      "iteration 11380 || Loss: 4.9755 || 10iter:7.9196 sec.\n",
      "iteration 11390 || Loss: 6.5533 || 10iter:7.8575 sec.\n",
      "iteration 11400 || Loss: 5.8667 || 10iter:7.7626 sec.\n",
      "iteration 11410 || Loss: 5.3982 || 10iter:7.9630 sec.\n",
      "iteration 11420 || Loss: 6.0529 || 10iter:7.9372 sec.\n",
      "iteration 11430 || Loss: 4.6230 || 10iter:7.7608 sec.\n",
      "iteration 11440 || Loss: 4.9929 || 10iter:7.6594 sec.\n",
      "-------------------\n",
      "epoch 16 || Epoch_TRAIN_Loss:3912.6083 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 17/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 11450 || Loss: 5.5409 || 10iter:7.8009 sec.\n",
      "iteration 11460 || Loss: 6.3050 || 10iter:7.8550 sec.\n",
      "iteration 11470 || Loss: 4.9845 || 10iter:7.8947 sec.\n",
      "iteration 11480 || Loss: 4.3401 || 10iter:7.9042 sec.\n",
      "iteration 11490 || Loss: 4.8583 || 10iter:7.9224 sec.\n",
      "iteration 11500 || Loss: 5.2385 || 10iter:7.9487 sec.\n",
      "iteration 11510 || Loss: 5.6473 || 10iter:7.8491 sec.\n",
      "iteration 11520 || Loss: 6.1281 || 10iter:7.7668 sec.\n",
      "iteration 11530 || Loss: 4.4465 || 10iter:7.8534 sec.\n",
      "iteration 11540 || Loss: 5.4462 || 10iter:7.7571 sec.\n",
      "iteration 11550 || Loss: 4.2888 || 10iter:7.8483 sec.\n",
      "iteration 11560 || Loss: 6.2354 || 10iter:7.7631 sec.\n",
      "iteration 11570 || Loss: 5.2163 || 10iter:7.8593 sec.\n",
      "iteration 11580 || Loss: 5.0678 || 10iter:7.8438 sec.\n",
      "iteration 11590 || Loss: 4.1022 || 10iter:7.8324 sec.\n",
      "iteration 11600 || Loss: 5.0176 || 10iter:7.8259 sec.\n",
      "iteration 11610 || Loss: 3.9201 || 10iter:7.7947 sec.\n",
      "iteration 11620 || Loss: 4.8327 || 10iter:7.8568 sec.\n",
      "iteration 11630 || Loss: 5.6828 || 10iter:7.8322 sec.\n",
      "iteration 11640 || Loss: 5.5781 || 10iter:7.8730 sec.\n",
      "iteration 11650 || Loss: 5.7857 || 10iter:7.8963 sec.\n",
      "iteration 11660 || Loss: 4.5685 || 10iter:7.8402 sec.\n",
      "iteration 11670 || Loss: 5.8452 || 10iter:7.8774 sec.\n",
      "iteration 11680 || Loss: 5.2778 || 10iter:7.8521 sec.\n",
      "iteration 11690 || Loss: 5.2941 || 10iter:7.9585 sec.\n",
      "iteration 11700 || Loss: 6.9634 || 10iter:8.0282 sec.\n",
      "iteration 11710 || Loss: 4.9972 || 10iter:7.7918 sec.\n",
      "iteration 11720 || Loss: 5.0628 || 10iter:7.9361 sec.\n",
      "iteration 11730 || Loss: 5.3213 || 10iter:7.8458 sec.\n",
      "iteration 11740 || Loss: 5.4715 || 10iter:7.9921 sec.\n",
      "iteration 11750 || Loss: 4.9035 || 10iter:7.8878 sec.\n",
      "iteration 11760 || Loss: 5.4352 || 10iter:7.9245 sec.\n",
      "iteration 11770 || Loss: 5.4574 || 10iter:7.9385 sec.\n",
      "iteration 11780 || Loss: 5.9720 || 10iter:7.8551 sec.\n",
      "iteration 11790 || Loss: 4.7272 || 10iter:7.7993 sec.\n",
      "iteration 11800 || Loss: 5.0573 || 10iter:7.8738 sec.\n",
      "iteration 11810 || Loss: 4.5529 || 10iter:7.9076 sec.\n",
      "iteration 11820 || Loss: 6.5168 || 10iter:7.9737 sec.\n",
      "iteration 11830 || Loss: 6.5467 || 10iter:7.9826 sec.\n",
      "iteration 11840 || Loss: 6.4799 || 10iter:7.8517 sec.\n",
      "iteration 11850 || Loss: 5.2385 || 10iter:7.7799 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11860 || Loss: 4.4885 || 10iter:7.8595 sec.\n",
      "iteration 11870 || Loss: 4.7477 || 10iter:7.8501 sec.\n",
      "iteration 11880 || Loss: 5.4381 || 10iter:7.8099 sec.\n",
      "iteration 11890 || Loss: 6.0897 || 10iter:7.8986 sec.\n",
      "iteration 11900 || Loss: 6.1897 || 10iter:7.9715 sec.\n",
      "iteration 11910 || Loss: 5.0142 || 10iter:7.8489 sec.\n",
      "iteration 11920 || Loss: 5.9279 || 10iter:7.8709 sec.\n",
      "iteration 11930 || Loss: 4.3580 || 10iter:7.8565 sec.\n",
      "iteration 11940 || Loss: 4.4612 || 10iter:7.7943 sec.\n",
      "iteration 11950 || Loss: 5.4573 || 10iter:7.8569 sec.\n",
      "iteration 11960 || Loss: 4.6470 || 10iter:7.8032 sec.\n",
      "iteration 11970 || Loss: 5.6565 || 10iter:7.7712 sec.\n",
      "iteration 11980 || Loss: 5.2328 || 10iter:7.8011 sec.\n",
      "iteration 11990 || Loss: 5.0861 || 10iter:7.8456 sec.\n",
      "iteration 12000 || Loss: 4.7411 || 10iter:7.8671 sec.\n",
      "iteration 12010 || Loss: 6.5140 || 10iter:7.9722 sec.\n",
      "iteration 12020 || Loss: 5.2929 || 10iter:7.8908 sec.\n",
      "iteration 12030 || Loss: 4.8673 || 10iter:7.7895 sec.\n",
      "iteration 12040 || Loss: 5.9387 || 10iter:7.8124 sec.\n",
      "iteration 12050 || Loss: 5.2181 || 10iter:7.8695 sec.\n",
      "iteration 12060 || Loss: 5.0687 || 10iter:7.9099 sec.\n",
      "iteration 12070 || Loss: 5.9363 || 10iter:7.9028 sec.\n",
      "iteration 12080 || Loss: 4.6795 || 10iter:7.8604 sec.\n",
      "iteration 12090 || Loss: 4.3270 || 10iter:7.7719 sec.\n",
      "iteration 12100 || Loss: 6.5785 || 10iter:7.9215 sec.\n",
      "iteration 12110 || Loss: 5.8008 || 10iter:7.8808 sec.\n",
      "iteration 12120 || Loss: 5.1880 || 10iter:7.9420 sec.\n",
      "iteration 12130 || Loss: 4.8534 || 10iter:7.9284 sec.\n",
      "iteration 12140 || Loss: 4.2326 || 10iter:7.8301 sec.\n",
      "iteration 12150 || Loss: 5.8809 || 10iter:7.8234 sec.\n",
      "-------------------\n",
      "epoch 17 || Epoch_TRAIN_Loss:3831.1446 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 18/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 12160 || Loss: 5.1441 || 10iter:3.7273 sec.\n",
      "iteration 12170 || Loss: 4.6438 || 10iter:7.8976 sec.\n",
      "iteration 12180 || Loss: 4.9462 || 10iter:7.8011 sec.\n",
      "iteration 12190 || Loss: 5.5504 || 10iter:7.8250 sec.\n",
      "iteration 12200 || Loss: 5.2021 || 10iter:7.9217 sec.\n",
      "iteration 12210 || Loss: 5.4082 || 10iter:7.9362 sec.\n",
      "iteration 12220 || Loss: 5.8168 || 10iter:7.8679 sec.\n",
      "iteration 12230 || Loss: 6.2893 || 10iter:7.8809 sec.\n",
      "iteration 12240 || Loss: 4.9637 || 10iter:7.8643 sec.\n",
      "iteration 12250 || Loss: 5.1030 || 10iter:7.9673 sec.\n",
      "iteration 12260 || Loss: 4.6214 || 10iter:7.9689 sec.\n",
      "iteration 12270 || Loss: 4.8133 || 10iter:7.8652 sec.\n",
      "iteration 12280 || Loss: 5.6734 || 10iter:7.8577 sec.\n",
      "iteration 12290 || Loss: 5.2986 || 10iter:7.7741 sec.\n",
      "iteration 12300 || Loss: 6.0165 || 10iter:7.8337 sec.\n",
      "iteration 12310 || Loss: 5.0655 || 10iter:7.8535 sec.\n",
      "iteration 12320 || Loss: 5.5056 || 10iter:7.8590 sec.\n",
      "iteration 12330 || Loss: 4.2216 || 10iter:7.8562 sec.\n",
      "iteration 12340 || Loss: 4.7686 || 10iter:7.8210 sec.\n",
      "iteration 12350 || Loss: 5.2016 || 10iter:7.9475 sec.\n",
      "iteration 12360 || Loss: 5.2109 || 10iter:7.8604 sec.\n",
      "iteration 12370 || Loss: 5.1297 || 10iter:7.8267 sec.\n",
      "iteration 12380 || Loss: 7.2812 || 10iter:7.9028 sec.\n",
      "iteration 12390 || Loss: 4.6887 || 10iter:7.8427 sec.\n",
      "iteration 12400 || Loss: 5.5010 || 10iter:7.9378 sec.\n",
      "iteration 12410 || Loss: 5.5255 || 10iter:7.9532 sec.\n",
      "iteration 12420 || Loss: 5.0405 || 10iter:7.9121 sec.\n",
      "iteration 12430 || Loss: 3.8317 || 10iter:7.8188 sec.\n",
      "iteration 12440 || Loss: 5.1767 || 10iter:7.9271 sec.\n",
      "iteration 12450 || Loss: 5.0963 || 10iter:7.8136 sec.\n",
      "iteration 12460 || Loss: 5.7646 || 10iter:7.8981 sec.\n",
      "iteration 12470 || Loss: 5.2058 || 10iter:7.9108 sec.\n",
      "iteration 12480 || Loss: 4.6626 || 10iter:7.8517 sec.\n",
      "iteration 12490 || Loss: 5.4163 || 10iter:7.8722 sec.\n",
      "iteration 12500 || Loss: 5.6686 || 10iter:7.8167 sec.\n",
      "iteration 12510 || Loss: 6.1374 || 10iter:8.0435 sec.\n",
      "iteration 12520 || Loss: 5.5069 || 10iter:7.8379 sec.\n",
      "iteration 12530 || Loss: 5.2804 || 10iter:7.9355 sec.\n",
      "iteration 12540 || Loss: 6.0189 || 10iter:7.9271 sec.\n",
      "iteration 12550 || Loss: 5.8715 || 10iter:7.9417 sec.\n",
      "iteration 12560 || Loss: 5.6022 || 10iter:7.9498 sec.\n",
      "iteration 12570 || Loss: 5.8595 || 10iter:7.9569 sec.\n",
      "iteration 12580 || Loss: 5.9420 || 10iter:7.8552 sec.\n",
      "iteration 12590 || Loss: 4.8380 || 10iter:7.9634 sec.\n",
      "iteration 12600 || Loss: 5.2508 || 10iter:7.9265 sec.\n",
      "iteration 12610 || Loss: 5.6486 || 10iter:7.8953 sec.\n",
      "iteration 12620 || Loss: 4.6260 || 10iter:7.8373 sec.\n",
      "iteration 12630 || Loss: 5.3416 || 10iter:7.9474 sec.\n",
      "iteration 12640 || Loss: 4.9568 || 10iter:7.7987 sec.\n",
      "iteration 12650 || Loss: 3.7182 || 10iter:7.9592 sec.\n",
      "iteration 12660 || Loss: 4.1012 || 10iter:7.9297 sec.\n",
      "iteration 12670 || Loss: 4.1896 || 10iter:7.8425 sec.\n",
      "iteration 12680 || Loss: 5.0576 || 10iter:7.9319 sec.\n",
      "iteration 12690 || Loss: 5.9417 || 10iter:8.0453 sec.\n",
      "iteration 12700 || Loss: 3.9031 || 10iter:7.8847 sec.\n",
      "iteration 12710 || Loss: 4.4350 || 10iter:7.8769 sec.\n",
      "iteration 12720 || Loss: 4.9060 || 10iter:7.8025 sec.\n",
      "iteration 12730 || Loss: 5.6207 || 10iter:7.9597 sec.\n",
      "iteration 12740 || Loss: 6.1269 || 10iter:7.7604 sec.\n",
      "iteration 12750 || Loss: 3.9765 || 10iter:7.7040 sec.\n",
      "iteration 12760 || Loss: 4.6139 || 10iter:7.8488 sec.\n",
      "iteration 12770 || Loss: 5.1921 || 10iter:7.8754 sec.\n",
      "iteration 12780 || Loss: 5.6616 || 10iter:7.9191 sec.\n",
      "iteration 12790 || Loss: 3.6149 || 10iter:7.8733 sec.\n",
      "iteration 12800 || Loss: 5.1645 || 10iter:7.9231 sec.\n",
      "iteration 12810 || Loss: 5.3872 || 10iter:7.9460 sec.\n",
      "iteration 12820 || Loss: 5.9459 || 10iter:7.9292 sec.\n",
      "iteration 12830 || Loss: 5.6987 || 10iter:7.8968 sec.\n",
      "iteration 12840 || Loss: 4.9428 || 10iter:8.0152 sec.\n",
      "iteration 12850 || Loss: 5.1196 || 10iter:7.9330 sec.\n",
      "iteration 12860 || Loss: 3.9419 || 10iter:7.8606 sec.\n",
      "iteration 12870 || Loss: 3.6065 || 10iter:7.6710 sec.\n",
      "-------------------\n",
      "epoch 18 || Epoch_TRAIN_Loss:3767.5446 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 19/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 12880 || Loss: 6.3121 || 10iter:7.9601 sec.\n",
      "iteration 12890 || Loss: 5.1303 || 10iter:7.8706 sec.\n",
      "iteration 12900 || Loss: 4.8809 || 10iter:7.8366 sec.\n",
      "iteration 12910 || Loss: 5.7541 || 10iter:8.0083 sec.\n",
      "iteration 12920 || Loss: 5.1946 || 10iter:7.9030 sec.\n",
      "iteration 12930 || Loss: 6.1247 || 10iter:8.0447 sec.\n",
      "iteration 12940 || Loss: 4.9870 || 10iter:7.9087 sec.\n",
      "iteration 12950 || Loss: 4.8712 || 10iter:7.7980 sec.\n",
      "iteration 12960 || Loss: 5.4872 || 10iter:7.9579 sec.\n",
      "iteration 12970 || Loss: 6.7420 || 10iter:7.9818 sec.\n",
      "iteration 12980 || Loss: 6.2124 || 10iter:8.1209 sec.\n",
      "iteration 12990 || Loss: 4.8671 || 10iter:7.8900 sec.\n",
      "iteration 13000 || Loss: 5.1604 || 10iter:7.8774 sec.\n",
      "iteration 13010 || Loss: 4.6835 || 10iter:7.7928 sec.\n",
      "iteration 13020 || Loss: 6.4932 || 10iter:7.9028 sec.\n",
      "iteration 13030 || Loss: 4.0834 || 10iter:7.7709 sec.\n",
      "iteration 13040 || Loss: 4.9684 || 10iter:7.9659 sec.\n",
      "iteration 13050 || Loss: 5.5012 || 10iter:7.8700 sec.\n",
      "iteration 13060 || Loss: 5.1026 || 10iter:7.9805 sec.\n",
      "iteration 13070 || Loss: 5.0703 || 10iter:7.8311 sec.\n",
      "iteration 13080 || Loss: 6.0153 || 10iter:7.9145 sec.\n",
      "iteration 13090 || Loss: 5.0592 || 10iter:7.9005 sec.\n",
      "iteration 13100 || Loss: 5.7913 || 10iter:7.7679 sec.\n",
      "iteration 13110 || Loss: 4.9688 || 10iter:7.9111 sec.\n",
      "iteration 13120 || Loss: 5.2105 || 10iter:7.9651 sec.\n",
      "iteration 13130 || Loss: 4.8677 || 10iter:7.9849 sec.\n",
      "iteration 13140 || Loss: 5.4763 || 10iter:7.8784 sec.\n",
      "iteration 13150 || Loss: 4.7419 || 10iter:7.9231 sec.\n",
      "iteration 13160 || Loss: 4.4887 || 10iter:7.9336 sec.\n",
      "iteration 13170 || Loss: 5.5257 || 10iter:7.8950 sec.\n",
      "iteration 13180 || Loss: 5.9160 || 10iter:7.8072 sec.\n",
      "iteration 13190 || Loss: 5.4103 || 10iter:7.8036 sec.\n",
      "iteration 13200 || Loss: 6.8567 || 10iter:7.8496 sec.\n",
      "iteration 13210 || Loss: 4.0104 || 10iter:7.7807 sec.\n",
      "iteration 13220 || Loss: 4.9109 || 10iter:7.9315 sec.\n",
      "iteration 13230 || Loss: 5.1140 || 10iter:7.8772 sec.\n",
      "iteration 13240 || Loss: 3.9631 || 10iter:7.8037 sec.\n",
      "iteration 13250 || Loss: 4.2065 || 10iter:7.8569 sec.\n",
      "iteration 13260 || Loss: 5.6908 || 10iter:7.9490 sec.\n",
      "iteration 13270 || Loss: 4.3639 || 10iter:7.9211 sec.\n",
      "iteration 13280 || Loss: 5.9240 || 10iter:7.8656 sec.\n",
      "iteration 13290 || Loss: 4.7849 || 10iter:7.7774 sec.\n",
      "iteration 13300 || Loss: 4.6219 || 10iter:7.9073 sec.\n",
      "iteration 13310 || Loss: 4.6241 || 10iter:7.7564 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 13320 || Loss: 4.9781 || 10iter:7.7922 sec.\n",
      "iteration 13330 || Loss: 5.5015 || 10iter:7.9145 sec.\n",
      "iteration 13340 || Loss: 4.8557 || 10iter:7.8427 sec.\n",
      "iteration 13350 || Loss: 4.9001 || 10iter:7.8039 sec.\n",
      "iteration 13360 || Loss: 4.9284 || 10iter:7.8640 sec.\n",
      "iteration 13370 || Loss: 4.4625 || 10iter:7.8699 sec.\n",
      "iteration 13380 || Loss: 3.7474 || 10iter:7.8665 sec.\n",
      "iteration 13390 || Loss: 5.9021 || 10iter:7.9558 sec.\n",
      "iteration 13400 || Loss: 4.7497 || 10iter:7.8794 sec.\n",
      "iteration 13410 || Loss: 5.2627 || 10iter:7.8581 sec.\n",
      "iteration 13420 || Loss: 4.5542 || 10iter:7.8679 sec.\n",
      "iteration 13430 || Loss: 5.7732 || 10iter:7.8352 sec.\n",
      "iteration 13440 || Loss: 6.1476 || 10iter:7.8831 sec.\n",
      "iteration 13450 || Loss: 5.4821 || 10iter:7.8712 sec.\n",
      "iteration 13460 || Loss: 5.2343 || 10iter:7.8048 sec.\n",
      "iteration 13470 || Loss: 6.7340 || 10iter:7.8782 sec.\n",
      "iteration 13480 || Loss: 5.8285 || 10iter:7.9484 sec.\n",
      "iteration 13490 || Loss: 5.4105 || 10iter:7.7711 sec.\n",
      "iteration 13500 || Loss: 5.4686 || 10iter:7.7255 sec.\n",
      "iteration 13510 || Loss: 5.6173 || 10iter:7.8302 sec.\n",
      "iteration 13520 || Loss: 6.2310 || 10iter:7.9661 sec.\n",
      "iteration 13530 || Loss: 5.2243 || 10iter:7.9059 sec.\n",
      "iteration 13540 || Loss: 4.5082 || 10iter:7.9105 sec.\n",
      "iteration 13550 || Loss: 5.1396 || 10iter:7.8725 sec.\n",
      "iteration 13560 || Loss: 5.6124 || 10iter:7.9053 sec.\n",
      "iteration 13570 || Loss: 5.3191 || 10iter:7.8995 sec.\n",
      "iteration 13580 || Loss: 5.6315 || 10iter:7.8842 sec.\n",
      "-------------------\n",
      "epoch 19 || Epoch_TRAIN_Loss:3725.7551 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 20/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 13590 || Loss: 5.1358 || 10iter:3.7423 sec.\n",
      "iteration 13600 || Loss: 4.4008 || 10iter:7.7483 sec.\n",
      "iteration 13610 || Loss: 6.3572 || 10iter:7.8821 sec.\n",
      "iteration 13620 || Loss: 5.7156 || 10iter:7.7078 sec.\n",
      "iteration 13630 || Loss: 4.7252 || 10iter:7.8009 sec.\n",
      "iteration 13640 || Loss: 5.0784 || 10iter:7.9201 sec.\n",
      "iteration 13650 || Loss: 5.3711 || 10iter:7.8581 sec.\n",
      "iteration 13660 || Loss: 4.7305 || 10iter:7.8914 sec.\n",
      "iteration 13670 || Loss: 4.7680 || 10iter:7.7926 sec.\n",
      "iteration 13680 || Loss: 5.1652 || 10iter:7.9206 sec.\n",
      "iteration 13690 || Loss: 5.6911 || 10iter:7.8959 sec.\n",
      "iteration 13700 || Loss: 5.3148 || 10iter:7.8871 sec.\n",
      "iteration 13710 || Loss: 4.6974 || 10iter:7.9055 sec.\n",
      "iteration 13720 || Loss: 4.4584 || 10iter:7.9216 sec.\n",
      "iteration 13730 || Loss: 4.6159 || 10iter:7.9080 sec.\n",
      "iteration 13740 || Loss: 4.2527 || 10iter:7.7987 sec.\n",
      "iteration 13750 || Loss: 4.2123 || 10iter:7.9158 sec.\n",
      "iteration 13760 || Loss: 5.5361 || 10iter:7.9550 sec.\n",
      "iteration 13770 || Loss: 4.8790 || 10iter:7.9200 sec.\n",
      "iteration 13780 || Loss: 5.8720 || 10iter:7.8650 sec.\n",
      "iteration 13790 || Loss: 5.3337 || 10iter:7.8921 sec.\n",
      "iteration 13800 || Loss: 5.2600 || 10iter:7.8313 sec.\n",
      "iteration 13810 || Loss: 6.1902 || 10iter:7.9980 sec.\n",
      "iteration 13820 || Loss: 5.5620 || 10iter:7.9161 sec.\n",
      "iteration 13830 || Loss: 5.2929 || 10iter:7.8361 sec.\n",
      "iteration 13840 || Loss: 6.6789 || 10iter:7.8419 sec.\n",
      "iteration 13850 || Loss: 4.5029 || 10iter:7.9448 sec.\n",
      "iteration 13860 || Loss: 5.5923 || 10iter:7.8913 sec.\n",
      "iteration 13870 || Loss: 4.4779 || 10iter:7.8096 sec.\n",
      "iteration 13880 || Loss: 5.2817 || 10iter:7.8392 sec.\n",
      "iteration 13890 || Loss: 5.7419 || 10iter:7.8100 sec.\n",
      "iteration 13900 || Loss: 5.2855 || 10iter:7.8114 sec.\n",
      "iteration 13910 || Loss: 5.1017 || 10iter:7.7799 sec.\n",
      "iteration 13920 || Loss: 4.6744 || 10iter:7.8392 sec.\n",
      "iteration 13930 || Loss: 4.0940 || 10iter:7.8834 sec.\n",
      "iteration 13940 || Loss: 5.7223 || 10iter:7.9379 sec.\n",
      "iteration 13950 || Loss: 4.7906 || 10iter:7.9059 sec.\n",
      "iteration 13960 || Loss: 5.2075 || 10iter:7.8320 sec.\n",
      "iteration 13970 || Loss: 4.7068 || 10iter:7.8458 sec.\n",
      "iteration 13980 || Loss: 5.0776 || 10iter:7.8856 sec.\n",
      "iteration 13990 || Loss: 5.3452 || 10iter:7.9897 sec.\n",
      "iteration 14000 || Loss: 4.8646 || 10iter:7.9285 sec.\n",
      "iteration 14010 || Loss: 4.2323 || 10iter:7.8937 sec.\n",
      "iteration 14020 || Loss: 4.9855 || 10iter:7.8716 sec.\n",
      "iteration 14030 || Loss: 5.1580 || 10iter:7.8406 sec.\n",
      "iteration 14040 || Loss: 5.1447 || 10iter:7.9311 sec.\n",
      "iteration 14050 || Loss: 4.5690 || 10iter:7.7772 sec.\n",
      "iteration 14060 || Loss: 5.4120 || 10iter:7.8027 sec.\n",
      "iteration 14070 || Loss: 5.8129 || 10iter:7.9018 sec.\n",
      "iteration 14080 || Loss: 5.1710 || 10iter:7.8379 sec.\n",
      "iteration 14090 || Loss: 4.9509 || 10iter:8.0630 sec.\n",
      "iteration 14100 || Loss: 5.1963 || 10iter:7.8952 sec.\n",
      "iteration 14110 || Loss: 4.4906 || 10iter:7.9153 sec.\n",
      "iteration 14120 || Loss: 5.0885 || 10iter:7.8832 sec.\n",
      "iteration 14130 || Loss: 5.0251 || 10iter:7.8077 sec.\n",
      "iteration 14140 || Loss: 4.7775 || 10iter:7.8000 sec.\n",
      "iteration 14150 || Loss: 4.3672 || 10iter:7.8790 sec.\n",
      "iteration 14160 || Loss: 4.6074 || 10iter:7.8415 sec.\n",
      "iteration 14170 || Loss: 4.9094 || 10iter:7.7157 sec.\n",
      "iteration 14180 || Loss: 5.4633 || 10iter:7.9646 sec.\n",
      "iteration 14190 || Loss: 4.0167 || 10iter:7.8863 sec.\n",
      "iteration 14200 || Loss: 4.1302 || 10iter:7.8465 sec.\n",
      "iteration 14210 || Loss: 4.1670 || 10iter:7.8110 sec.\n",
      "iteration 14220 || Loss: 4.5731 || 10iter:7.8486 sec.\n",
      "iteration 14230 || Loss: 4.1965 || 10iter:7.8024 sec.\n",
      "iteration 14240 || Loss: 4.4828 || 10iter:7.8389 sec.\n",
      "iteration 14250 || Loss: 5.1989 || 10iter:7.8747 sec.\n",
      "iteration 14260 || Loss: 4.8160 || 10iter:7.9452 sec.\n",
      "iteration 14270 || Loss: 5.0111 || 10iter:7.9461 sec.\n",
      "iteration 14280 || Loss: 5.4202 || 10iter:7.8268 sec.\n",
      "iteration 14290 || Loss: 4.3002 || 10iter:7.9285 sec.\n",
      "iteration 14300 || Loss: 6.2601 || 10iter:7.6524 sec.\n",
      "-------------------\n",
      " (val) \n",
      "-------------------\n",
      "epoch 20 || Epoch_TRAIN_Loss:3606.9111 || Epoch_VAL_Loss:3705.9747\n",
      "-------------------\n",
      "Epoch 21/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 14310 || Loss: 6.5906 || 10iter:7.8559 sec.\n",
      "iteration 14320 || Loss: 5.0208 || 10iter:7.8503 sec.\n",
      "iteration 14330 || Loss: 5.4456 || 10iter:7.8637 sec.\n",
      "iteration 14340 || Loss: 4.3348 || 10iter:7.9842 sec.\n",
      "iteration 14350 || Loss: 4.7390 || 10iter:7.8421 sec.\n",
      "iteration 14360 || Loss: 3.6818 || 10iter:7.8261 sec.\n",
      "iteration 14370 || Loss: 4.8477 || 10iter:7.8547 sec.\n",
      "iteration 14380 || Loss: 5.3240 || 10iter:8.0091 sec.\n",
      "iteration 14390 || Loss: 5.0275 || 10iter:7.8597 sec.\n",
      "iteration 14400 || Loss: 4.4117 || 10iter:7.8520 sec.\n",
      "iteration 14410 || Loss: 5.1637 || 10iter:7.8984 sec.\n",
      "iteration 14420 || Loss: 4.7184 || 10iter:7.8688 sec.\n",
      "iteration 14430 || Loss: 5.9111 || 10iter:7.8069 sec.\n",
      "iteration 14440 || Loss: 4.1304 || 10iter:7.7744 sec.\n",
      "iteration 14450 || Loss: 6.1480 || 10iter:7.9045 sec.\n",
      "iteration 14460 || Loss: 4.9802 || 10iter:7.9830 sec.\n",
      "iteration 14470 || Loss: 5.5851 || 10iter:7.8972 sec.\n",
      "iteration 14480 || Loss: 4.9540 || 10iter:7.7960 sec.\n",
      "iteration 14490 || Loss: 4.4785 || 10iter:7.9105 sec.\n",
      "iteration 14500 || Loss: 5.0717 || 10iter:7.8557 sec.\n",
      "iteration 14510 || Loss: 6.0139 || 10iter:7.8549 sec.\n",
      "iteration 14520 || Loss: 4.9985 || 10iter:7.8728 sec.\n",
      "iteration 14530 || Loss: 4.9637 || 10iter:8.0381 sec.\n",
      "iteration 14540 || Loss: 5.1831 || 10iter:7.7999 sec.\n",
      "iteration 14550 || Loss: 4.7566 || 10iter:7.8352 sec.\n",
      "iteration 14560 || Loss: 4.0344 || 10iter:7.7732 sec.\n",
      "iteration 14570 || Loss: 5.9981 || 10iter:7.8606 sec.\n",
      "iteration 14580 || Loss: 5.4502 || 10iter:7.8654 sec.\n",
      "iteration 14590 || Loss: 3.9154 || 10iter:7.8290 sec.\n",
      "iteration 14600 || Loss: 5.6003 || 10iter:7.8313 sec.\n",
      "iteration 14610 || Loss: 3.8050 || 10iter:7.8593 sec.\n",
      "iteration 14620 || Loss: 5.8837 || 10iter:7.8878 sec.\n",
      "iteration 14630 || Loss: 5.0106 || 10iter:7.9089 sec.\n",
      "iteration 14640 || Loss: 5.2774 || 10iter:7.9924 sec.\n",
      "iteration 14650 || Loss: 5.5704 || 10iter:7.8660 sec.\n",
      "iteration 14660 || Loss: 5.2803 || 10iter:7.9775 sec.\n",
      "iteration 14670 || Loss: 4.5622 || 10iter:7.9109 sec.\n",
      "iteration 14680 || Loss: 5.5067 || 10iter:8.0130 sec.\n",
      "iteration 14690 || Loss: 5.4453 || 10iter:7.9677 sec.\n",
      "iteration 14700 || Loss: 5.7498 || 10iter:7.8642 sec.\n",
      "iteration 14710 || Loss: 5.6537 || 10iter:7.8582 sec.\n",
      "iteration 14720 || Loss: 4.7414 || 10iter:7.8969 sec.\n",
      "iteration 14730 || Loss: 6.0194 || 10iter:7.9348 sec.\n",
      "iteration 14740 || Loss: 5.6558 || 10iter:7.9144 sec.\n",
      "iteration 14750 || Loss: 5.2974 || 10iter:7.8465 sec.\n",
      "iteration 14760 || Loss: 4.5191 || 10iter:7.8905 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 14770 || Loss: 5.7121 || 10iter:7.9196 sec.\n",
      "iteration 14780 || Loss: 3.8943 || 10iter:7.9927 sec.\n",
      "iteration 14790 || Loss: 6.0242 || 10iter:7.8002 sec.\n",
      "iteration 14800 || Loss: 5.4931 || 10iter:7.8852 sec.\n",
      "iteration 14810 || Loss: 4.4432 || 10iter:7.9561 sec.\n",
      "iteration 14820 || Loss: 4.3878 || 10iter:7.9111 sec.\n",
      "iteration 14830 || Loss: 4.9275 || 10iter:7.8214 sec.\n",
      "iteration 14840 || Loss: 4.9526 || 10iter:7.8339 sec.\n",
      "iteration 14850 || Loss: 4.8886 || 10iter:7.8395 sec.\n",
      "iteration 14860 || Loss: 4.7943 || 10iter:7.9790 sec.\n",
      "iteration 14870 || Loss: 4.3321 || 10iter:7.8354 sec.\n",
      "iteration 14880 || Loss: 4.0009 || 10iter:7.8012 sec.\n",
      "iteration 14890 || Loss: 4.7364 || 10iter:7.9341 sec.\n",
      "iteration 14900 || Loss: 4.6797 || 10iter:7.7974 sec.\n",
      "iteration 14910 || Loss: 5.3834 || 10iter:7.7667 sec.\n",
      "iteration 14920 || Loss: 4.9057 || 10iter:7.7550 sec.\n",
      "iteration 14930 || Loss: 5.6889 || 10iter:7.8778 sec.\n",
      "iteration 14940 || Loss: 6.1298 || 10iter:7.9088 sec.\n",
      "iteration 14950 || Loss: 5.2154 || 10iter:7.8279 sec.\n",
      "iteration 14960 || Loss: 4.6273 || 10iter:7.8248 sec.\n",
      "iteration 14970 || Loss: 5.0501 || 10iter:7.8705 sec.\n",
      "iteration 14980 || Loss: 5.1022 || 10iter:8.0036 sec.\n",
      "iteration 14990 || Loss: 6.1174 || 10iter:7.8169 sec.\n",
      "iteration 15000 || Loss: 5.0618 || 10iter:7.8464 sec.\n",
      "iteration 15010 || Loss: 4.4707 || 10iter:7.7673 sec.\n",
      "-------------------\n",
      "epoch 21 || Epoch_TRAIN_Loss:3568.2470 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 22/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 15020 || Loss: 4.3834 || 10iter:3.7820 sec.\n",
      "iteration 15030 || Loss: 4.3305 || 10iter:7.8865 sec.\n",
      "iteration 15040 || Loss: 5.0789 || 10iter:7.8463 sec.\n",
      "iteration 15050 || Loss: 4.6676 || 10iter:7.9067 sec.\n",
      "iteration 15060 || Loss: 4.6656 || 10iter:7.8586 sec.\n",
      "iteration 15070 || Loss: 3.4051 || 10iter:7.8485 sec.\n",
      "iteration 15080 || Loss: 5.3104 || 10iter:7.8850 sec.\n",
      "iteration 15090 || Loss: 4.6046 || 10iter:7.9245 sec.\n",
      "iteration 15100 || Loss: 5.2654 || 10iter:7.8093 sec.\n",
      "iteration 15110 || Loss: 4.8611 || 10iter:7.8802 sec.\n",
      "iteration 15120 || Loss: 4.9431 || 10iter:7.9265 sec.\n",
      "iteration 15130 || Loss: 4.5139 || 10iter:7.8086 sec.\n",
      "iteration 15140 || Loss: 5.2368 || 10iter:7.8378 sec.\n",
      "iteration 15150 || Loss: 4.4954 || 10iter:7.7408 sec.\n",
      "iteration 15160 || Loss: 4.4049 || 10iter:7.8631 sec.\n",
      "iteration 15170 || Loss: 5.8822 || 10iter:8.0417 sec.\n",
      "iteration 15180 || Loss: 6.7669 || 10iter:7.9228 sec.\n",
      "iteration 15190 || Loss: 4.1852 || 10iter:7.8386 sec.\n",
      "iteration 15200 || Loss: 4.4275 || 10iter:7.9344 sec.\n",
      "iteration 15210 || Loss: 4.0445 || 10iter:7.8491 sec.\n",
      "iteration 15220 || Loss: 4.9209 || 10iter:7.8958 sec.\n",
      "iteration 15230 || Loss: 4.7801 || 10iter:7.8284 sec.\n",
      "iteration 15240 || Loss: 4.9345 || 10iter:7.9401 sec.\n",
      "iteration 15250 || Loss: 3.5607 || 10iter:7.8466 sec.\n",
      "iteration 15260 || Loss: 4.5253 || 10iter:7.9881 sec.\n",
      "iteration 15270 || Loss: 5.0865 || 10iter:7.8854 sec.\n",
      "iteration 15280 || Loss: 4.9796 || 10iter:7.9501 sec.\n",
      "iteration 15290 || Loss: 5.7261 || 10iter:7.8442 sec.\n",
      "iteration 15300 || Loss: 4.9304 || 10iter:7.9426 sec.\n",
      "iteration 15310 || Loss: 4.4648 || 10iter:7.8140 sec.\n",
      "iteration 15320 || Loss: 5.2481 || 10iter:7.8026 sec.\n",
      "iteration 15330 || Loss: 5.1205 || 10iter:7.9120 sec.\n",
      "iteration 15340 || Loss: 5.4547 || 10iter:7.7837 sec.\n",
      "iteration 15350 || Loss: 5.4253 || 10iter:7.8503 sec.\n",
      "iteration 15360 || Loss: 5.5027 || 10iter:7.8290 sec.\n",
      "iteration 15370 || Loss: 5.2657 || 10iter:7.8051 sec.\n",
      "iteration 15380 || Loss: 4.8332 || 10iter:7.7759 sec.\n",
      "iteration 15390 || Loss: 5.3686 || 10iter:7.8746 sec.\n",
      "iteration 15400 || Loss: 5.3652 || 10iter:7.8665 sec.\n",
      "iteration 15410 || Loss: 3.9835 || 10iter:7.8627 sec.\n",
      "iteration 15420 || Loss: 5.2670 || 10iter:7.8086 sec.\n",
      "iteration 15430 || Loss: 5.6359 || 10iter:7.8051 sec.\n",
      "iteration 15440 || Loss: 4.2205 || 10iter:7.8570 sec.\n",
      "iteration 15450 || Loss: 4.5825 || 10iter:7.9177 sec.\n",
      "iteration 15460 || Loss: 4.1574 || 10iter:7.8241 sec.\n",
      "iteration 15470 || Loss: 4.6022 || 10iter:7.7458 sec.\n",
      "iteration 15480 || Loss: 4.6570 || 10iter:7.7313 sec.\n",
      "iteration 15490 || Loss: 4.7014 || 10iter:7.7271 sec.\n",
      "iteration 15500 || Loss: 4.3810 || 10iter:7.8405 sec.\n",
      "iteration 15510 || Loss: 4.5516 || 10iter:7.8624 sec.\n",
      "iteration 15520 || Loss: 5.3456 || 10iter:7.8137 sec.\n",
      "iteration 15530 || Loss: 4.0070 || 10iter:7.8168 sec.\n",
      "iteration 15540 || Loss: 5.5318 || 10iter:7.8337 sec.\n",
      "iteration 15550 || Loss: 5.6955 || 10iter:7.8828 sec.\n",
      "iteration 15560 || Loss: 5.0334 || 10iter:7.7793 sec.\n",
      "iteration 15570 || Loss: 5.7074 || 10iter:7.8767 sec.\n",
      "iteration 15580 || Loss: 5.3969 || 10iter:7.8464 sec.\n",
      "iteration 15590 || Loss: 4.3648 || 10iter:7.8803 sec.\n",
      "iteration 15600 || Loss: 5.6989 || 10iter:7.8665 sec.\n",
      "iteration 15610 || Loss: 3.8971 || 10iter:7.9008 sec.\n",
      "iteration 15620 || Loss: 5.1029 || 10iter:7.7882 sec.\n",
      "iteration 15630 || Loss: 3.9362 || 10iter:7.8871 sec.\n",
      "iteration 15640 || Loss: 3.1027 || 10iter:7.8408 sec.\n",
      "iteration 15650 || Loss: 5.0453 || 10iter:7.8098 sec.\n",
      "iteration 15660 || Loss: 4.6696 || 10iter:7.8506 sec.\n",
      "iteration 15670 || Loss: 4.3573 || 10iter:7.8312 sec.\n",
      "iteration 15680 || Loss: 4.5016 || 10iter:7.7630 sec.\n",
      "iteration 15690 || Loss: 5.3495 || 10iter:7.8625 sec.\n",
      "iteration 15700 || Loss: 4.8182 || 10iter:7.8633 sec.\n",
      "iteration 15710 || Loss: 4.7453 || 10iter:7.9132 sec.\n",
      "iteration 15720 || Loss: 4.6477 || 10iter:7.8227 sec.\n",
      "iteration 15730 || Loss: 5.4493 || 10iter:7.7035 sec.\n",
      "-------------------\n",
      "epoch 22 || Epoch_TRAIN_Loss:3490.6949 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 23/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 15740 || Loss: 4.0667 || 10iter:7.8283 sec.\n",
      "iteration 15750 || Loss: 4.7561 || 10iter:7.8533 sec.\n",
      "iteration 15760 || Loss: 3.9701 || 10iter:7.8912 sec.\n",
      "iteration 15770 || Loss: 4.6172 || 10iter:7.9082 sec.\n",
      "iteration 15780 || Loss: 4.7143 || 10iter:7.9405 sec.\n",
      "iteration 15790 || Loss: 5.4080 || 10iter:8.0099 sec.\n",
      "iteration 15800 || Loss: 4.2012 || 10iter:7.9705 sec.\n",
      "iteration 15810 || Loss: 3.8903 || 10iter:7.8274 sec.\n",
      "iteration 15820 || Loss: 4.5265 || 10iter:7.9277 sec.\n",
      "iteration 15830 || Loss: 4.4626 || 10iter:7.8348 sec.\n",
      "iteration 15840 || Loss: 5.1143 || 10iter:7.7974 sec.\n",
      "iteration 15850 || Loss: 6.7976 || 10iter:7.9494 sec.\n",
      "iteration 15860 || Loss: 4.0392 || 10iter:7.9716 sec.\n",
      "iteration 15870 || Loss: 5.0129 || 10iter:7.7586 sec.\n",
      "iteration 15880 || Loss: 4.9819 || 10iter:7.9595 sec.\n",
      "iteration 15890 || Loss: 4.9222 || 10iter:7.7924 sec.\n",
      "iteration 15900 || Loss: 4.5396 || 10iter:7.8839 sec.\n",
      "iteration 15910 || Loss: 5.3567 || 10iter:8.0080 sec.\n",
      "iteration 15920 || Loss: 4.4058 || 10iter:7.8583 sec.\n",
      "iteration 15930 || Loss: 5.5950 || 10iter:7.8878 sec.\n",
      "iteration 15940 || Loss: 4.2581 || 10iter:7.7873 sec.\n",
      "iteration 15950 || Loss: 4.8307 || 10iter:7.8389 sec.\n",
      "iteration 15960 || Loss: 4.1320 || 10iter:8.0467 sec.\n",
      "iteration 15970 || Loss: 4.0970 || 10iter:7.9231 sec.\n",
      "iteration 15980 || Loss: 5.4115 || 10iter:7.8745 sec.\n",
      "iteration 15990 || Loss: 4.9403 || 10iter:7.9195 sec.\n",
      "iteration 16000 || Loss: 4.9816 || 10iter:7.8613 sec.\n",
      "iteration 16010 || Loss: 5.6763 || 10iter:7.9193 sec.\n",
      "iteration 16020 || Loss: 4.4225 || 10iter:7.9140 sec.\n",
      "iteration 16030 || Loss: 4.7131 || 10iter:7.9544 sec.\n",
      "iteration 16040 || Loss: 4.7525 || 10iter:7.9954 sec.\n",
      "iteration 16050 || Loss: 5.1552 || 10iter:7.9923 sec.\n",
      "iteration 16060 || Loss: 5.0799 || 10iter:7.8538 sec.\n",
      "iteration 16070 || Loss: 6.0920 || 10iter:7.9125 sec.\n",
      "iteration 16080 || Loss: 4.5800 || 10iter:7.8949 sec.\n",
      "iteration 16090 || Loss: 3.9816 || 10iter:7.8821 sec.\n",
      "iteration 16100 || Loss: 5.8724 || 10iter:7.9430 sec.\n",
      "iteration 16110 || Loss: 3.5833 || 10iter:7.8304 sec.\n",
      "iteration 16120 || Loss: 3.8280 || 10iter:7.8847 sec.\n",
      "iteration 16130 || Loss: 4.1786 || 10iter:7.9561 sec.\n",
      "iteration 16140 || Loss: 5.4536 || 10iter:7.9751 sec.\n",
      "iteration 16150 || Loss: 4.8337 || 10iter:7.9287 sec.\n",
      "iteration 16160 || Loss: 5.9030 || 10iter:7.8334 sec.\n",
      "iteration 16170 || Loss: 4.1327 || 10iter:7.8883 sec.\n",
      "iteration 16180 || Loss: 5.3315 || 10iter:7.7927 sec.\n",
      "iteration 16190 || Loss: 5.1683 || 10iter:7.9958 sec.\n",
      "iteration 16200 || Loss: 4.4043 || 10iter:7.8732 sec.\n",
      "iteration 16210 || Loss: 3.3049 || 10iter:7.9091 sec.\n",
      "iteration 16220 || Loss: 4.4262 || 10iter:7.8477 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 16230 || Loss: 4.2264 || 10iter:7.9232 sec.\n",
      "iteration 16240 || Loss: 4.4769 || 10iter:7.7439 sec.\n",
      "iteration 16250 || Loss: 3.8010 || 10iter:7.8887 sec.\n",
      "iteration 16260 || Loss: 5.5133 || 10iter:7.8481 sec.\n",
      "iteration 16270 || Loss: 5.5987 || 10iter:7.8193 sec.\n",
      "iteration 16280 || Loss: 6.3616 || 10iter:7.9672 sec.\n",
      "iteration 16290 || Loss: 4.6808 || 10iter:7.9348 sec.\n",
      "iteration 16300 || Loss: 4.3247 || 10iter:7.9721 sec.\n",
      "iteration 16310 || Loss: 4.7898 || 10iter:7.9311 sec.\n",
      "iteration 16320 || Loss: 5.7481 || 10iter:7.8188 sec.\n",
      "iteration 16330 || Loss: 5.3180 || 10iter:7.9041 sec.\n",
      "iteration 16340 || Loss: 4.1007 || 10iter:7.8497 sec.\n",
      "iteration 16350 || Loss: 5.7248 || 10iter:7.8676 sec.\n",
      "iteration 16360 || Loss: 5.3950 || 10iter:7.9119 sec.\n",
      "iteration 16370 || Loss: 5.2011 || 10iter:7.9049 sec.\n",
      "iteration 16380 || Loss: 5.6527 || 10iter:7.8707 sec.\n",
      "iteration 16390 || Loss: 5.4762 || 10iter:7.7985 sec.\n",
      "iteration 16400 || Loss: 4.3466 || 10iter:7.7845 sec.\n",
      "iteration 16410 || Loss: 5.6559 || 10iter:7.8912 sec.\n",
      "iteration 16420 || Loss: 7.1862 || 10iter:7.8468 sec.\n",
      "iteration 16430 || Loss: 4.7700 || 10iter:7.9277 sec.\n",
      "iteration 16440 || Loss: 4.0708 || 10iter:7.9455 sec.\n",
      "-------------------\n",
      "epoch 23 || Epoch_TRAIN_Loss:3467.5308 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 24/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 16450 || Loss: 5.0875 || 10iter:3.7305 sec.\n",
      "iteration 16460 || Loss: 3.8088 || 10iter:7.9063 sec.\n",
      "iteration 16470 || Loss: 5.1911 || 10iter:7.9400 sec.\n",
      "iteration 16480 || Loss: 5.1911 || 10iter:7.8589 sec.\n",
      "iteration 16490 || Loss: 4.1560 || 10iter:7.7951 sec.\n",
      "iteration 16500 || Loss: 4.4229 || 10iter:7.9552 sec.\n",
      "iteration 16510 || Loss: 3.6519 || 10iter:7.7928 sec.\n",
      "iteration 16520 || Loss: 5.0141 || 10iter:7.7961 sec.\n",
      "iteration 16530 || Loss: 5.1100 || 10iter:7.8015 sec.\n",
      "iteration 16540 || Loss: 3.9665 || 10iter:7.8902 sec.\n",
      "iteration 16550 || Loss: 5.9262 || 10iter:7.8423 sec.\n",
      "iteration 16560 || Loss: 4.7607 || 10iter:7.9368 sec.\n",
      "iteration 16570 || Loss: 4.6614 || 10iter:7.7828 sec.\n",
      "iteration 16580 || Loss: 4.6933 || 10iter:7.8583 sec.\n",
      "iteration 16590 || Loss: 4.8832 || 10iter:7.8143 sec.\n",
      "iteration 16600 || Loss: 4.4141 || 10iter:7.7702 sec.\n",
      "iteration 16610 || Loss: 4.7192 || 10iter:7.9435 sec.\n",
      "iteration 16620 || Loss: 4.8657 || 10iter:7.9261 sec.\n",
      "iteration 16630 || Loss: 4.5759 || 10iter:7.7378 sec.\n",
      "iteration 16640 || Loss: 4.2814 || 10iter:7.8073 sec.\n",
      "iteration 16650 || Loss: 5.9125 || 10iter:7.9041 sec.\n",
      "iteration 16660 || Loss: 5.1918 || 10iter:7.9941 sec.\n",
      "iteration 16670 || Loss: 4.8738 || 10iter:7.8067 sec.\n",
      "iteration 16680 || Loss: 5.3538 || 10iter:7.9156 sec.\n",
      "iteration 16690 || Loss: 4.1146 || 10iter:7.8472 sec.\n",
      "iteration 16700 || Loss: 3.6125 || 10iter:7.7580 sec.\n",
      "iteration 16710 || Loss: 5.3800 || 10iter:7.7776 sec.\n",
      "iteration 16720 || Loss: 4.6179 || 10iter:7.9180 sec.\n",
      "iteration 16730 || Loss: 3.6141 || 10iter:7.9154 sec.\n",
      "iteration 16740 || Loss: 2.8470 || 10iter:7.9781 sec.\n",
      "iteration 16750 || Loss: 4.3308 || 10iter:7.8858 sec.\n",
      "iteration 16760 || Loss: 4.7588 || 10iter:7.8643 sec.\n",
      "iteration 16770 || Loss: 4.8717 || 10iter:7.9087 sec.\n",
      "iteration 16780 || Loss: 5.0335 || 10iter:7.7996 sec.\n",
      "iteration 16790 || Loss: 5.6499 || 10iter:7.8233 sec.\n",
      "iteration 16800 || Loss: 4.7265 || 10iter:7.7937 sec.\n",
      "iteration 16810 || Loss: 4.5606 || 10iter:7.7876 sec.\n",
      "iteration 16820 || Loss: 4.2772 || 10iter:7.8237 sec.\n",
      "iteration 16830 || Loss: 3.8369 || 10iter:7.8451 sec.\n",
      "iteration 16840 || Loss: 6.1458 || 10iter:7.8287 sec.\n",
      "iteration 16850 || Loss: 6.0055 || 10iter:7.8846 sec.\n",
      "iteration 16860 || Loss: 5.2828 || 10iter:7.9082 sec.\n",
      "iteration 16870 || Loss: 4.5984 || 10iter:7.8872 sec.\n",
      "iteration 16880 || Loss: 3.8271 || 10iter:7.9263 sec.\n",
      "iteration 16890 || Loss: 4.7663 || 10iter:7.9198 sec.\n",
      "iteration 16900 || Loss: 4.6417 || 10iter:7.8602 sec.\n",
      "iteration 16910 || Loss: 4.8850 || 10iter:7.9297 sec.\n",
      "iteration 16920 || Loss: 4.6402 || 10iter:7.9438 sec.\n",
      "iteration 16930 || Loss: 4.4571 || 10iter:7.7674 sec.\n",
      "iteration 16940 || Loss: 4.2922 || 10iter:7.7763 sec.\n",
      "iteration 16950 || Loss: 4.8181 || 10iter:7.8092 sec.\n",
      "iteration 16960 || Loss: 3.9096 || 10iter:7.8732 sec.\n",
      "iteration 16970 || Loss: 5.0626 || 10iter:7.7826 sec.\n",
      "iteration 16980 || Loss: 5.3308 || 10iter:7.8360 sec.\n",
      "iteration 16990 || Loss: 5.7599 || 10iter:7.8776 sec.\n",
      "iteration 17000 || Loss: 4.4936 || 10iter:7.8568 sec.\n",
      "iteration 17010 || Loss: 4.5717 || 10iter:7.9788 sec.\n",
      "iteration 17020 || Loss: 5.3081 || 10iter:7.8108 sec.\n",
      "iteration 17030 || Loss: 4.3291 || 10iter:7.8509 sec.\n",
      "iteration 17040 || Loss: 4.4547 || 10iter:7.8739 sec.\n",
      "iteration 17050 || Loss: 5.2454 || 10iter:7.9170 sec.\n",
      "iteration 17060 || Loss: 6.0251 || 10iter:7.8989 sec.\n",
      "iteration 17070 || Loss: 4.3747 || 10iter:7.8149 sec.\n",
      "iteration 17080 || Loss: 3.8593 || 10iter:7.9534 sec.\n",
      "iteration 17090 || Loss: 3.8656 || 10iter:7.9087 sec.\n",
      "iteration 17100 || Loss: 5.4667 || 10iter:7.9239 sec.\n",
      "iteration 17110 || Loss: 4.7373 || 10iter:7.9279 sec.\n",
      "iteration 17120 || Loss: 3.7231 || 10iter:7.8610 sec.\n",
      "iteration 17130 || Loss: 4.7794 || 10iter:7.8775 sec.\n",
      "iteration 17140 || Loss: 4.4802 || 10iter:7.7352 sec.\n",
      "iteration 17150 || Loss: 4.5937 || 10iter:7.8263 sec.\n",
      "iteration 17160 || Loss: 5.5560 || 10iter:7.7865 sec.\n",
      "-------------------\n",
      "epoch 24 || Epoch_TRAIN_Loss:3430.5275 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 25/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 17170 || Loss: 4.8697 || 10iter:7.8621 sec.\n",
      "iteration 17180 || Loss: 5.6156 || 10iter:7.9969 sec.\n",
      "iteration 17190 || Loss: 4.7602 || 10iter:8.0322 sec.\n",
      "iteration 17200 || Loss: 4.1763 || 10iter:7.8495 sec.\n",
      "iteration 17210 || Loss: 5.2350 || 10iter:7.9582 sec.\n",
      "iteration 17220 || Loss: 4.2360 || 10iter:7.9354 sec.\n",
      "iteration 17230 || Loss: 4.4033 || 10iter:7.8044 sec.\n",
      "iteration 17240 || Loss: 5.1038 || 10iter:7.8306 sec.\n",
      "iteration 17250 || Loss: 4.9722 || 10iter:7.8501 sec.\n",
      "iteration 17260 || Loss: 5.0412 || 10iter:7.8519 sec.\n",
      "iteration 17270 || Loss: 4.3618 || 10iter:7.8538 sec.\n",
      "iteration 17280 || Loss: 4.9451 || 10iter:7.9452 sec.\n",
      "iteration 17290 || Loss: 4.2981 || 10iter:7.8178 sec.\n",
      "iteration 17300 || Loss: 4.4357 || 10iter:7.8085 sec.\n",
      "iteration 17310 || Loss: 5.1412 || 10iter:7.7640 sec.\n",
      "iteration 17320 || Loss: 4.7863 || 10iter:7.8488 sec.\n",
      "iteration 17330 || Loss: 3.5907 || 10iter:7.9561 sec.\n",
      "iteration 17340 || Loss: 5.7692 || 10iter:7.8412 sec.\n",
      "iteration 17350 || Loss: 5.9804 || 10iter:7.8513 sec.\n",
      "iteration 17360 || Loss: 4.1381 || 10iter:7.9133 sec.\n",
      "iteration 17370 || Loss: 5.0215 || 10iter:7.9928 sec.\n",
      "iteration 17380 || Loss: 4.1526 || 10iter:7.8350 sec.\n",
      "iteration 17390 || Loss: 4.6128 || 10iter:7.7907 sec.\n",
      "iteration 17400 || Loss: 4.0004 || 10iter:7.7867 sec.\n",
      "iteration 17410 || Loss: 4.6820 || 10iter:7.9040 sec.\n",
      "iteration 17420 || Loss: 5.1034 || 10iter:7.8015 sec.\n",
      "iteration 17430 || Loss: 4.4073 || 10iter:7.9167 sec.\n",
      "iteration 17440 || Loss: 4.7064 || 10iter:7.9351 sec.\n",
      "iteration 17450 || Loss: 5.6880 || 10iter:8.0124 sec.\n",
      "iteration 17460 || Loss: 4.3200 || 10iter:7.8491 sec.\n",
      "iteration 17470 || Loss: 6.0953 || 10iter:7.9643 sec.\n",
      "iteration 17480 || Loss: 4.9585 || 10iter:7.7677 sec.\n",
      "iteration 17490 || Loss: 4.7352 || 10iter:7.7605 sec.\n",
      "iteration 17500 || Loss: 3.8382 || 10iter:7.8923 sec.\n",
      "iteration 17510 || Loss: 4.5547 || 10iter:7.9148 sec.\n",
      "iteration 17520 || Loss: 3.9093 || 10iter:7.9093 sec.\n",
      "iteration 17530 || Loss: 5.1965 || 10iter:8.0211 sec.\n",
      "iteration 17540 || Loss: 5.6881 || 10iter:7.7968 sec.\n",
      "iteration 17550 || Loss: 4.7473 || 10iter:7.9296 sec.\n",
      "iteration 17560 || Loss: 4.7136 || 10iter:7.9449 sec.\n",
      "iteration 17570 || Loss: 5.2189 || 10iter:7.8547 sec.\n",
      "iteration 17580 || Loss: 5.9803 || 10iter:7.8800 sec.\n",
      "iteration 17590 || Loss: 4.4278 || 10iter:7.8076 sec.\n",
      "iteration 17600 || Loss: 5.2120 || 10iter:8.0292 sec.\n",
      "iteration 17610 || Loss: 5.5182 || 10iter:7.9036 sec.\n",
      "iteration 17620 || Loss: 4.3678 || 10iter:7.8219 sec.\n",
      "iteration 17630 || Loss: 5.5821 || 10iter:7.9624 sec.\n",
      "iteration 17640 || Loss: 5.0431 || 10iter:7.9464 sec.\n",
      "iteration 17650 || Loss: 4.2528 || 10iter:7.7888 sec.\n",
      "iteration 17660 || Loss: 4.8283 || 10iter:7.7770 sec.\n",
      "iteration 17670 || Loss: 3.5619 || 10iter:7.9146 sec.\n",
      "iteration 17680 || Loss: 4.0567 || 10iter:7.7798 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 17690 || Loss: 3.6955 || 10iter:7.8888 sec.\n",
      "iteration 17700 || Loss: 4.7683 || 10iter:7.9083 sec.\n",
      "iteration 17710 || Loss: 4.8337 || 10iter:7.8078 sec.\n",
      "iteration 17720 || Loss: 4.9308 || 10iter:7.8683 sec.\n",
      "iteration 17730 || Loss: 4.7263 || 10iter:7.7567 sec.\n",
      "iteration 17740 || Loss: 4.5197 || 10iter:7.9293 sec.\n",
      "iteration 17750 || Loss: 4.6765 || 10iter:7.9570 sec.\n",
      "iteration 17760 || Loss: 5.9615 || 10iter:7.8331 sec.\n",
      "iteration 17770 || Loss: 4.4306 || 10iter:7.8449 sec.\n",
      "iteration 17780 || Loss: 4.0522 || 10iter:7.9432 sec.\n",
      "iteration 17790 || Loss: 3.8226 || 10iter:7.9179 sec.\n",
      "iteration 17800 || Loss: 6.1673 || 10iter:7.7602 sec.\n",
      "iteration 17810 || Loss: 4.6141 || 10iter:7.8332 sec.\n",
      "iteration 17820 || Loss: 4.7176 || 10iter:7.8853 sec.\n",
      "iteration 17830 || Loss: 5.4819 || 10iter:7.9315 sec.\n",
      "iteration 17840 || Loss: 5.7478 || 10iter:7.8484 sec.\n",
      "iteration 17850 || Loss: 4.8595 || 10iter:7.8810 sec.\n",
      "iteration 17860 || Loss: 3.9477 || 10iter:7.8267 sec.\n",
      "iteration 17870 || Loss: 4.7012 || 10iter:7.7819 sec.\n",
      "-------------------\n",
      "epoch 25 || Epoch_TRAIN_Loss:3371.2380 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 26/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 17880 || Loss: 5.0514 || 10iter:3.8209 sec.\n",
      "iteration 17890 || Loss: 4.9072 || 10iter:7.8908 sec.\n",
      "iteration 17900 || Loss: 4.6643 || 10iter:7.8569 sec.\n",
      "iteration 17910 || Loss: 5.3636 || 10iter:7.8816 sec.\n",
      "iteration 17920 || Loss: 3.9883 || 10iter:7.8709 sec.\n",
      "iteration 17930 || Loss: 5.1247 || 10iter:7.7999 sec.\n",
      "iteration 17940 || Loss: 4.6416 || 10iter:7.9293 sec.\n",
      "iteration 17950 || Loss: 4.0483 || 10iter:7.8229 sec.\n",
      "iteration 17960 || Loss: 5.8095 || 10iter:7.7669 sec.\n",
      "iteration 17970 || Loss: 4.2652 || 10iter:7.9314 sec.\n",
      "iteration 17980 || Loss: 4.5717 || 10iter:7.7494 sec.\n",
      "iteration 17990 || Loss: 5.6181 || 10iter:8.0198 sec.\n",
      "iteration 18000 || Loss: 4.7594 || 10iter:7.8030 sec.\n",
      "iteration 18010 || Loss: 4.3109 || 10iter:7.8772 sec.\n",
      "iteration 18020 || Loss: 3.8712 || 10iter:7.8344 sec.\n",
      "iteration 18030 || Loss: 4.0445 || 10iter:8.0208 sec.\n",
      "iteration 18040 || Loss: 4.1979 || 10iter:7.8882 sec.\n",
      "iteration 18050 || Loss: 6.0131 || 10iter:7.8551 sec.\n",
      "iteration 18060 || Loss: 4.8663 || 10iter:7.8320 sec.\n",
      "iteration 18070 || Loss: 4.0950 || 10iter:7.8138 sec.\n",
      "iteration 18080 || Loss: 4.0627 || 10iter:7.9901 sec.\n",
      "iteration 18090 || Loss: 4.2961 || 10iter:7.8665 sec.\n",
      "iteration 18100 || Loss: 3.9871 || 10iter:7.8235 sec.\n",
      "iteration 18110 || Loss: 5.9680 || 10iter:7.8986 sec.\n",
      "iteration 18120 || Loss: 4.2431 || 10iter:7.9535 sec.\n",
      "iteration 18130 || Loss: 6.2834 || 10iter:8.0234 sec.\n",
      "iteration 18140 || Loss: 4.1571 || 10iter:7.9324 sec.\n",
      "iteration 18150 || Loss: 4.6239 || 10iter:7.8881 sec.\n",
      "iteration 18160 || Loss: 4.4830 || 10iter:7.8229 sec.\n",
      "iteration 18170 || Loss: 4.3693 || 10iter:7.8337 sec.\n",
      "iteration 18180 || Loss: 4.4595 || 10iter:7.8647 sec.\n",
      "iteration 18190 || Loss: 6.2045 || 10iter:7.9484 sec.\n",
      "iteration 18200 || Loss: 5.6012 || 10iter:7.9767 sec.\n",
      "iteration 18210 || Loss: 4.3287 || 10iter:7.8203 sec.\n",
      "iteration 18220 || Loss: 3.7663 || 10iter:7.9441 sec.\n",
      "iteration 18230 || Loss: 3.2993 || 10iter:7.8185 sec.\n",
      "iteration 18240 || Loss: 5.7048 || 10iter:7.8920 sec.\n",
      "iteration 18250 || Loss: 4.4140 || 10iter:7.8698 sec.\n",
      "iteration 18260 || Loss: 4.7158 || 10iter:7.9395 sec.\n",
      "iteration 18270 || Loss: 4.3955 || 10iter:7.8754 sec.\n",
      "iteration 18280 || Loss: 5.6101 || 10iter:7.7890 sec.\n",
      "iteration 18290 || Loss: 4.2581 || 10iter:7.8443 sec.\n",
      "iteration 18300 || Loss: 4.6590 || 10iter:7.8659 sec.\n",
      "iteration 18310 || Loss: 4.3240 || 10iter:7.8607 sec.\n",
      "iteration 18320 || Loss: 4.3160 || 10iter:7.8900 sec.\n",
      "iteration 18330 || Loss: 4.7461 || 10iter:8.0713 sec.\n",
      "iteration 18340 || Loss: 4.5735 || 10iter:7.9425 sec.\n",
      "iteration 18350 || Loss: 5.0473 || 10iter:7.9326 sec.\n",
      "iteration 18360 || Loss: 4.1040 || 10iter:7.8438 sec.\n",
      "iteration 18370 || Loss: 4.7612 || 10iter:7.7337 sec.\n",
      "iteration 18380 || Loss: 3.8404 || 10iter:7.9289 sec.\n",
      "iteration 18390 || Loss: 5.0496 || 10iter:7.8893 sec.\n",
      "iteration 18400 || Loss: 5.2612 || 10iter:7.9870 sec.\n",
      "iteration 18410 || Loss: 4.7802 || 10iter:7.8827 sec.\n",
      "iteration 18420 || Loss: 5.6828 || 10iter:7.8780 sec.\n",
      "iteration 18430 || Loss: 4.3724 || 10iter:7.9241 sec.\n",
      "iteration 18440 || Loss: 5.2989 || 10iter:7.7914 sec.\n",
      "iteration 18450 || Loss: 4.1447 || 10iter:7.7999 sec.\n",
      "iteration 18460 || Loss: 4.9826 || 10iter:7.9141 sec.\n",
      "iteration 18470 || Loss: 4.2253 || 10iter:7.8446 sec.\n",
      "iteration 18480 || Loss: 4.4533 || 10iter:7.8552 sec.\n",
      "iteration 18490 || Loss: 3.4719 || 10iter:7.9275 sec.\n",
      "iteration 18500 || Loss: 5.2678 || 10iter:7.9130 sec.\n",
      "iteration 18510 || Loss: 4.2517 || 10iter:7.9434 sec.\n",
      "iteration 18520 || Loss: 3.6161 || 10iter:7.8662 sec.\n",
      "iteration 18530 || Loss: 4.8763 || 10iter:8.0011 sec.\n",
      "iteration 18540 || Loss: 4.6810 || 10iter:7.8226 sec.\n",
      "iteration 18550 || Loss: 4.9355 || 10iter:7.8861 sec.\n",
      "iteration 18560 || Loss: 4.2388 || 10iter:7.9556 sec.\n",
      "iteration 18570 || Loss: 5.2529 || 10iter:7.9672 sec.\n",
      "iteration 18580 || Loss: 4.5797 || 10iter:7.8568 sec.\n",
      "iteration 18590 || Loss: 5.4847 || 10iter:7.6332 sec.\n",
      "-------------------\n",
      "epoch 26 || Epoch_TRAIN_Loss:3342.1896 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 27/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 18600 || Loss: 5.0489 || 10iter:7.7342 sec.\n",
      "iteration 18610 || Loss: 5.4061 || 10iter:7.8106 sec.\n",
      "iteration 18620 || Loss: 3.7586 || 10iter:7.9339 sec.\n",
      "iteration 18630 || Loss: 5.7250 || 10iter:8.0261 sec.\n",
      "iteration 18640 || Loss: 5.0415 || 10iter:7.9526 sec.\n",
      "iteration 18650 || Loss: 4.4559 || 10iter:7.9018 sec.\n",
      "iteration 18660 || Loss: 3.1697 || 10iter:7.9013 sec.\n",
      "iteration 18670 || Loss: 3.7377 || 10iter:7.7625 sec.\n",
      "iteration 18680 || Loss: 3.7745 || 10iter:7.8958 sec.\n",
      "iteration 18690 || Loss: 4.4707 || 10iter:7.7850 sec.\n",
      "iteration 18700 || Loss: 3.9072 || 10iter:7.8164 sec.\n",
      "iteration 18710 || Loss: 4.1903 || 10iter:7.8156 sec.\n",
      "iteration 18720 || Loss: 4.4379 || 10iter:7.7876 sec.\n",
      "iteration 18730 || Loss: 4.7352 || 10iter:7.9240 sec.\n",
      "iteration 18740 || Loss: 4.6277 || 10iter:7.8799 sec.\n",
      "iteration 18750 || Loss: 5.6388 || 10iter:7.8902 sec.\n",
      "iteration 18760 || Loss: 5.1118 || 10iter:7.8683 sec.\n",
      "iteration 18770 || Loss: 4.1611 || 10iter:7.9460 sec.\n",
      "iteration 18780 || Loss: 3.6018 || 10iter:7.8708 sec.\n",
      "iteration 18790 || Loss: 4.9309 || 10iter:7.7956 sec.\n",
      "iteration 18800 || Loss: 4.8635 || 10iter:7.8691 sec.\n",
      "iteration 18810 || Loss: 5.2938 || 10iter:7.8543 sec.\n",
      "iteration 18820 || Loss: 5.1504 || 10iter:7.8534 sec.\n",
      "iteration 18830 || Loss: 3.5128 || 10iter:7.9597 sec.\n",
      "iteration 18840 || Loss: 5.1141 || 10iter:7.8602 sec.\n",
      "iteration 18850 || Loss: 4.4202 || 10iter:7.8569 sec.\n",
      "iteration 18860 || Loss: 5.1945 || 10iter:7.9328 sec.\n",
      "iteration 18870 || Loss: 4.6371 || 10iter:7.8249 sec.\n",
      "iteration 18880 || Loss: 3.7696 || 10iter:7.9262 sec.\n",
      "iteration 18890 || Loss: 6.0765 || 10iter:7.8518 sec.\n",
      "iteration 18900 || Loss: 4.4343 || 10iter:7.8879 sec.\n",
      "iteration 18910 || Loss: 5.5437 || 10iter:7.9294 sec.\n",
      "iteration 18920 || Loss: 3.8001 || 10iter:7.9019 sec.\n",
      "iteration 18930 || Loss: 4.8986 || 10iter:7.8664 sec.\n",
      "iteration 18940 || Loss: 4.9409 || 10iter:7.8164 sec.\n",
      "iteration 18950 || Loss: 4.2870 || 10iter:7.8897 sec.\n",
      "iteration 18960 || Loss: 5.3953 || 10iter:7.8596 sec.\n",
      "iteration 18970 || Loss: 4.4250 || 10iter:7.8010 sec.\n",
      "iteration 18980 || Loss: 3.7821 || 10iter:7.8799 sec.\n",
      "iteration 18990 || Loss: 5.0408 || 10iter:8.0062 sec.\n",
      "iteration 19000 || Loss: 3.3075 || 10iter:7.8280 sec.\n",
      "iteration 19010 || Loss: 4.9298 || 10iter:7.8931 sec.\n",
      "iteration 19020 || Loss: 4.8033 || 10iter:7.9736 sec.\n",
      "iteration 19030 || Loss: 3.7755 || 10iter:7.8412 sec.\n",
      "iteration 19040 || Loss: 4.4750 || 10iter:7.9530 sec.\n",
      "iteration 19050 || Loss: 6.6443 || 10iter:7.9096 sec.\n",
      "iteration 19060 || Loss: 4.9456 || 10iter:8.1151 sec.\n",
      "iteration 19070 || Loss: 4.0231 || 10iter:7.7744 sec.\n",
      "iteration 19080 || Loss: 4.3863 || 10iter:7.7761 sec.\n",
      "iteration 19090 || Loss: 4.6891 || 10iter:7.9200 sec.\n",
      "iteration 19100 || Loss: 5.5031 || 10iter:7.8899 sec.\n",
      "iteration 19110 || Loss: 5.0812 || 10iter:7.9322 sec.\n",
      "iteration 19120 || Loss: 5.4173 || 10iter:7.8256 sec.\n",
      "iteration 19130 || Loss: 4.6628 || 10iter:8.0009 sec.\n",
      "iteration 19140 || Loss: 4.6270 || 10iter:7.8840 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 19150 || Loss: 4.0436 || 10iter:7.9221 sec.\n",
      "iteration 19160 || Loss: 3.9897 || 10iter:7.6324 sec.\n",
      "iteration 19170 || Loss: 4.5373 || 10iter:7.8271 sec.\n",
      "iteration 19180 || Loss: 5.3137 || 10iter:7.8136 sec.\n",
      "iteration 19190 || Loss: 4.3889 || 10iter:7.8903 sec.\n",
      "iteration 19200 || Loss: 4.0549 || 10iter:7.9338 sec.\n",
      "iteration 19210 || Loss: 4.2942 || 10iter:7.9051 sec.\n",
      "iteration 19220 || Loss: 4.4814 || 10iter:7.8022 sec.\n",
      "iteration 19230 || Loss: 3.8768 || 10iter:8.0055 sec.\n",
      "iteration 19240 || Loss: 4.7639 || 10iter:7.9263 sec.\n",
      "iteration 19250 || Loss: 4.1622 || 10iter:7.8325 sec.\n",
      "iteration 19260 || Loss: 4.8812 || 10iter:7.8498 sec.\n",
      "iteration 19270 || Loss: 4.6545 || 10iter:7.8497 sec.\n",
      "iteration 19280 || Loss: 3.9747 || 10iter:7.9005 sec.\n",
      "iteration 19290 || Loss: 4.8967 || 10iter:7.9417 sec.\n",
      "iteration 19300 || Loss: 4.7719 || 10iter:8.0069 sec.\n",
      "-------------------\n",
      "epoch 27 || Epoch_TRAIN_Loss:3290.1990 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 28/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 19310 || Loss: 4.7580 || 10iter:3.7286 sec.\n",
      "iteration 19320 || Loss: 3.5089 || 10iter:7.9111 sec.\n",
      "iteration 19330 || Loss: 4.8675 || 10iter:7.9424 sec.\n",
      "iteration 19340 || Loss: 4.6648 || 10iter:7.8831 sec.\n",
      "iteration 19350 || Loss: 5.2422 || 10iter:7.8734 sec.\n",
      "iteration 19360 || Loss: 5.6438 || 10iter:7.8445 sec.\n",
      "iteration 19370 || Loss: 4.1301 || 10iter:7.7953 sec.\n",
      "iteration 19380 || Loss: 5.4898 || 10iter:7.9084 sec.\n",
      "iteration 19390 || Loss: 4.8677 || 10iter:7.9711 sec.\n",
      "iteration 19400 || Loss: 4.1681 || 10iter:7.8668 sec.\n",
      "iteration 19410 || Loss: 3.2050 || 10iter:7.9103 sec.\n",
      "iteration 19420 || Loss: 4.4319 || 10iter:8.0175 sec.\n",
      "iteration 19430 || Loss: 4.5890 || 10iter:7.8260 sec.\n",
      "iteration 19440 || Loss: 5.2430 || 10iter:7.8900 sec.\n",
      "iteration 19450 || Loss: 4.2654 || 10iter:7.9205 sec.\n",
      "iteration 19460 || Loss: 4.4033 || 10iter:7.7603 sec.\n",
      "iteration 19470 || Loss: 4.0924 || 10iter:7.8673 sec.\n",
      "iteration 19480 || Loss: 4.0330 || 10iter:8.0110 sec.\n",
      "iteration 19490 || Loss: 4.8417 || 10iter:7.9411 sec.\n",
      "iteration 19500 || Loss: 5.2708 || 10iter:7.8632 sec.\n",
      "iteration 19510 || Loss: 3.4132 || 10iter:7.7182 sec.\n",
      "iteration 19520 || Loss: 5.0165 || 10iter:7.9418 sec.\n",
      "iteration 19530 || Loss: 3.7512 || 10iter:7.8398 sec.\n",
      "iteration 19540 || Loss: 4.4110 || 10iter:7.8952 sec.\n",
      "iteration 19550 || Loss: 4.2597 || 10iter:7.8893 sec.\n",
      "iteration 19560 || Loss: 4.5543 || 10iter:7.8717 sec.\n",
      "iteration 19570 || Loss: 3.7289 || 10iter:7.7226 sec.\n",
      "iteration 19580 || Loss: 4.0851 || 10iter:7.7842 sec.\n",
      "iteration 19590 || Loss: 3.6603 || 10iter:7.7720 sec.\n",
      "iteration 19600 || Loss: 3.6656 || 10iter:7.7121 sec.\n",
      "iteration 19610 || Loss: 5.0050 || 10iter:7.8506 sec.\n",
      "iteration 19620 || Loss: 4.6786 || 10iter:7.9065 sec.\n",
      "iteration 19630 || Loss: 4.2722 || 10iter:7.8619 sec.\n",
      "iteration 19640 || Loss: 4.5635 || 10iter:7.9506 sec.\n",
      "iteration 19650 || Loss: 4.5643 || 10iter:7.9801 sec.\n",
      "iteration 19660 || Loss: 4.3194 || 10iter:7.8742 sec.\n",
      "iteration 19670 || Loss: 3.8396 || 10iter:7.7900 sec.\n",
      "iteration 19680 || Loss: 4.2412 || 10iter:7.8170 sec.\n",
      "iteration 19690 || Loss: 4.3768 || 10iter:7.8691 sec.\n",
      "iteration 19700 || Loss: 4.0171 || 10iter:7.8479 sec.\n",
      "iteration 19710 || Loss: 3.4719 || 10iter:7.8550 sec.\n",
      "iteration 19720 || Loss: 4.7915 || 10iter:7.8679 sec.\n",
      "iteration 19730 || Loss: 3.6628 || 10iter:7.8844 sec.\n",
      "iteration 19740 || Loss: 4.1693 || 10iter:7.8062 sec.\n",
      "iteration 19750 || Loss: 5.0677 || 10iter:7.8628 sec.\n",
      "iteration 19760 || Loss: 4.6890 || 10iter:7.9099 sec.\n",
      "iteration 19770 || Loss: 5.1328 || 10iter:7.9422 sec.\n",
      "iteration 19780 || Loss: 3.6817 || 10iter:7.8065 sec.\n",
      "iteration 19790 || Loss: 5.4047 || 10iter:7.8945 sec.\n",
      "iteration 19800 || Loss: 4.5844 || 10iter:7.8796 sec.\n",
      "iteration 19810 || Loss: 4.7407 || 10iter:7.8873 sec.\n",
      "iteration 19820 || Loss: 3.9740 || 10iter:7.8754 sec.\n",
      "iteration 19830 || Loss: 4.7385 || 10iter:7.8553 sec.\n",
      "iteration 19840 || Loss: 6.0546 || 10iter:7.9111 sec.\n",
      "iteration 19850 || Loss: 3.8880 || 10iter:7.8229 sec.\n",
      "iteration 19860 || Loss: 4.3321 || 10iter:7.8815 sec.\n",
      "iteration 19870 || Loss: 4.4163 || 10iter:7.8342 sec.\n",
      "iteration 19880 || Loss: 4.7923 || 10iter:7.9106 sec.\n",
      "iteration 19890 || Loss: 4.1053 || 10iter:7.8747 sec.\n",
      "iteration 19900 || Loss: 5.0062 || 10iter:7.8811 sec.\n",
      "iteration 19910 || Loss: 3.0613 || 10iter:7.7767 sec.\n",
      "iteration 19920 || Loss: 4.8591 || 10iter:7.8889 sec.\n",
      "iteration 19930 || Loss: 5.1708 || 10iter:7.9659 sec.\n",
      "iteration 19940 || Loss: 4.7860 || 10iter:7.8872 sec.\n",
      "iteration 19950 || Loss: 4.9920 || 10iter:7.8728 sec.\n",
      "iteration 19960 || Loss: 3.5431 || 10iter:7.8041 sec.\n",
      "iteration 19970 || Loss: 4.5546 || 10iter:7.9086 sec.\n",
      "iteration 19980 || Loss: 4.2391 || 10iter:7.9163 sec.\n",
      "iteration 19990 || Loss: 5.0082 || 10iter:7.8741 sec.\n",
      "iteration 20000 || Loss: 3.7640 || 10iter:7.8413 sec.\n",
      "iteration 20010 || Loss: 3.7434 || 10iter:7.9741 sec.\n",
      "iteration 20020 || Loss: 3.6288 || 10iter:7.7337 sec.\n",
      "-------------------\n",
      "epoch 28 || Epoch_TRAIN_Loss:3275.7054 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 29/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 20030 || Loss: 3.4004 || 10iter:7.8507 sec.\n",
      "iteration 20040 || Loss: 5.9239 || 10iter:7.8318 sec.\n",
      "iteration 20050 || Loss: 5.0082 || 10iter:7.8605 sec.\n",
      "iteration 20060 || Loss: 4.1212 || 10iter:7.8104 sec.\n",
      "iteration 20070 || Loss: 3.9713 || 10iter:7.9108 sec.\n",
      "iteration 20080 || Loss: 5.1470 || 10iter:7.8713 sec.\n",
      "iteration 20090 || Loss: 4.4036 || 10iter:7.8557 sec.\n",
      "iteration 20100 || Loss: 3.8187 || 10iter:7.8658 sec.\n",
      "iteration 20110 || Loss: 4.8354 || 10iter:7.9816 sec.\n",
      "iteration 20120 || Loss: 4.1893 || 10iter:7.9111 sec.\n",
      "iteration 20130 || Loss: 5.2986 || 10iter:7.9096 sec.\n",
      "iteration 20140 || Loss: 5.6795 || 10iter:7.8951 sec.\n",
      "iteration 20150 || Loss: 4.9532 || 10iter:7.8790 sec.\n",
      "iteration 20160 || Loss: 5.1557 || 10iter:7.8145 sec.\n",
      "iteration 20170 || Loss: 5.1058 || 10iter:7.9165 sec.\n",
      "iteration 20180 || Loss: 3.3040 || 10iter:7.8201 sec.\n",
      "iteration 20190 || Loss: 3.1974 || 10iter:7.8117 sec.\n",
      "iteration 20200 || Loss: 4.3480 || 10iter:7.9181 sec.\n",
      "iteration 20210 || Loss: 3.9788 || 10iter:7.9694 sec.\n",
      "iteration 20220 || Loss: 4.5510 || 10iter:7.8597 sec.\n",
      "iteration 20230 || Loss: 4.7529 || 10iter:7.8770 sec.\n",
      "iteration 20240 || Loss: 4.0320 || 10iter:7.8720 sec.\n",
      "iteration 20250 || Loss: 5.2952 || 10iter:7.9104 sec.\n",
      "iteration 20260 || Loss: 3.5039 || 10iter:7.9421 sec.\n",
      "iteration 20270 || Loss: 4.7014 || 10iter:7.8634 sec.\n",
      "iteration 20280 || Loss: 4.7217 || 10iter:7.9896 sec.\n",
      "iteration 20290 || Loss: 3.3815 || 10iter:7.9144 sec.\n",
      "iteration 20300 || Loss: 4.3198 || 10iter:7.8191 sec.\n",
      "iteration 20310 || Loss: 5.0657 || 10iter:7.8932 sec.\n",
      "iteration 20320 || Loss: 4.8363 || 10iter:7.8893 sec.\n",
      "iteration 20330 || Loss: 4.4936 || 10iter:8.0123 sec.\n",
      "iteration 20340 || Loss: 4.6656 || 10iter:7.8483 sec.\n",
      "iteration 20350 || Loss: 5.2559 || 10iter:7.8266 sec.\n",
      "iteration 20360 || Loss: 5.2303 || 10iter:7.8003 sec.\n",
      "iteration 20370 || Loss: 5.1842 || 10iter:7.8178 sec.\n",
      "iteration 20380 || Loss: 4.5180 || 10iter:7.8790 sec.\n",
      "iteration 20390 || Loss: 3.9673 || 10iter:7.8651 sec.\n",
      "iteration 20400 || Loss: 4.2211 || 10iter:7.8381 sec.\n",
      "iteration 20410 || Loss: 4.1811 || 10iter:7.8111 sec.\n",
      "iteration 20420 || Loss: 4.8517 || 10iter:7.7929 sec.\n",
      "iteration 20430 || Loss: 3.8114 || 10iter:7.8792 sec.\n",
      "iteration 20440 || Loss: 4.0106 || 10iter:7.7373 sec.\n",
      "iteration 20450 || Loss: 4.4583 || 10iter:7.9086 sec.\n",
      "iteration 20460 || Loss: 4.5663 || 10iter:7.9302 sec.\n",
      "iteration 20470 || Loss: 3.7557 || 10iter:7.9181 sec.\n",
      "iteration 20480 || Loss: 4.4351 || 10iter:7.8160 sec.\n",
      "iteration 20490 || Loss: 3.9192 || 10iter:7.9199 sec.\n",
      "iteration 20500 || Loss: 4.8186 || 10iter:7.8434 sec.\n",
      "iteration 20510 || Loss: 4.3241 || 10iter:7.8275 sec.\n",
      "iteration 20520 || Loss: 4.4525 || 10iter:7.8427 sec.\n",
      "iteration 20530 || Loss: 4.3947 || 10iter:7.8500 sec.\n",
      "iteration 20540 || Loss: 4.3284 || 10iter:7.8306 sec.\n",
      "iteration 20550 || Loss: 4.3537 || 10iter:7.9210 sec.\n",
      "iteration 20560 || Loss: 5.6259 || 10iter:7.9242 sec.\n",
      "iteration 20570 || Loss: 5.1173 || 10iter:7.8914 sec.\n",
      "iteration 20580 || Loss: 5.0415 || 10iter:7.9468 sec.\n",
      "iteration 20590 || Loss: 4.5962 || 10iter:7.8435 sec.\n",
      "iteration 20600 || Loss: 4.3193 || 10iter:7.9431 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 20610 || Loss: 4.2541 || 10iter:7.8450 sec.\n",
      "iteration 20620 || Loss: 4.2200 || 10iter:7.8680 sec.\n",
      "iteration 20630 || Loss: 4.2362 || 10iter:7.7893 sec.\n",
      "iteration 20640 || Loss: 4.9019 || 10iter:7.8907 sec.\n",
      "iteration 20650 || Loss: 4.4951 || 10iter:7.8400 sec.\n",
      "iteration 20660 || Loss: 4.6419 || 10iter:7.9078 sec.\n",
      "iteration 20670 || Loss: 5.4173 || 10iter:7.8396 sec.\n",
      "iteration 20680 || Loss: 3.5860 || 10iter:7.8483 sec.\n",
      "iteration 20690 || Loss: 3.9087 || 10iter:8.0125 sec.\n",
      "iteration 20700 || Loss: 4.6575 || 10iter:7.8649 sec.\n",
      "iteration 20710 || Loss: 4.2602 || 10iter:7.8655 sec.\n",
      "iteration 20720 || Loss: 5.0546 || 10iter:7.8143 sec.\n",
      "iteration 20730 || Loss: 4.0661 || 10iter:7.8838 sec.\n",
      "-------------------\n",
      "epoch 29 || Epoch_TRAIN_Loss:3204.3970 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 30/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 20740 || Loss: 5.5031 || 10iter:3.7851 sec.\n",
      "iteration 20750 || Loss: 4.7158 || 10iter:8.0186 sec.\n",
      "iteration 20760 || Loss: 3.8236 || 10iter:7.8929 sec.\n",
      "iteration 20770 || Loss: 3.8484 || 10iter:7.7671 sec.\n",
      "iteration 20780 || Loss: 5.0522 || 10iter:7.8695 sec.\n",
      "iteration 20790 || Loss: 2.9608 || 10iter:7.8620 sec.\n",
      "iteration 20800 || Loss: 4.8873 || 10iter:7.8760 sec.\n",
      "iteration 20810 || Loss: 4.3836 || 10iter:7.9222 sec.\n",
      "iteration 20820 || Loss: 4.3216 || 10iter:7.7882 sec.\n",
      "iteration 20830 || Loss: 3.6667 || 10iter:7.9073 sec.\n",
      "iteration 20840 || Loss: 4.6342 || 10iter:7.7994 sec.\n",
      "iteration 20850 || Loss: 4.6567 || 10iter:7.8420 sec.\n",
      "iteration 20860 || Loss: 4.4260 || 10iter:7.8787 sec.\n",
      "iteration 20870 || Loss: 3.2017 || 10iter:7.9656 sec.\n",
      "iteration 20880 || Loss: 4.3263 || 10iter:7.8732 sec.\n",
      "iteration 20890 || Loss: 3.6465 || 10iter:7.7273 sec.\n",
      "iteration 20900 || Loss: 5.2019 || 10iter:7.9018 sec.\n",
      "iteration 20910 || Loss: 5.7492 || 10iter:7.8574 sec.\n",
      "iteration 20920 || Loss: 4.6671 || 10iter:8.0051 sec.\n",
      "iteration 20930 || Loss: 4.9297 || 10iter:7.8848 sec.\n",
      "iteration 20940 || Loss: 4.3300 || 10iter:7.9338 sec.\n",
      "iteration 20950 || Loss: 4.7769 || 10iter:7.8581 sec.\n",
      "iteration 20960 || Loss: 4.9872 || 10iter:7.9482 sec.\n",
      "iteration 20970 || Loss: 5.3522 || 10iter:8.1434 sec.\n",
      "iteration 20980 || Loss: 3.9606 || 10iter:7.9535 sec.\n",
      "iteration 20990 || Loss: 3.7065 || 10iter:8.1056 sec.\n",
      "iteration 21000 || Loss: 5.0777 || 10iter:8.0637 sec.\n",
      "iteration 21010 || Loss: 5.2585 || 10iter:8.0831 sec.\n",
      "iteration 21020 || Loss: 4.3990 || 10iter:8.1988 sec.\n",
      "iteration 21030 || Loss: 4.2268 || 10iter:8.0678 sec.\n",
      "iteration 21040 || Loss: 4.9438 || 10iter:8.0756 sec.\n",
      "iteration 21050 || Loss: 3.9865 || 10iter:8.1408 sec.\n",
      "iteration 21060 || Loss: 4.7468 || 10iter:8.1707 sec.\n",
      "iteration 21070 || Loss: 3.3832 || 10iter:8.1266 sec.\n",
      "iteration 21080 || Loss: 3.8247 || 10iter:8.1105 sec.\n",
      "iteration 21090 || Loss: 4.7479 || 10iter:8.0851 sec.\n",
      "iteration 21100 || Loss: 4.3289 || 10iter:8.1379 sec.\n",
      "iteration 21110 || Loss: 4.2050 || 10iter:8.0149 sec.\n",
      "iteration 21120 || Loss: 6.3066 || 10iter:8.1110 sec.\n",
      "iteration 21130 || Loss: 4.2828 || 10iter:7.9999 sec.\n",
      "iteration 21140 || Loss: 4.9350 || 10iter:8.1612 sec.\n",
      "iteration 21150 || Loss: 3.8057 || 10iter:8.1454 sec.\n",
      "iteration 21160 || Loss: 5.1515 || 10iter:8.0467 sec.\n",
      "iteration 21170 || Loss: 3.8041 || 10iter:8.0573 sec.\n",
      "iteration 21180 || Loss: 3.7765 || 10iter:8.2492 sec.\n",
      "iteration 21190 || Loss: 4.5487 || 10iter:7.9619 sec.\n",
      "iteration 21200 || Loss: 4.2119 || 10iter:8.0453 sec.\n",
      "iteration 21210 || Loss: 3.0684 || 10iter:8.1366 sec.\n",
      "iteration 21220 || Loss: 5.4461 || 10iter:8.0493 sec.\n",
      "iteration 21230 || Loss: 4.7994 || 10iter:8.1487 sec.\n",
      "iteration 21240 || Loss: 3.6173 || 10iter:8.1924 sec.\n",
      "iteration 21250 || Loss: 3.7394 || 10iter:8.1680 sec.\n",
      "iteration 21260 || Loss: 4.7236 || 10iter:8.1562 sec.\n",
      "iteration 21270 || Loss: 4.2394 || 10iter:8.0698 sec.\n",
      "iteration 21280 || Loss: 5.0938 || 10iter:8.1198 sec.\n",
      "iteration 21290 || Loss: 4.2967 || 10iter:8.1171 sec.\n",
      "iteration 21300 || Loss: 4.0198 || 10iter:8.1043 sec.\n",
      "iteration 21310 || Loss: 4.3098 || 10iter:8.0926 sec.\n",
      "iteration 21320 || Loss: 3.9965 || 10iter:8.1128 sec.\n",
      "iteration 21330 || Loss: 3.9991 || 10iter:8.0369 sec.\n",
      "iteration 21340 || Loss: 5.0345 || 10iter:8.0623 sec.\n",
      "iteration 21350 || Loss: 5.0046 || 10iter:8.0848 sec.\n",
      "iteration 21360 || Loss: 3.5602 || 10iter:8.1210 sec.\n",
      "iteration 21370 || Loss: 4.3987 || 10iter:8.1667 sec.\n",
      "iteration 21380 || Loss: 3.6769 || 10iter:8.1697 sec.\n",
      "iteration 21390 || Loss: 5.1968 || 10iter:8.1556 sec.\n",
      "iteration 21400 || Loss: 3.8313 || 10iter:8.1948 sec.\n",
      "iteration 21410 || Loss: 4.0755 || 10iter:8.1141 sec.\n",
      "iteration 21420 || Loss: 3.6228 || 10iter:8.1846 sec.\n",
      "iteration 21430 || Loss: 4.4424 || 10iter:8.1552 sec.\n",
      "iteration 21440 || Loss: 3.6143 || 10iter:8.0651 sec.\n",
      "iteration 21450 || Loss: 4.4023 || 10iter:7.9678 sec.\n",
      "-------------------\n",
      " (val) \n",
      "-------------------\n",
      "epoch 30 || Epoch_TRAIN_Loss:3177.0989 || Epoch_VAL_Loss:3247.0177\n",
      "-------------------\n",
      "Epoch 31/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 21460 || Loss: 4.6520 || 10iter:8.1473 sec.\n",
      "iteration 21470 || Loss: 4.0421 || 10iter:8.0797 sec.\n",
      "iteration 21480 || Loss: 4.7098 || 10iter:8.1089 sec.\n",
      "iteration 21490 || Loss: 4.3095 || 10iter:8.1533 sec.\n",
      "iteration 21500 || Loss: 4.2768 || 10iter:8.0953 sec.\n",
      "iteration 21510 || Loss: 4.6303 || 10iter:8.0694 sec.\n",
      "iteration 21520 || Loss: 4.9791 || 10iter:8.1591 sec.\n",
      "iteration 21530 || Loss: 3.7991 || 10iter:8.1122 sec.\n",
      "iteration 21540 || Loss: 4.7200 || 10iter:8.1759 sec.\n",
      "iteration 21550 || Loss: 5.3398 || 10iter:8.1142 sec.\n",
      "iteration 21560 || Loss: 4.2112 || 10iter:8.0888 sec.\n",
      "iteration 21570 || Loss: 4.1985 || 10iter:7.7815 sec.\n",
      "iteration 21580 || Loss: 3.7114 || 10iter:7.8424 sec.\n",
      "iteration 21590 || Loss: 4.4443 || 10iter:7.9554 sec.\n",
      "iteration 21600 || Loss: 3.4826 || 10iter:7.9729 sec.\n",
      "iteration 21610 || Loss: 5.6206 || 10iter:7.9450 sec.\n",
      "iteration 21620 || Loss: 4.6951 || 10iter:7.8645 sec.\n",
      "iteration 21630 || Loss: 4.2446 || 10iter:7.9374 sec.\n",
      "iteration 21640 || Loss: 4.3126 || 10iter:7.6885 sec.\n",
      "iteration 21650 || Loss: 4.6933 || 10iter:7.9681 sec.\n",
      "iteration 21660 || Loss: 4.8196 || 10iter:7.8685 sec.\n",
      "iteration 21670 || Loss: 4.5702 || 10iter:7.7615 sec.\n",
      "iteration 21680 || Loss: 3.8532 || 10iter:7.8516 sec.\n",
      "iteration 21690 || Loss: 4.9597 || 10iter:7.7549 sec.\n",
      "iteration 21700 || Loss: 5.2957 || 10iter:7.8165 sec.\n",
      "iteration 21710 || Loss: 5.3609 || 10iter:7.7722 sec.\n",
      "iteration 21720 || Loss: 3.9057 || 10iter:7.8639 sec.\n",
      "iteration 21730 || Loss: 3.4195 || 10iter:7.9194 sec.\n",
      "iteration 21740 || Loss: 5.2375 || 10iter:7.9025 sec.\n",
      "iteration 21750 || Loss: 5.3909 || 10iter:7.8448 sec.\n",
      "iteration 21760 || Loss: 4.3651 || 10iter:7.8411 sec.\n",
      "iteration 21770 || Loss: 4.1156 || 10iter:7.9589 sec.\n",
      "iteration 21780 || Loss: 3.4302 || 10iter:7.7846 sec.\n",
      "iteration 21790 || Loss: 4.8678 || 10iter:7.8316 sec.\n",
      "iteration 21800 || Loss: 4.9918 || 10iter:7.8550 sec.\n",
      "iteration 21810 || Loss: 3.8193 || 10iter:7.9136 sec.\n",
      "iteration 21820 || Loss: 4.5071 || 10iter:7.8291 sec.\n",
      "iteration 21830 || Loss: 3.7621 || 10iter:7.8716 sec.\n",
      "iteration 21840 || Loss: 5.2461 || 10iter:7.8905 sec.\n",
      "iteration 21850 || Loss: 4.3882 || 10iter:7.7972 sec.\n",
      "iteration 21860 || Loss: 4.9009 || 10iter:7.8184 sec.\n",
      "iteration 21870 || Loss: 4.2423 || 10iter:7.7542 sec.\n",
      "iteration 21880 || Loss: 5.5484 || 10iter:7.8938 sec.\n",
      "iteration 21890 || Loss: 4.3913 || 10iter:7.8379 sec.\n",
      "iteration 21900 || Loss: 5.1900 || 10iter:7.8084 sec.\n",
      "iteration 21910 || Loss: 4.0283 || 10iter:7.8700 sec.\n",
      "iteration 21920 || Loss: 4.3664 || 10iter:7.8599 sec.\n",
      "iteration 21930 || Loss: 4.3501 || 10iter:7.9194 sec.\n",
      "iteration 21940 || Loss: 4.3660 || 10iter:7.9136 sec.\n",
      "iteration 21950 || Loss: 4.7806 || 10iter:7.7808 sec.\n",
      "iteration 21960 || Loss: 5.0902 || 10iter:8.0660 sec.\n",
      "iteration 21970 || Loss: 3.9445 || 10iter:7.9302 sec.\n",
      "iteration 21980 || Loss: 3.9870 || 10iter:7.8965 sec.\n",
      "iteration 21990 || Loss: 3.7501 || 10iter:7.8278 sec.\n",
      "iteration 22000 || Loss: 4.2384 || 10iter:7.8282 sec.\n",
      "iteration 22010 || Loss: 5.1539 || 10iter:7.9405 sec.\n",
      "iteration 22020 || Loss: 5.5130 || 10iter:7.9470 sec.\n",
      "iteration 22030 || Loss: 4.1528 || 10iter:7.8747 sec.\n",
      "iteration 22040 || Loss: 4.8431 || 10iter:7.9031 sec.\n",
      "iteration 22050 || Loss: 4.7508 || 10iter:7.7679 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 22060 || Loss: 3.9157 || 10iter:7.8088 sec.\n",
      "iteration 22070 || Loss: 4.4158 || 10iter:7.7890 sec.\n",
      "iteration 22080 || Loss: 4.8023 || 10iter:7.7853 sec.\n",
      "iteration 22090 || Loss: 3.1511 || 10iter:7.8904 sec.\n",
      "iteration 22100 || Loss: 3.7627 || 10iter:7.8726 sec.\n",
      "iteration 22110 || Loss: 5.1495 || 10iter:7.7996 sec.\n",
      "iteration 22120 || Loss: 4.2626 || 10iter:7.9134 sec.\n",
      "iteration 22130 || Loss: 5.3490 || 10iter:7.8923 sec.\n",
      "iteration 22140 || Loss: 4.1053 || 10iter:7.8988 sec.\n",
      "iteration 22150 || Loss: 4.1340 || 10iter:7.8770 sec.\n",
      "iteration 22160 || Loss: 4.3058 || 10iter:7.8786 sec.\n",
      "-------------------\n",
      "epoch 31 || Epoch_TRAIN_Loss:3155.1897 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 32/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 22170 || Loss: 3.9382 || 10iter:3.7047 sec.\n",
      "iteration 22180 || Loss: 3.0570 || 10iter:7.9731 sec.\n",
      "iteration 22190 || Loss: 3.6268 || 10iter:7.8648 sec.\n",
      "iteration 22200 || Loss: 3.7162 || 10iter:7.8431 sec.\n",
      "iteration 22210 || Loss: 5.0884 || 10iter:7.9412 sec.\n",
      "iteration 22220 || Loss: 4.1817 || 10iter:7.8492 sec.\n",
      "iteration 22230 || Loss: 5.0119 || 10iter:7.6966 sec.\n",
      "iteration 22240 || Loss: 3.8800 || 10iter:7.8893 sec.\n",
      "iteration 22250 || Loss: 3.3234 || 10iter:7.8512 sec.\n",
      "iteration 22260 || Loss: 4.6223 || 10iter:7.8523 sec.\n",
      "iteration 22270 || Loss: 3.7695 || 10iter:7.8340 sec.\n",
      "iteration 22280 || Loss: 4.7218 || 10iter:7.9493 sec.\n",
      "iteration 22290 || Loss: 5.0743 || 10iter:7.8636 sec.\n",
      "iteration 22300 || Loss: 4.9637 || 10iter:7.7083 sec.\n",
      "iteration 22310 || Loss: 4.6024 || 10iter:7.7871 sec.\n",
      "iteration 22320 || Loss: 5.8646 || 10iter:7.8306 sec.\n",
      "iteration 22330 || Loss: 4.6145 || 10iter:7.9038 sec.\n",
      "iteration 22340 || Loss: 3.9861 || 10iter:7.8587 sec.\n",
      "iteration 22350 || Loss: 3.9281 || 10iter:7.8770 sec.\n",
      "iteration 22360 || Loss: 3.8183 || 10iter:7.8852 sec.\n",
      "iteration 22370 || Loss: 4.6388 || 10iter:7.9403 sec.\n",
      "iteration 22380 || Loss: 4.1220 || 10iter:7.8906 sec.\n",
      "iteration 22390 || Loss: 5.2133 || 10iter:7.8585 sec.\n",
      "iteration 22400 || Loss: 5.4404 || 10iter:7.8000 sec.\n",
      "iteration 22410 || Loss: 4.6413 || 10iter:7.8430 sec.\n",
      "iteration 22420 || Loss: 3.8922 || 10iter:7.8692 sec.\n",
      "iteration 22430 || Loss: 3.9360 || 10iter:7.8762 sec.\n",
      "iteration 22440 || Loss: 4.2407 || 10iter:7.7957 sec.\n",
      "iteration 22450 || Loss: 3.6705 || 10iter:7.9601 sec.\n",
      "iteration 22460 || Loss: 3.5863 || 10iter:7.8834 sec.\n",
      "iteration 22470 || Loss: 4.6154 || 10iter:7.9413 sec.\n",
      "iteration 22480 || Loss: 4.0137 || 10iter:7.8432 sec.\n",
      "iteration 22490 || Loss: 3.3263 || 10iter:7.8122 sec.\n",
      "iteration 22500 || Loss: 4.4099 || 10iter:7.8865 sec.\n",
      "iteration 22510 || Loss: 4.0567 || 10iter:7.8284 sec.\n",
      "iteration 22520 || Loss: 3.4025 || 10iter:7.8544 sec.\n",
      "iteration 22530 || Loss: 4.1184 || 10iter:7.8724 sec.\n",
      "iteration 22540 || Loss: 4.9049 || 10iter:7.8961 sec.\n",
      "iteration 22550 || Loss: 4.9366 || 10iter:7.8322 sec.\n",
      "iteration 22560 || Loss: 4.5094 || 10iter:7.9257 sec.\n",
      "iteration 22570 || Loss: 4.9133 || 10iter:7.8568 sec.\n",
      "iteration 22580 || Loss: 4.7230 || 10iter:7.9188 sec.\n",
      "iteration 22590 || Loss: 4.2312 || 10iter:7.9687 sec.\n",
      "iteration 22600 || Loss: 3.9170 || 10iter:7.9276 sec.\n",
      "iteration 22610 || Loss: 3.1229 || 10iter:7.9416 sec.\n",
      "iteration 22620 || Loss: 5.0117 || 10iter:7.9174 sec.\n",
      "iteration 22630 || Loss: 4.4315 || 10iter:7.8434 sec.\n",
      "iteration 22640 || Loss: 5.5376 || 10iter:7.8161 sec.\n",
      "iteration 22650 || Loss: 5.0210 || 10iter:7.8755 sec.\n",
      "iteration 22660 || Loss: 4.1566 || 10iter:7.9602 sec.\n",
      "iteration 22670 || Loss: 4.2757 || 10iter:7.8728 sec.\n",
      "iteration 22680 || Loss: 4.1414 || 10iter:7.8685 sec.\n",
      "iteration 22690 || Loss: 3.5856 || 10iter:7.9154 sec.\n",
      "iteration 22700 || Loss: 4.6913 || 10iter:7.8595 sec.\n",
      "iteration 22710 || Loss: 4.4439 || 10iter:7.9283 sec.\n",
      "iteration 22720 || Loss: 4.4263 || 10iter:7.8336 sec.\n",
      "iteration 22730 || Loss: 4.4649 || 10iter:7.7648 sec.\n",
      "iteration 22740 || Loss: 5.4835 || 10iter:7.8278 sec.\n",
      "iteration 22750 || Loss: 5.8211 || 10iter:7.8654 sec.\n",
      "iteration 22760 || Loss: 5.3341 || 10iter:7.9988 sec.\n",
      "iteration 22770 || Loss: 3.2915 || 10iter:7.9250 sec.\n",
      "iteration 22780 || Loss: 3.8821 || 10iter:7.9317 sec.\n",
      "iteration 22790 || Loss: 4.4159 || 10iter:7.9150 sec.\n",
      "iteration 22800 || Loss: 4.5825 || 10iter:7.7734 sec.\n",
      "iteration 22810 || Loss: 3.9435 || 10iter:7.8601 sec.\n",
      "iteration 22820 || Loss: 4.0475 || 10iter:7.8769 sec.\n",
      "iteration 22830 || Loss: 4.9036 || 10iter:7.9144 sec.\n",
      "iteration 22840 || Loss: 3.7881 || 10iter:7.8821 sec.\n",
      "iteration 22850 || Loss: 5.2921 || 10iter:7.9273 sec.\n",
      "iteration 22860 || Loss: 4.5357 || 10iter:7.8638 sec.\n",
      "iteration 22870 || Loss: 4.9239 || 10iter:7.9123 sec.\n",
      "iteration 22880 || Loss: 4.2639 || 10iter:7.5547 sec.\n",
      "-------------------\n",
      "epoch 32 || Epoch_TRAIN_Loss:3111.4170 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 33/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 22890 || Loss: 3.9538 || 10iter:7.9341 sec.\n",
      "iteration 22900 || Loss: 3.7275 || 10iter:7.8813 sec.\n",
      "iteration 22910 || Loss: 4.0695 || 10iter:7.9347 sec.\n",
      "iteration 22920 || Loss: 4.0377 || 10iter:7.9773 sec.\n",
      "iteration 22930 || Loss: 4.0694 || 10iter:7.8195 sec.\n",
      "iteration 22940 || Loss: 4.5104 || 10iter:7.8309 sec.\n",
      "iteration 22950 || Loss: 4.8639 || 10iter:7.7585 sec.\n",
      "iteration 22960 || Loss: 3.9644 || 10iter:7.7560 sec.\n",
      "iteration 22970 || Loss: 4.5736 || 10iter:7.8159 sec.\n",
      "iteration 22980 || Loss: 4.3271 || 10iter:7.8255 sec.\n",
      "iteration 22990 || Loss: 4.0234 || 10iter:7.9574 sec.\n",
      "iteration 23000 || Loss: 4.1227 || 10iter:7.9210 sec.\n",
      "iteration 23010 || Loss: 4.3656 || 10iter:7.9152 sec.\n",
      "iteration 23020 || Loss: 4.3801 || 10iter:7.9673 sec.\n",
      "iteration 23030 || Loss: 4.7666 || 10iter:7.7954 sec.\n",
      "iteration 23040 || Loss: 4.7430 || 10iter:7.8999 sec.\n",
      "iteration 23050 || Loss: 4.8362 || 10iter:7.9002 sec.\n",
      "iteration 23060 || Loss: 4.4076 || 10iter:7.8924 sec.\n",
      "iteration 23070 || Loss: 4.1670 || 10iter:7.9281 sec.\n",
      "iteration 23080 || Loss: 5.1898 || 10iter:7.8791 sec.\n",
      "iteration 23090 || Loss: 3.9724 || 10iter:7.8966 sec.\n",
      "iteration 23100 || Loss: 5.3367 || 10iter:7.8830 sec.\n",
      "iteration 23110 || Loss: 4.5178 || 10iter:7.9328 sec.\n",
      "iteration 23120 || Loss: 4.1324 || 10iter:7.8116 sec.\n",
      "iteration 23130 || Loss: 4.8808 || 10iter:7.9361 sec.\n",
      "iteration 23140 || Loss: 3.8790 || 10iter:7.8713 sec.\n",
      "iteration 23150 || Loss: 3.6400 || 10iter:7.9228 sec.\n",
      "iteration 23160 || Loss: 4.0247 || 10iter:7.8752 sec.\n",
      "iteration 23170 || Loss: 4.5190 || 10iter:7.9475 sec.\n",
      "iteration 23180 || Loss: 4.1472 || 10iter:7.9179 sec.\n",
      "iteration 23190 || Loss: 4.3651 || 10iter:7.8275 sec.\n",
      "iteration 23200 || Loss: 4.2662 || 10iter:7.9670 sec.\n",
      "iteration 23210 || Loss: 4.7484 || 10iter:7.9060 sec.\n",
      "iteration 23220 || Loss: 3.5170 || 10iter:7.9839 sec.\n",
      "iteration 23230 || Loss: 4.7302 || 10iter:7.9447 sec.\n",
      "iteration 23240 || Loss: 5.5327 || 10iter:7.9077 sec.\n",
      "iteration 23250 || Loss: 4.5265 || 10iter:8.0231 sec.\n",
      "iteration 23260 || Loss: 3.5378 || 10iter:7.8960 sec.\n",
      "iteration 23270 || Loss: 3.2129 || 10iter:7.8976 sec.\n",
      "iteration 23280 || Loss: 4.4311 || 10iter:7.8510 sec.\n",
      "iteration 23290 || Loss: 4.8038 || 10iter:7.9536 sec.\n",
      "iteration 23300 || Loss: 4.1024 || 10iter:7.9635 sec.\n",
      "iteration 23310 || Loss: 3.2796 || 10iter:7.9885 sec.\n",
      "iteration 23320 || Loss: 4.9906 || 10iter:7.9291 sec.\n",
      "iteration 23330 || Loss: 4.6495 || 10iter:7.8880 sec.\n",
      "iteration 23340 || Loss: 4.3734 || 10iter:7.8767 sec.\n",
      "iteration 23350 || Loss: 4.7111 || 10iter:7.9356 sec.\n",
      "iteration 23360 || Loss: 3.6873 || 10iter:7.9206 sec.\n",
      "iteration 23370 || Loss: 4.1059 || 10iter:7.9360 sec.\n",
      "iteration 23380 || Loss: 4.2416 || 10iter:7.8583 sec.\n",
      "iteration 23390 || Loss: 3.9927 || 10iter:7.9498 sec.\n",
      "iteration 23400 || Loss: 4.9144 || 10iter:7.9235 sec.\n",
      "iteration 23410 || Loss: 5.0716 || 10iter:7.9125 sec.\n",
      "iteration 23420 || Loss: 4.1928 || 10iter:7.8886 sec.\n",
      "iteration 23430 || Loss: 4.6094 || 10iter:7.9914 sec.\n",
      "iteration 23440 || Loss: 4.6393 || 10iter:7.9575 sec.\n",
      "iteration 23450 || Loss: 3.8595 || 10iter:7.7624 sec.\n",
      "iteration 23460 || Loss: 3.9761 || 10iter:7.9215 sec.\n",
      "iteration 23470 || Loss: 3.6846 || 10iter:7.8716 sec.\n",
      "iteration 23480 || Loss: 5.7796 || 10iter:7.9655 sec.\n",
      "iteration 23490 || Loss: 3.6408 || 10iter:7.8080 sec.\n",
      "iteration 23500 || Loss: 4.2842 || 10iter:7.9283 sec.\n",
      "iteration 23510 || Loss: 4.6542 || 10iter:7.9655 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 23520 || Loss: 3.0630 || 10iter:7.8535 sec.\n",
      "iteration 23530 || Loss: 3.3813 || 10iter:8.0319 sec.\n",
      "iteration 23540 || Loss: 4.4257 || 10iter:7.9651 sec.\n",
      "iteration 23550 || Loss: 4.5321 || 10iter:7.8378 sec.\n",
      "iteration 23560 || Loss: 4.9927 || 10iter:7.8356 sec.\n",
      "iteration 23570 || Loss: 5.9328 || 10iter:7.9534 sec.\n",
      "iteration 23580 || Loss: 3.9993 || 10iter:7.8294 sec.\n",
      "iteration 23590 || Loss: 3.7237 || 10iter:7.8991 sec.\n",
      "-------------------\n",
      "epoch 33 || Epoch_TRAIN_Loss:3078.7099 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 34/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 23600 || Loss: 3.9849 || 10iter:3.7467 sec.\n",
      "iteration 23610 || Loss: 4.1835 || 10iter:7.8305 sec.\n",
      "iteration 23620 || Loss: 3.7427 || 10iter:7.9440 sec.\n",
      "iteration 23630 || Loss: 3.7783 || 10iter:7.9450 sec.\n",
      "iteration 23640 || Loss: 4.6925 || 10iter:7.9796 sec.\n",
      "iteration 23650 || Loss: 4.0865 || 10iter:7.8930 sec.\n",
      "iteration 23660 || Loss: 4.4406 || 10iter:7.8468 sec.\n",
      "iteration 23670 || Loss: 3.3777 || 10iter:7.8378 sec.\n",
      "iteration 23680 || Loss: 5.2741 || 10iter:7.8413 sec.\n",
      "iteration 23690 || Loss: 4.5870 || 10iter:7.8207 sec.\n",
      "iteration 23700 || Loss: 4.8016 || 10iter:8.0009 sec.\n",
      "iteration 23710 || Loss: 5.0646 || 10iter:7.8875 sec.\n",
      "iteration 23720 || Loss: 3.3269 || 10iter:7.7574 sec.\n",
      "iteration 23730 || Loss: 5.0839 || 10iter:7.9498 sec.\n",
      "iteration 23740 || Loss: 4.8090 || 10iter:7.8218 sec.\n",
      "iteration 23750 || Loss: 3.3034 || 10iter:7.7291 sec.\n",
      "iteration 23760 || Loss: 4.2406 || 10iter:7.8954 sec.\n",
      "iteration 23770 || Loss: 3.2387 || 10iter:7.7848 sec.\n",
      "iteration 23780 || Loss: 4.2597 || 10iter:7.8527 sec.\n",
      "iteration 23790 || Loss: 4.0751 || 10iter:7.8966 sec.\n",
      "iteration 23800 || Loss: 4.1978 || 10iter:7.8812 sec.\n",
      "iteration 23810 || Loss: 3.9357 || 10iter:7.9176 sec.\n",
      "iteration 23820 || Loss: 4.5840 || 10iter:7.8405 sec.\n",
      "iteration 23830 || Loss: 3.8607 || 10iter:7.8150 sec.\n",
      "iteration 23840 || Loss: 3.7731 || 10iter:7.8819 sec.\n",
      "iteration 23850 || Loss: 4.2572 || 10iter:7.8265 sec.\n",
      "iteration 23860 || Loss: 5.7906 || 10iter:7.9129 sec.\n",
      "iteration 23870 || Loss: 3.7227 || 10iter:7.8853 sec.\n",
      "iteration 23880 || Loss: 4.8089 || 10iter:7.8962 sec.\n",
      "iteration 23890 || Loss: 5.6838 || 10iter:7.8030 sec.\n",
      "iteration 23900 || Loss: 4.2591 || 10iter:7.8485 sec.\n",
      "iteration 23910 || Loss: 5.2792 || 10iter:8.0273 sec.\n",
      "iteration 23920 || Loss: 3.8151 || 10iter:7.8002 sec.\n",
      "iteration 23930 || Loss: 4.7760 || 10iter:7.8868 sec.\n",
      "iteration 23940 || Loss: 5.3088 || 10iter:7.8664 sec.\n",
      "iteration 23950 || Loss: 3.2062 || 10iter:7.8962 sec.\n",
      "iteration 23960 || Loss: 3.9361 || 10iter:7.8799 sec.\n",
      "iteration 23970 || Loss: 4.8418 || 10iter:7.8030 sec.\n",
      "iteration 23980 || Loss: 3.3682 || 10iter:7.8915 sec.\n",
      "iteration 23990 || Loss: 5.2204 || 10iter:7.8608 sec.\n",
      "iteration 24000 || Loss: 4.6623 || 10iter:7.9665 sec.\n",
      "iteration 24010 || Loss: 5.1083 || 10iter:7.7992 sec.\n",
      "iteration 24020 || Loss: 4.3407 || 10iter:7.8303 sec.\n",
      "iteration 24030 || Loss: 4.6191 || 10iter:7.8224 sec.\n",
      "iteration 24040 || Loss: 4.4482 || 10iter:7.8477 sec.\n",
      "iteration 24050 || Loss: 4.2280 || 10iter:7.9304 sec.\n",
      "iteration 24060 || Loss: 4.4664 || 10iter:7.9972 sec.\n",
      "iteration 24070 || Loss: 4.8434 || 10iter:7.8807 sec.\n",
      "iteration 24080 || Loss: 3.1941 || 10iter:7.8736 sec.\n",
      "iteration 24090 || Loss: 4.6689 || 10iter:7.9316 sec.\n",
      "iteration 24100 || Loss: 4.1344 || 10iter:7.8390 sec.\n",
      "iteration 24110 || Loss: 4.0717 || 10iter:7.9049 sec.\n",
      "iteration 24120 || Loss: 3.3603 || 10iter:7.7418 sec.\n",
      "iteration 24130 || Loss: 4.0191 || 10iter:7.8024 sec.\n",
      "iteration 24140 || Loss: 4.4975 || 10iter:7.9024 sec.\n",
      "iteration 24150 || Loss: 4.2546 || 10iter:7.8588 sec.\n",
      "iteration 24160 || Loss: 4.7775 || 10iter:7.9227 sec.\n",
      "iteration 24170 || Loss: 4.4895 || 10iter:7.8227 sec.\n",
      "iteration 24180 || Loss: 3.4265 || 10iter:7.9268 sec.\n",
      "iteration 24190 || Loss: 5.1039 || 10iter:7.8106 sec.\n",
      "iteration 24200 || Loss: 3.8770 || 10iter:7.8374 sec.\n",
      "iteration 24210 || Loss: 4.1994 || 10iter:7.9284 sec.\n",
      "iteration 24220 || Loss: 4.1006 || 10iter:7.8753 sec.\n",
      "iteration 24230 || Loss: 3.4259 || 10iter:7.8825 sec.\n",
      "iteration 24240 || Loss: 3.9065 || 10iter:7.8711 sec.\n",
      "iteration 24250 || Loss: 5.0395 || 10iter:7.8790 sec.\n",
      "iteration 24260 || Loss: 4.1741 || 10iter:7.9445 sec.\n",
      "iteration 24270 || Loss: 4.3641 || 10iter:7.8133 sec.\n",
      "iteration 24280 || Loss: 4.4417 || 10iter:7.8675 sec.\n",
      "iteration 24290 || Loss: 4.8194 || 10iter:7.7877 sec.\n",
      "iteration 24300 || Loss: 3.5035 || 10iter:7.7970 sec.\n",
      "iteration 24310 || Loss: 3.4718 || 10iter:7.7864 sec.\n",
      "-------------------\n",
      "epoch 34 || Epoch_TRAIN_Loss:3076.0815 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 35/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 24320 || Loss: 3.8238 || 10iter:7.8008 sec.\n",
      "iteration 24330 || Loss: 4.1817 || 10iter:7.8194 sec.\n",
      "iteration 24340 || Loss: 2.5283 || 10iter:7.9065 sec.\n",
      "iteration 24350 || Loss: 3.8190 || 10iter:7.9899 sec.\n",
      "iteration 24360 || Loss: 3.9843 || 10iter:7.9009 sec.\n",
      "iteration 24370 || Loss: 4.3428 || 10iter:7.9335 sec.\n",
      "iteration 24380 || Loss: 3.4795 || 10iter:7.9356 sec.\n",
      "iteration 24390 || Loss: 3.6297 || 10iter:7.8576 sec.\n",
      "iteration 24400 || Loss: 3.9896 || 10iter:7.8513 sec.\n",
      "iteration 24410 || Loss: 4.5245 || 10iter:7.7769 sec.\n",
      "iteration 24420 || Loss: 3.7884 || 10iter:7.8997 sec.\n",
      "iteration 24430 || Loss: 2.9596 || 10iter:7.8322 sec.\n",
      "iteration 24440 || Loss: 3.2598 || 10iter:7.7811 sec.\n",
      "iteration 24450 || Loss: 3.2433 || 10iter:7.8868 sec.\n",
      "iteration 24460 || Loss: 3.5487 || 10iter:7.9107 sec.\n",
      "iteration 24470 || Loss: 5.3335 || 10iter:7.9066 sec.\n",
      "iteration 24480 || Loss: 4.9110 || 10iter:7.9843 sec.\n",
      "iteration 24490 || Loss: 4.7010 || 10iter:7.8501 sec.\n",
      "iteration 24500 || Loss: 3.0975 || 10iter:7.7818 sec.\n",
      "iteration 24510 || Loss: 3.8404 || 10iter:7.9348 sec.\n",
      "iteration 24520 || Loss: 3.6153 || 10iter:7.8861 sec.\n",
      "iteration 24530 || Loss: 3.9729 || 10iter:7.9166 sec.\n",
      "iteration 24540 || Loss: 3.4011 || 10iter:7.8896 sec.\n",
      "iteration 24550 || Loss: 4.2360 || 10iter:7.8095 sec.\n",
      "iteration 24560 || Loss: 3.8599 || 10iter:7.8972 sec.\n",
      "iteration 24570 || Loss: 4.4805 || 10iter:7.8625 sec.\n",
      "iteration 24580 || Loss: 5.2417 || 10iter:7.8362 sec.\n",
      "iteration 24590 || Loss: 4.5796 || 10iter:7.8525 sec.\n",
      "iteration 24600 || Loss: 4.3890 || 10iter:7.8771 sec.\n",
      "iteration 24610 || Loss: 3.2946 || 10iter:7.7931 sec.\n",
      "iteration 24620 || Loss: 4.0577 || 10iter:7.9014 sec.\n",
      "iteration 24630 || Loss: 4.1296 || 10iter:7.8507 sec.\n",
      "iteration 24640 || Loss: 5.0996 || 10iter:7.8410 sec.\n",
      "iteration 24650 || Loss: 3.8182 || 10iter:7.9692 sec.\n",
      "iteration 24660 || Loss: 4.2044 || 10iter:7.8657 sec.\n",
      "iteration 24670 || Loss: 4.1022 || 10iter:7.8912 sec.\n",
      "iteration 24680 || Loss: 4.7893 || 10iter:7.8726 sec.\n",
      "iteration 24690 || Loss: 4.0928 || 10iter:8.0283 sec.\n",
      "iteration 24700 || Loss: 4.8935 || 10iter:7.8702 sec.\n",
      "iteration 24710 || Loss: 5.3790 || 10iter:7.8225 sec.\n",
      "iteration 24720 || Loss: 4.4829 || 10iter:7.8569 sec.\n",
      "iteration 24730 || Loss: 5.1857 || 10iter:7.8429 sec.\n",
      "iteration 24740 || Loss: 4.3637 || 10iter:7.8479 sec.\n",
      "iteration 24750 || Loss: 3.8664 || 10iter:7.8848 sec.\n",
      "iteration 24760 || Loss: 4.3770 || 10iter:7.8619 sec.\n",
      "iteration 24770 || Loss: 5.0276 || 10iter:7.7835 sec.\n",
      "iteration 24780 || Loss: 6.0750 || 10iter:7.8378 sec.\n",
      "iteration 24790 || Loss: 4.4418 || 10iter:7.8875 sec.\n",
      "iteration 24800 || Loss: 3.7049 || 10iter:7.7775 sec.\n",
      "iteration 24810 || Loss: 4.5374 || 10iter:7.9631 sec.\n",
      "iteration 24820 || Loss: 3.2686 || 10iter:7.8793 sec.\n",
      "iteration 24830 || Loss: 3.5826 || 10iter:7.8172 sec.\n",
      "iteration 24840 || Loss: 4.7165 || 10iter:8.0423 sec.\n",
      "iteration 24850 || Loss: 5.1339 || 10iter:7.8885 sec.\n",
      "iteration 24860 || Loss: 3.9921 || 10iter:7.9190 sec.\n",
      "iteration 24870 || Loss: 4.1421 || 10iter:7.7645 sec.\n",
      "iteration 24880 || Loss: 5.2096 || 10iter:7.7263 sec.\n",
      "iteration 24890 || Loss: 3.7652 || 10iter:7.7573 sec.\n",
      "iteration 24900 || Loss: 3.8467 || 10iter:7.8063 sec.\n",
      "iteration 24910 || Loss: 4.4375 || 10iter:8.0061 sec.\n",
      "iteration 24920 || Loss: 3.9699 || 10iter:7.8849 sec.\n",
      "iteration 24930 || Loss: 4.6260 || 10iter:7.8145 sec.\n",
      "iteration 24940 || Loss: 4.5005 || 10iter:7.8949 sec.\n",
      "iteration 24950 || Loss: 3.6389 || 10iter:7.9892 sec.\n",
      "iteration 24960 || Loss: 5.4755 || 10iter:7.9511 sec.\n",
      "iteration 24970 || Loss: 2.9337 || 10iter:7.8170 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 24980 || Loss: 3.2372 || 10iter:7.8917 sec.\n",
      "iteration 24990 || Loss: 3.6644 || 10iter:7.8295 sec.\n",
      "iteration 25000 || Loss: 3.5169 || 10iter:7.8601 sec.\n",
      "iteration 25010 || Loss: 3.7363 || 10iter:7.8871 sec.\n",
      "iteration 25020 || Loss: 4.0214 || 10iter:7.7816 sec.\n",
      "-------------------\n",
      "epoch 35 || Epoch_TRAIN_Loss:2993.1037 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 36/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 25030 || Loss: 3.9550 || 10iter:3.7001 sec.\n",
      "iteration 25040 || Loss: 4.3221 || 10iter:7.8315 sec.\n",
      "iteration 25050 || Loss: 4.6340 || 10iter:8.0624 sec.\n",
      "iteration 25060 || Loss: 4.8530 || 10iter:7.7773 sec.\n",
      "iteration 25070 || Loss: 5.2028 || 10iter:7.8474 sec.\n",
      "iteration 25080 || Loss: 4.2365 || 10iter:7.7788 sec.\n",
      "iteration 25090 || Loss: 5.6688 || 10iter:7.8864 sec.\n",
      "iteration 25100 || Loss: 4.7396 || 10iter:7.8781 sec.\n",
      "iteration 25110 || Loss: 4.7139 || 10iter:7.7525 sec.\n",
      "iteration 25120 || Loss: 3.2537 || 10iter:7.7557 sec.\n",
      "iteration 25130 || Loss: 4.0241 || 10iter:7.8576 sec.\n",
      "iteration 25140 || Loss: 4.2125 || 10iter:7.8977 sec.\n",
      "iteration 25150 || Loss: 4.9014 || 10iter:7.9189 sec.\n",
      "iteration 25160 || Loss: 4.6139 || 10iter:7.8481 sec.\n",
      "iteration 25170 || Loss: 3.9100 || 10iter:7.8454 sec.\n",
      "iteration 25180 || Loss: 4.0402 || 10iter:7.7490 sec.\n",
      "iteration 25190 || Loss: 4.4249 || 10iter:7.8650 sec.\n",
      "iteration 25200 || Loss: 4.1364 || 10iter:7.9560 sec.\n",
      "iteration 25210 || Loss: 3.9566 || 10iter:7.9185 sec.\n",
      "iteration 25220 || Loss: 4.6963 || 10iter:7.9153 sec.\n",
      "iteration 25230 || Loss: 4.2521 || 10iter:7.8758 sec.\n",
      "iteration 25240 || Loss: 5.2297 || 10iter:7.9031 sec.\n",
      "iteration 25250 || Loss: 4.1714 || 10iter:7.8069 sec.\n",
      "iteration 25260 || Loss: 4.1709 || 10iter:7.9094 sec.\n",
      "iteration 25270 || Loss: 4.8095 || 10iter:7.9240 sec.\n",
      "iteration 25280 || Loss: 5.4304 || 10iter:7.8885 sec.\n",
      "iteration 25290 || Loss: 3.0387 || 10iter:7.8117 sec.\n",
      "iteration 25300 || Loss: 3.2091 || 10iter:7.8909 sec.\n",
      "iteration 25310 || Loss: 3.0749 || 10iter:7.7964 sec.\n",
      "iteration 25320 || Loss: 3.1155 || 10iter:7.8123 sec.\n",
      "iteration 25330 || Loss: 4.0565 || 10iter:7.8977 sec.\n",
      "iteration 25340 || Loss: 4.8571 || 10iter:7.8868 sec.\n",
      "iteration 25350 || Loss: 3.9565 || 10iter:7.8316 sec.\n",
      "iteration 25360 || Loss: 3.8790 || 10iter:8.0282 sec.\n",
      "iteration 25370 || Loss: 4.1844 || 10iter:7.8174 sec.\n",
      "iteration 25380 || Loss: 4.5390 || 10iter:7.9240 sec.\n",
      "iteration 25390 || Loss: 4.0446 || 10iter:7.8519 sec.\n",
      "iteration 25400 || Loss: 4.1647 || 10iter:7.9747 sec.\n",
      "iteration 25410 || Loss: 3.9422 || 10iter:7.8414 sec.\n",
      "iteration 25420 || Loss: 3.7271 || 10iter:7.9148 sec.\n",
      "iteration 25430 || Loss: 6.3641 || 10iter:7.8527 sec.\n",
      "iteration 25440 || Loss: 3.3662 || 10iter:7.9351 sec.\n",
      "iteration 25450 || Loss: 3.8936 || 10iter:7.8909 sec.\n",
      "iteration 25460 || Loss: 4.3401 || 10iter:7.7806 sec.\n",
      "iteration 25470 || Loss: 4.3279 || 10iter:7.8443 sec.\n",
      "iteration 25480 || Loss: 3.8621 || 10iter:7.9087 sec.\n",
      "iteration 25490 || Loss: 4.7308 || 10iter:7.7698 sec.\n",
      "iteration 25500 || Loss: 4.6081 || 10iter:7.9028 sec.\n",
      "iteration 25510 || Loss: 3.4482 || 10iter:7.9043 sec.\n",
      "iteration 25520 || Loss: 5.2494 || 10iter:7.8211 sec.\n",
      "iteration 25530 || Loss: 4.1051 || 10iter:7.8058 sec.\n",
      "iteration 25540 || Loss: 4.2935 || 10iter:7.9495 sec.\n",
      "iteration 25550 || Loss: 5.0780 || 10iter:7.7292 sec.\n",
      "iteration 25560 || Loss: 3.8001 || 10iter:8.0346 sec.\n",
      "iteration 25570 || Loss: 3.9572 || 10iter:7.8834 sec.\n",
      "iteration 25580 || Loss: 4.1127 || 10iter:7.9158 sec.\n",
      "iteration 25590 || Loss: 4.4146 || 10iter:7.9006 sec.\n",
      "iteration 25600 || Loss: 4.4817 || 10iter:7.9438 sec.\n",
      "iteration 25610 || Loss: 5.3744 || 10iter:7.9077 sec.\n",
      "iteration 25620 || Loss: 4.4660 || 10iter:7.8554 sec.\n",
      "iteration 25630 || Loss: 4.3441 || 10iter:7.8741 sec.\n",
      "iteration 25640 || Loss: 4.1091 || 10iter:7.8347 sec.\n",
      "iteration 25650 || Loss: 4.9465 || 10iter:7.7966 sec.\n",
      "iteration 25660 || Loss: 4.0814 || 10iter:7.9853 sec.\n",
      "iteration 25670 || Loss: 3.6294 || 10iter:7.9392 sec.\n",
      "iteration 25680 || Loss: 4.2282 || 10iter:7.8326 sec.\n",
      "iteration 25690 || Loss: 2.8035 || 10iter:8.0001 sec.\n",
      "iteration 25700 || Loss: 4.9817 || 10iter:7.9252 sec.\n",
      "iteration 25710 || Loss: 3.5139 || 10iter:7.9214 sec.\n",
      "iteration 25720 || Loss: 3.9066 || 10iter:7.7714 sec.\n",
      "iteration 25730 || Loss: 4.8931 || 10iter:7.8944 sec.\n",
      "iteration 25740 || Loss: 3.8916 || 10iter:7.6125 sec.\n",
      "-------------------\n",
      "epoch 36 || Epoch_TRAIN_Loss:2996.1958 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 37/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 25750 || Loss: 4.0815 || 10iter:7.9359 sec.\n",
      "iteration 25760 || Loss: 3.2720 || 10iter:7.8352 sec.\n",
      "iteration 25770 || Loss: 4.3600 || 10iter:7.8251 sec.\n",
      "iteration 25780 || Loss: 3.8792 || 10iter:7.8957 sec.\n",
      "iteration 25790 || Loss: 4.6336 || 10iter:7.8496 sec.\n",
      "iteration 25800 || Loss: 4.7876 || 10iter:7.8733 sec.\n",
      "iteration 25810 || Loss: 4.7078 || 10iter:7.8104 sec.\n",
      "iteration 25820 || Loss: 5.3853 || 10iter:7.8576 sec.\n",
      "iteration 25830 || Loss: 3.3726 || 10iter:7.8894 sec.\n",
      "iteration 25840 || Loss: 4.1495 || 10iter:7.9970 sec.\n",
      "iteration 25850 || Loss: 3.9828 || 10iter:7.8155 sec.\n",
      "iteration 25860 || Loss: 3.0113 || 10iter:7.8561 sec.\n",
      "iteration 25870 || Loss: 3.5373 || 10iter:7.9168 sec.\n",
      "iteration 25880 || Loss: 5.1680 || 10iter:7.8579 sec.\n",
      "iteration 25890 || Loss: 3.9127 || 10iter:7.9922 sec.\n",
      "iteration 25900 || Loss: 4.1094 || 10iter:7.7955 sec.\n",
      "iteration 25910 || Loss: 3.8642 || 10iter:7.8516 sec.\n",
      "iteration 25920 || Loss: 4.3412 || 10iter:7.8619 sec.\n",
      "iteration 25930 || Loss: 3.9108 || 10iter:7.7944 sec.\n",
      "iteration 25940 || Loss: 4.0255 || 10iter:7.8466 sec.\n",
      "iteration 25950 || Loss: 4.3227 || 10iter:7.7558 sec.\n",
      "iteration 25960 || Loss: 4.0195 || 10iter:7.8936 sec.\n",
      "iteration 25970 || Loss: 3.2294 || 10iter:7.9237 sec.\n",
      "iteration 25980 || Loss: 3.6819 || 10iter:7.9336 sec.\n",
      "iteration 25990 || Loss: 3.8846 || 10iter:7.8384 sec.\n",
      "iteration 26000 || Loss: 5.0122 || 10iter:7.9427 sec.\n",
      "iteration 26010 || Loss: 4.2991 || 10iter:7.8824 sec.\n",
      "iteration 26020 || Loss: 3.0387 || 10iter:7.8390 sec.\n",
      "iteration 26030 || Loss: 4.5619 || 10iter:7.9984 sec.\n",
      "iteration 26040 || Loss: 4.7476 || 10iter:7.9811 sec.\n",
      "iteration 26050 || Loss: 3.2724 || 10iter:7.8104 sec.\n",
      "iteration 26060 || Loss: 4.1849 || 10iter:7.7879 sec.\n",
      "iteration 26070 || Loss: 4.4465 || 10iter:7.8002 sec.\n",
      "iteration 26080 || Loss: 3.5328 || 10iter:7.8652 sec.\n",
      "iteration 26090 || Loss: 4.9634 || 10iter:7.8818 sec.\n",
      "iteration 26100 || Loss: 4.0879 || 10iter:7.9118 sec.\n",
      "iteration 26110 || Loss: 4.8374 || 10iter:7.9505 sec.\n",
      "iteration 26120 || Loss: 4.5833 || 10iter:7.7560 sec.\n",
      "iteration 26130 || Loss: 5.6123 || 10iter:7.9554 sec.\n",
      "iteration 26140 || Loss: 3.3846 || 10iter:7.8874 sec.\n",
      "iteration 26150 || Loss: 5.0884 || 10iter:7.8412 sec.\n",
      "iteration 26160 || Loss: 5.2762 || 10iter:7.8677 sec.\n",
      "iteration 26170 || Loss: 3.9424 || 10iter:7.7524 sec.\n",
      "iteration 26180 || Loss: 3.5206 || 10iter:7.8013 sec.\n",
      "iteration 26190 || Loss: 4.4601 || 10iter:7.8739 sec.\n",
      "iteration 26200 || Loss: 4.3191 || 10iter:7.7763 sec.\n",
      "iteration 26210 || Loss: 4.0108 || 10iter:7.7819 sec.\n",
      "iteration 26220 || Loss: 3.4898 || 10iter:7.8966 sec.\n",
      "iteration 26230 || Loss: 4.1213 || 10iter:7.8738 sec.\n",
      "iteration 26240 || Loss: 5.8843 || 10iter:7.8796 sec.\n",
      "iteration 26250 || Loss: 4.8188 || 10iter:7.7958 sec.\n",
      "iteration 26260 || Loss: 5.0340 || 10iter:7.8664 sec.\n",
      "iteration 26270 || Loss: 5.0237 || 10iter:7.9053 sec.\n",
      "iteration 26280 || Loss: 4.0575 || 10iter:7.8775 sec.\n",
      "iteration 26290 || Loss: 3.8852 || 10iter:7.7828 sec.\n",
      "iteration 26300 || Loss: 4.5337 || 10iter:7.8433 sec.\n",
      "iteration 26310 || Loss: 4.1139 || 10iter:7.9470 sec.\n",
      "iteration 26320 || Loss: 3.6032 || 10iter:7.8393 sec.\n",
      "iteration 26330 || Loss: 4.5345 || 10iter:7.8666 sec.\n",
      "iteration 26340 || Loss: 3.8259 || 10iter:7.8530 sec.\n",
      "iteration 26350 || Loss: 4.2068 || 10iter:7.8696 sec.\n",
      "iteration 26360 || Loss: 4.0911 || 10iter:7.7920 sec.\n",
      "iteration 26370 || Loss: 5.3880 || 10iter:7.8724 sec.\n",
      "iteration 26380 || Loss: 3.7589 || 10iter:7.8071 sec.\n",
      "iteration 26390 || Loss: 3.7765 || 10iter:7.9505 sec.\n",
      "iteration 26400 || Loss: 6.1555 || 10iter:7.8526 sec.\n",
      "iteration 26410 || Loss: 5.0143 || 10iter:7.8255 sec.\n",
      "iteration 26420 || Loss: 5.2907 || 10iter:7.8649 sec.\n",
      "iteration 26430 || Loss: 4.6347 || 10iter:7.8518 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 26440 || Loss: 4.8443 || 10iter:7.8353 sec.\n",
      "iteration 26450 || Loss: 4.1605 || 10iter:7.8977 sec.\n",
      "-------------------\n",
      "epoch 37 || Epoch_TRAIN_Loss:2950.1833 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 38/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 26460 || Loss: 3.8310 || 10iter:3.6708 sec.\n",
      "iteration 26470 || Loss: 4.0404 || 10iter:7.8136 sec.\n",
      "iteration 26480 || Loss: 3.6865 || 10iter:7.9179 sec.\n",
      "iteration 26490 || Loss: 4.4274 || 10iter:7.8981 sec.\n",
      "iteration 26500 || Loss: 5.0757 || 10iter:7.8792 sec.\n",
      "iteration 26510 || Loss: 4.0977 || 10iter:7.9604 sec.\n",
      "iteration 26520 || Loss: 4.5905 || 10iter:7.9080 sec.\n",
      "iteration 26530 || Loss: 4.0698 || 10iter:7.8539 sec.\n",
      "iteration 26540 || Loss: 3.0167 || 10iter:7.8114 sec.\n",
      "iteration 26550 || Loss: 3.4063 || 10iter:7.8655 sec.\n",
      "iteration 26560 || Loss: 3.8829 || 10iter:7.9157 sec.\n",
      "iteration 26570 || Loss: 4.1207 || 10iter:7.7651 sec.\n",
      "iteration 26580 || Loss: 4.2802 || 10iter:8.0157 sec.\n",
      "iteration 26590 || Loss: 4.2349 || 10iter:7.9436 sec.\n",
      "iteration 26600 || Loss: 3.7165 || 10iter:7.9104 sec.\n",
      "iteration 26610 || Loss: 3.1679 || 10iter:7.9076 sec.\n",
      "iteration 26620 || Loss: 3.8206 || 10iter:7.9712 sec.\n",
      "iteration 26630 || Loss: 3.3094 || 10iter:7.8706 sec.\n",
      "iteration 26640 || Loss: 3.7012 || 10iter:7.8260 sec.\n",
      "iteration 26650 || Loss: 2.6854 || 10iter:7.7856 sec.\n",
      "iteration 26660 || Loss: 3.3200 || 10iter:7.8061 sec.\n",
      "iteration 26670 || Loss: 3.8278 || 10iter:7.9138 sec.\n",
      "iteration 26680 || Loss: 4.1377 || 10iter:7.8523 sec.\n",
      "iteration 26690 || Loss: 5.5334 || 10iter:7.8581 sec.\n",
      "iteration 26700 || Loss: 3.6558 || 10iter:7.8907 sec.\n",
      "iteration 26710 || Loss: 3.7721 || 10iter:7.9136 sec.\n",
      "iteration 26720 || Loss: 4.8544 || 10iter:7.8507 sec.\n",
      "iteration 26730 || Loss: 4.7293 || 10iter:7.9727 sec.\n",
      "iteration 26740 || Loss: 3.9912 || 10iter:7.8498 sec.\n",
      "iteration 26750 || Loss: 4.1272 || 10iter:7.7665 sec.\n",
      "iteration 26760 || Loss: 4.4136 || 10iter:7.9891 sec.\n",
      "iteration 26770 || Loss: 5.0201 || 10iter:7.9411 sec.\n",
      "iteration 26780 || Loss: 4.0200 || 10iter:7.8615 sec.\n",
      "iteration 26790 || Loss: 4.8417 || 10iter:7.7871 sec.\n",
      "iteration 26800 || Loss: 4.1958 || 10iter:7.9613 sec.\n",
      "iteration 26810 || Loss: 3.5338 || 10iter:7.9135 sec.\n",
      "iteration 26820 || Loss: 4.3980 || 10iter:7.8735 sec.\n",
      "iteration 26830 || Loss: 3.6035 || 10iter:7.7316 sec.\n",
      "iteration 26840 || Loss: 2.9682 || 10iter:7.8750 sec.\n",
      "iteration 26850 || Loss: 3.4476 || 10iter:7.8889 sec.\n",
      "iteration 26860 || Loss: 4.2892 || 10iter:7.8724 sec.\n",
      "iteration 26870 || Loss: 3.4214 || 10iter:7.9075 sec.\n",
      "iteration 26880 || Loss: 5.1089 || 10iter:7.8001 sec.\n",
      "iteration 26890 || Loss: 4.8080 || 10iter:7.7529 sec.\n",
      "iteration 26900 || Loss: 4.3124 || 10iter:7.7389 sec.\n",
      "iteration 26910 || Loss: 4.2141 || 10iter:7.7637 sec.\n",
      "iteration 26920 || Loss: 4.6899 || 10iter:7.9384 sec.\n",
      "iteration 26930 || Loss: 3.5009 || 10iter:7.8955 sec.\n",
      "iteration 26940 || Loss: 3.8330 || 10iter:7.7743 sec.\n",
      "iteration 26950 || Loss: 4.1909 || 10iter:7.8405 sec.\n",
      "iteration 26960 || Loss: 4.4051 || 10iter:7.8116 sec.\n",
      "iteration 26970 || Loss: 4.3719 || 10iter:7.9271 sec.\n",
      "iteration 26980 || Loss: 3.4141 || 10iter:7.8586 sec.\n",
      "iteration 26990 || Loss: 4.5880 || 10iter:7.7902 sec.\n",
      "iteration 27000 || Loss: 4.3842 || 10iter:7.8253 sec.\n",
      "iteration 27010 || Loss: 3.2488 || 10iter:7.8539 sec.\n",
      "iteration 27020 || Loss: 3.3797 || 10iter:7.8139 sec.\n",
      "iteration 27030 || Loss: 3.9982 || 10iter:8.0055 sec.\n",
      "iteration 27040 || Loss: 4.2309 || 10iter:7.8876 sec.\n",
      "iteration 27050 || Loss: 3.7176 || 10iter:7.9318 sec.\n",
      "iteration 27060 || Loss: 5.2897 || 10iter:7.7179 sec.\n",
      "iteration 27070 || Loss: 3.1853 || 10iter:7.8314 sec.\n",
      "iteration 27080 || Loss: 4.0853 || 10iter:7.8540 sec.\n",
      "iteration 27090 || Loss: 4.0769 || 10iter:7.8736 sec.\n",
      "iteration 27100 || Loss: 5.0594 || 10iter:7.9511 sec.\n",
      "iteration 27110 || Loss: 4.2419 || 10iter:7.8974 sec.\n",
      "iteration 27120 || Loss: 4.0592 || 10iter:7.8254 sec.\n",
      "iteration 27130 || Loss: 3.9832 || 10iter:7.9177 sec.\n",
      "iteration 27140 || Loss: 4.4433 || 10iter:7.8861 sec.\n",
      "iteration 27150 || Loss: 3.6124 || 10iter:7.8489 sec.\n",
      "iteration 27160 || Loss: 4.4454 || 10iter:7.7524 sec.\n",
      "iteration 27170 || Loss: 4.4333 || 10iter:7.7531 sec.\n",
      "-------------------\n",
      "epoch 38 || Epoch_TRAIN_Loss:2924.2340 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 39/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 27180 || Loss: 3.8606 || 10iter:7.8558 sec.\n",
      "iteration 27190 || Loss: 3.4448 || 10iter:7.8688 sec.\n",
      "iteration 27200 || Loss: 3.3785 || 10iter:7.9758 sec.\n",
      "iteration 27210 || Loss: 4.5774 || 10iter:7.8432 sec.\n",
      "iteration 27220 || Loss: 4.2433 || 10iter:7.8201 sec.\n",
      "iteration 27230 || Loss: 4.8861 || 10iter:7.8831 sec.\n",
      "iteration 27240 || Loss: 5.8378 || 10iter:7.9157 sec.\n",
      "iteration 27250 || Loss: 4.0175 || 10iter:7.9060 sec.\n",
      "iteration 27260 || Loss: 4.0388 || 10iter:7.8979 sec.\n",
      "iteration 27270 || Loss: 4.4643 || 10iter:7.8811 sec.\n",
      "iteration 27280 || Loss: 3.6899 || 10iter:8.0018 sec.\n",
      "iteration 27290 || Loss: 4.1060 || 10iter:7.9677 sec.\n",
      "iteration 27300 || Loss: 4.8131 || 10iter:7.9147 sec.\n",
      "iteration 27310 || Loss: 4.6962 || 10iter:8.0049 sec.\n",
      "iteration 27320 || Loss: 4.3278 || 10iter:7.8238 sec.\n",
      "iteration 27330 || Loss: 4.0149 || 10iter:7.9491 sec.\n",
      "iteration 27340 || Loss: 3.4983 || 10iter:7.8774 sec.\n",
      "iteration 27350 || Loss: 5.1489 || 10iter:7.8662 sec.\n",
      "iteration 27360 || Loss: 5.0805 || 10iter:7.9643 sec.\n",
      "iteration 27370 || Loss: 4.7063 || 10iter:7.8852 sec.\n",
      "iteration 27380 || Loss: 3.4692 || 10iter:7.9057 sec.\n",
      "iteration 27390 || Loss: 4.2473 || 10iter:7.8886 sec.\n",
      "iteration 27400 || Loss: 4.8963 || 10iter:7.8648 sec.\n",
      "iteration 27410 || Loss: 5.5652 || 10iter:7.9897 sec.\n",
      "iteration 27420 || Loss: 4.3795 || 10iter:7.9590 sec.\n",
      "iteration 27430 || Loss: 4.5870 || 10iter:7.8244 sec.\n",
      "iteration 27440 || Loss: 4.0022 || 10iter:7.8769 sec.\n",
      "iteration 27450 || Loss: 4.6763 || 10iter:8.0008 sec.\n",
      "iteration 27460 || Loss: 3.1599 || 10iter:7.9499 sec.\n",
      "iteration 27470 || Loss: 3.8237 || 10iter:8.0272 sec.\n",
      "iteration 27480 || Loss: 3.0265 || 10iter:7.8823 sec.\n",
      "iteration 27490 || Loss: 3.6568 || 10iter:7.8748 sec.\n",
      "iteration 27500 || Loss: 3.7318 || 10iter:7.8724 sec.\n",
      "iteration 27510 || Loss: 4.3713 || 10iter:7.7652 sec.\n",
      "iteration 27520 || Loss: 3.1377 || 10iter:7.9606 sec.\n",
      "iteration 27530 || Loss: 3.1239 || 10iter:7.8789 sec.\n",
      "iteration 27540 || Loss: 3.4377 || 10iter:7.8316 sec.\n",
      "iteration 27550 || Loss: 3.5720 || 10iter:7.8657 sec.\n",
      "iteration 27560 || Loss: 3.9398 || 10iter:7.9336 sec.\n",
      "iteration 27570 || Loss: 3.4302 || 10iter:7.8943 sec.\n",
      "iteration 27580 || Loss: 3.2361 || 10iter:7.8706 sec.\n",
      "iteration 27590 || Loss: 3.9722 || 10iter:7.9164 sec.\n",
      "iteration 27600 || Loss: 2.6919 || 10iter:7.8832 sec.\n",
      "iteration 27610 || Loss: 3.9684 || 10iter:7.8529 sec.\n",
      "iteration 27620 || Loss: 4.8595 || 10iter:7.9515 sec.\n",
      "iteration 27630 || Loss: 4.6646 || 10iter:7.8910 sec.\n",
      "iteration 27640 || Loss: 3.7992 || 10iter:7.8865 sec.\n",
      "iteration 27650 || Loss: 4.5358 || 10iter:7.8911 sec.\n",
      "iteration 27660 || Loss: 4.1380 || 10iter:7.7649 sec.\n",
      "iteration 27670 || Loss: 5.9648 || 10iter:7.9519 sec.\n",
      "iteration 27680 || Loss: 4.7575 || 10iter:8.0303 sec.\n",
      "iteration 27690 || Loss: 4.3961 || 10iter:7.9797 sec.\n",
      "iteration 27700 || Loss: 3.4359 || 10iter:7.8343 sec.\n",
      "iteration 27710 || Loss: 3.5663 || 10iter:7.8176 sec.\n",
      "iteration 27720 || Loss: 3.4820 || 10iter:7.7797 sec.\n",
      "iteration 27730 || Loss: 2.7675 || 10iter:7.9358 sec.\n",
      "iteration 27740 || Loss: 4.3624 || 10iter:7.8236 sec.\n",
      "iteration 27750 || Loss: 4.0510 || 10iter:7.8470 sec.\n",
      "iteration 27760 || Loss: 3.7946 || 10iter:7.8984 sec.\n",
      "iteration 27770 || Loss: 5.1721 || 10iter:7.7975 sec.\n",
      "iteration 27780 || Loss: 4.9107 || 10iter:7.8996 sec.\n",
      "iteration 27790 || Loss: 4.5275 || 10iter:7.8714 sec.\n",
      "iteration 27800 || Loss: 5.0008 || 10iter:7.8364 sec.\n",
      "iteration 27810 || Loss: 3.4747 || 10iter:7.8619 sec.\n",
      "iteration 27820 || Loss: 4.2352 || 10iter:7.9270 sec.\n",
      "iteration 27830 || Loss: 4.7682 || 10iter:7.9159 sec.\n",
      "iteration 27840 || Loss: 3.8430 || 10iter:7.8571 sec.\n",
      "iteration 27850 || Loss: 4.9189 || 10iter:7.7526 sec.\n",
      "iteration 27860 || Loss: 4.6948 || 10iter:7.8470 sec.\n",
      "iteration 27870 || Loss: 3.7730 || 10iter:7.9136 sec.\n",
      "iteration 27880 || Loss: 3.7297 || 10iter:7.9732 sec.\n",
      "-------------------\n",
      "epoch 39 || Epoch_TRAIN_Loss:2933.9362 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 40/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 27890 || Loss: 4.9227 || 10iter:3.7367 sec.\n",
      "iteration 27900 || Loss: 3.6415 || 10iter:7.8780 sec.\n",
      "iteration 27910 || Loss: 3.2413 || 10iter:7.8936 sec.\n",
      "iteration 27920 || Loss: 4.4786 || 10iter:7.9163 sec.\n",
      "iteration 27930 || Loss: 5.0458 || 10iter:7.8448 sec.\n",
      "iteration 27940 || Loss: 2.5348 || 10iter:7.8116 sec.\n",
      "iteration 27950 || Loss: 3.8162 || 10iter:8.0014 sec.\n",
      "iteration 27960 || Loss: 4.9365 || 10iter:7.9374 sec.\n",
      "iteration 27970 || Loss: 3.2389 || 10iter:7.9489 sec.\n",
      "iteration 27980 || Loss: 3.3724 || 10iter:7.9994 sec.\n",
      "iteration 27990 || Loss: 4.3577 || 10iter:7.7635 sec.\n",
      "iteration 28000 || Loss: 3.5424 || 10iter:7.7949 sec.\n",
      "iteration 28010 || Loss: 3.4481 || 10iter:7.9680 sec.\n",
      "iteration 28020 || Loss: 4.6072 || 10iter:7.8350 sec.\n",
      "iteration 28030 || Loss: 3.4481 || 10iter:7.8353 sec.\n",
      "iteration 28040 || Loss: 4.0902 || 10iter:7.9143 sec.\n",
      "iteration 28050 || Loss: 4.1211 || 10iter:7.8303 sec.\n",
      "iteration 28060 || Loss: 3.8789 || 10iter:7.8789 sec.\n",
      "iteration 28070 || Loss: 4.3553 || 10iter:7.9845 sec.\n",
      "iteration 28080 || Loss: 4.5202 || 10iter:7.8031 sec.\n",
      "iteration 28090 || Loss: 4.7182 || 10iter:7.7189 sec.\n",
      "iteration 28100 || Loss: 4.1715 || 10iter:7.8823 sec.\n",
      "iteration 28110 || Loss: 4.4585 || 10iter:7.9317 sec.\n",
      "iteration 28120 || Loss: 4.5291 || 10iter:7.8463 sec.\n",
      "iteration 28130 || Loss: 3.7471 || 10iter:7.8849 sec.\n",
      "iteration 28140 || Loss: 3.5709 || 10iter:7.8505 sec.\n",
      "iteration 28150 || Loss: 4.8661 || 10iter:7.9120 sec.\n",
      "iteration 28160 || Loss: 3.6634 || 10iter:8.0043 sec.\n",
      "iteration 28170 || Loss: 3.5741 || 10iter:7.9880 sec.\n",
      "iteration 28180 || Loss: 3.7436 || 10iter:7.9855 sec.\n",
      "iteration 28190 || Loss: 3.5989 || 10iter:7.8541 sec.\n",
      "iteration 28200 || Loss: 3.4403 || 10iter:7.8081 sec.\n",
      "iteration 28210 || Loss: 4.0186 || 10iter:7.8627 sec.\n",
      "iteration 28220 || Loss: 4.7207 || 10iter:7.8097 sec.\n",
      "iteration 28230 || Loss: 3.9104 || 10iter:7.9728 sec.\n",
      "iteration 28240 || Loss: 4.2365 || 10iter:7.8751 sec.\n",
      "iteration 28250 || Loss: 4.2900 || 10iter:7.9139 sec.\n",
      "iteration 28260 || Loss: 4.1533 || 10iter:7.9298 sec.\n",
      "iteration 28270 || Loss: 4.1606 || 10iter:7.8658 sec.\n",
      "iteration 28280 || Loss: 3.2460 || 10iter:7.8678 sec.\n",
      "iteration 28290 || Loss: 4.1382 || 10iter:7.8349 sec.\n",
      "iteration 28300 || Loss: 3.3931 || 10iter:7.9759 sec.\n",
      "iteration 28310 || Loss: 3.6071 || 10iter:7.8056 sec.\n",
      "iteration 28320 || Loss: 3.9272 || 10iter:7.9794 sec.\n",
      "iteration 28330 || Loss: 3.6602 || 10iter:7.8575 sec.\n",
      "iteration 28340 || Loss: 4.5028 || 10iter:7.8823 sec.\n",
      "iteration 28350 || Loss: 4.8815 || 10iter:7.9397 sec.\n",
      "iteration 28360 || Loss: 4.1394 || 10iter:7.9088 sec.\n",
      "iteration 28370 || Loss: 5.2270 || 10iter:7.7936 sec.\n",
      "iteration 28380 || Loss: 4.6656 || 10iter:7.9258 sec.\n",
      "iteration 28390 || Loss: 4.3930 || 10iter:7.8360 sec.\n",
      "iteration 28400 || Loss: 4.0695 || 10iter:7.7972 sec.\n",
      "iteration 28410 || Loss: 4.8413 || 10iter:7.8922 sec.\n",
      "iteration 28420 || Loss: 3.9401 || 10iter:7.8666 sec.\n",
      "iteration 28430 || Loss: 4.4531 || 10iter:7.7499 sec.\n",
      "iteration 28440 || Loss: 3.5674 || 10iter:7.8902 sec.\n",
      "iteration 28450 || Loss: 3.2412 || 10iter:7.9317 sec.\n",
      "iteration 28460 || Loss: 4.2826 || 10iter:7.9328 sec.\n",
      "iteration 28470 || Loss: 4.6227 || 10iter:7.9395 sec.\n",
      "iteration 28480 || Loss: 3.6130 || 10iter:7.8426 sec.\n",
      "iteration 28490 || Loss: 4.5853 || 10iter:7.9812 sec.\n",
      "iteration 28500 || Loss: 4.6463 || 10iter:7.7973 sec.\n",
      "iteration 28510 || Loss: 3.5253 || 10iter:7.9695 sec.\n",
      "iteration 28520 || Loss: 3.2344 || 10iter:7.8297 sec.\n",
      "iteration 28530 || Loss: 3.9836 || 10iter:7.9110 sec.\n",
      "iteration 28540 || Loss: 3.3783 || 10iter:7.9252 sec.\n",
      "iteration 28550 || Loss: 5.0572 || 10iter:7.9300 sec.\n",
      "iteration 28560 || Loss: 4.0435 || 10iter:7.7983 sec.\n",
      "iteration 28570 || Loss: 4.2735 || 10iter:7.9095 sec.\n",
      "iteration 28580 || Loss: 4.2154 || 10iter:7.9154 sec.\n",
      "iteration 28590 || Loss: 2.3787 || 10iter:7.8185 sec.\n",
      "iteration 28600 || Loss: 3.6782 || 10iter:7.7622 sec.\n",
      "-------------------\n",
      " (val) \n",
      "-------------------\n",
      "epoch 40 || Epoch_TRAIN_Loss:2874.0212 || Epoch_VAL_Loss:3132.2238\n",
      "-------------------\n",
      "Epoch 41/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 28610 || Loss: 4.3989 || 10iter:7.8660 sec.\n",
      "iteration 28620 || Loss: 3.3085 || 10iter:7.8403 sec.\n",
      "iteration 28630 || Loss: 4.1174 || 10iter:7.8889 sec.\n",
      "iteration 28640 || Loss: 4.9274 || 10iter:7.8708 sec.\n",
      "iteration 28650 || Loss: 3.6516 || 10iter:8.0294 sec.\n",
      "iteration 28660 || Loss: 3.7227 || 10iter:7.9245 sec.\n",
      "iteration 28670 || Loss: 4.4089 || 10iter:7.8258 sec.\n",
      "iteration 28680 || Loss: 4.0923 || 10iter:7.8771 sec.\n",
      "iteration 28690 || Loss: 4.0662 || 10iter:8.0016 sec.\n",
      "iteration 28700 || Loss: 4.9779 || 10iter:7.8299 sec.\n",
      "iteration 28710 || Loss: 5.0203 || 10iter:7.7935 sec.\n",
      "iteration 28720 || Loss: 4.6026 || 10iter:7.7666 sec.\n",
      "iteration 28730 || Loss: 3.2682 || 10iter:7.9299 sec.\n",
      "iteration 28740 || Loss: 4.3490 || 10iter:7.9000 sec.\n",
      "iteration 28750 || Loss: 3.7690 || 10iter:7.7881 sec.\n",
      "iteration 28760 || Loss: 3.1822 || 10iter:7.8631 sec.\n",
      "iteration 28770 || Loss: 4.1861 || 10iter:7.9261 sec.\n",
      "iteration 28780 || Loss: 3.4925 || 10iter:7.9127 sec.\n",
      "iteration 28790 || Loss: 3.7807 || 10iter:7.8740 sec.\n",
      "iteration 28800 || Loss: 3.5973 || 10iter:7.8758 sec.\n",
      "iteration 28810 || Loss: 4.7380 || 10iter:7.8734 sec.\n",
      "iteration 28820 || Loss: 2.8664 || 10iter:7.9785 sec.\n",
      "iteration 28830 || Loss: 4.0574 || 10iter:7.9401 sec.\n",
      "iteration 28840 || Loss: 4.0785 || 10iter:7.9304 sec.\n",
      "iteration 28850 || Loss: 5.2043 || 10iter:7.8440 sec.\n",
      "iteration 28860 || Loss: 3.6164 || 10iter:7.8323 sec.\n",
      "iteration 28870 || Loss: 3.8340 || 10iter:7.9789 sec.\n",
      "iteration 28880 || Loss: 4.3368 || 10iter:7.8628 sec.\n",
      "iteration 28890 || Loss: 3.7076 || 10iter:7.8144 sec.\n",
      "iteration 28900 || Loss: 3.2807 || 10iter:7.8719 sec.\n",
      "iteration 28910 || Loss: 4.5612 || 10iter:7.8458 sec.\n",
      "iteration 28920 || Loss: 3.5205 || 10iter:7.8745 sec.\n",
      "iteration 28930 || Loss: 3.7483 || 10iter:7.9186 sec.\n",
      "iteration 28940 || Loss: 4.8596 || 10iter:7.8660 sec.\n",
      "iteration 28950 || Loss: 3.9822 || 10iter:7.7753 sec.\n",
      "iteration 28960 || Loss: 4.3095 || 10iter:8.0182 sec.\n",
      "iteration 28970 || Loss: 3.9887 || 10iter:7.8495 sec.\n",
      "iteration 28980 || Loss: 4.2282 || 10iter:7.7914 sec.\n",
      "iteration 28990 || Loss: 4.5253 || 10iter:7.9801 sec.\n",
      "iteration 29000 || Loss: 3.5278 || 10iter:7.8453 sec.\n",
      "iteration 29010 || Loss: 3.9633 || 10iter:7.8236 sec.\n",
      "iteration 29020 || Loss: 4.5208 || 10iter:7.8923 sec.\n",
      "iteration 29030 || Loss: 3.1380 || 10iter:7.8535 sec.\n",
      "iteration 29040 || Loss: 3.9201 || 10iter:7.9020 sec.\n",
      "iteration 29050 || Loss: 4.2011 || 10iter:7.9508 sec.\n",
      "iteration 29060 || Loss: 4.6353 || 10iter:7.9275 sec.\n",
      "iteration 29070 || Loss: 3.9662 || 10iter:7.9251 sec.\n",
      "iteration 29080 || Loss: 3.9800 || 10iter:7.8854 sec.\n",
      "iteration 29090 || Loss: 3.1963 || 10iter:7.9073 sec.\n",
      "iteration 29100 || Loss: 3.8342 || 10iter:7.7663 sec.\n",
      "iteration 29110 || Loss: 4.3904 || 10iter:7.8987 sec.\n",
      "iteration 29120 || Loss: 3.8495 || 10iter:7.8788 sec.\n",
      "iteration 29130 || Loss: 3.8791 || 10iter:7.9802 sec.\n",
      "iteration 29140 || Loss: 4.4829 || 10iter:7.9017 sec.\n",
      "iteration 29150 || Loss: 4.0310 || 10iter:7.9143 sec.\n",
      "iteration 29160 || Loss: 3.4324 || 10iter:7.8767 sec.\n",
      "iteration 29170 || Loss: 3.2399 || 10iter:7.9151 sec.\n",
      "iteration 29180 || Loss: 4.3311 || 10iter:7.8063 sec.\n",
      "iteration 29190 || Loss: 4.4154 || 10iter:8.0018 sec.\n",
      "iteration 29200 || Loss: 3.3557 || 10iter:7.8474 sec.\n",
      "iteration 29210 || Loss: 3.9974 || 10iter:7.8939 sec.\n",
      "iteration 29220 || Loss: 3.1687 || 10iter:7.8063 sec.\n",
      "iteration 29230 || Loss: 3.7150 || 10iter:7.9561 sec.\n",
      "iteration 29240 || Loss: 3.8105 || 10iter:7.9565 sec.\n",
      "iteration 29250 || Loss: 3.8193 || 10iter:7.8800 sec.\n",
      "iteration 29260 || Loss: 3.3258 || 10iter:7.8480 sec.\n",
      "iteration 29270 || Loss: 3.8111 || 10iter:7.9035 sec.\n",
      "iteration 29280 || Loss: 5.0317 || 10iter:7.8755 sec.\n",
      "iteration 29290 || Loss: 4.3843 || 10iter:7.8329 sec.\n",
      "iteration 29300 || Loss: 3.7146 || 10iter:7.8049 sec.\n",
      "iteration 29310 || Loss: 2.9567 || 10iter:7.8591 sec.\n",
      "-------------------\n",
      "epoch 41 || Epoch_TRAIN_Loss:2905.4694 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 42/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 29320 || Loss: 4.3839 || 10iter:3.8152 sec.\n",
      "iteration 29330 || Loss: 4.3800 || 10iter:7.9785 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 29340 || Loss: 2.9244 || 10iter:7.9814 sec.\n",
      "iteration 29350 || Loss: 4.8834 || 10iter:7.9309 sec.\n",
      "iteration 29360 || Loss: 3.4122 || 10iter:7.8755 sec.\n",
      "iteration 29370 || Loss: 3.6185 || 10iter:7.8402 sec.\n",
      "iteration 29380 || Loss: 3.9857 || 10iter:7.9016 sec.\n",
      "iteration 29390 || Loss: 4.2109 || 10iter:7.9797 sec.\n",
      "iteration 29400 || Loss: 3.1566 || 10iter:7.9608 sec.\n",
      "iteration 29410 || Loss: 4.1209 || 10iter:8.0315 sec.\n",
      "iteration 29420 || Loss: 3.9366 || 10iter:7.9312 sec.\n",
      "iteration 29430 || Loss: 4.1159 || 10iter:7.9787 sec.\n",
      "iteration 29440 || Loss: 3.8136 || 10iter:7.8219 sec.\n",
      "iteration 29450 || Loss: 4.5473 || 10iter:7.9792 sec.\n",
      "iteration 29460 || Loss: 5.2258 || 10iter:7.9849 sec.\n",
      "iteration 29470 || Loss: 2.8274 || 10iter:7.8037 sec.\n",
      "iteration 29480 || Loss: 4.0416 || 10iter:7.9455 sec.\n",
      "iteration 29490 || Loss: 3.8087 || 10iter:7.9298 sec.\n",
      "iteration 29500 || Loss: 3.7669 || 10iter:7.8347 sec.\n",
      "iteration 29510 || Loss: 4.6277 || 10iter:7.8666 sec.\n",
      "iteration 29520 || Loss: 5.1814 || 10iter:7.8197 sec.\n",
      "iteration 29530 || Loss: 3.7629 || 10iter:7.8728 sec.\n",
      "iteration 29540 || Loss: 3.6916 || 10iter:7.8951 sec.\n",
      "iteration 29550 || Loss: 4.5238 || 10iter:7.8752 sec.\n",
      "iteration 29560 || Loss: 3.9489 || 10iter:7.8941 sec.\n",
      "iteration 29570 || Loss: 3.0844 || 10iter:7.8193 sec.\n",
      "iteration 29580 || Loss: 3.6422 || 10iter:7.8307 sec.\n",
      "iteration 29590 || Loss: 4.4301 || 10iter:7.9510 sec.\n",
      "iteration 29600 || Loss: 4.5731 || 10iter:7.9098 sec.\n",
      "iteration 29610 || Loss: 3.7993 || 10iter:7.8250 sec.\n",
      "iteration 29620 || Loss: 3.9933 || 10iter:8.0076 sec.\n",
      "iteration 29630 || Loss: 3.2161 || 10iter:7.8622 sec.\n",
      "iteration 29640 || Loss: 3.7454 || 10iter:7.9183 sec.\n",
      "iteration 29650 || Loss: 3.8060 || 10iter:7.7840 sec.\n",
      "iteration 29660 || Loss: 3.2202 || 10iter:7.7581 sec.\n",
      "iteration 29670 || Loss: 3.5975 || 10iter:7.8982 sec.\n",
      "iteration 29680 || Loss: 3.9521 || 10iter:7.8969 sec.\n",
      "iteration 29690 || Loss: 3.7388 || 10iter:7.9560 sec.\n",
      "iteration 29700 || Loss: 2.7252 || 10iter:7.9146 sec.\n",
      "iteration 29710 || Loss: 3.0279 || 10iter:7.9954 sec.\n",
      "iteration 29720 || Loss: 3.5448 || 10iter:7.9419 sec.\n",
      "iteration 29730 || Loss: 4.5576 || 10iter:7.8392 sec.\n",
      "iteration 29740 || Loss: 3.8121 || 10iter:7.9773 sec.\n",
      "iteration 29750 || Loss: 4.1421 || 10iter:7.8962 sec.\n",
      "iteration 29760 || Loss: 4.1050 || 10iter:7.9046 sec.\n",
      "iteration 29770 || Loss: 3.8544 || 10iter:7.8861 sec.\n",
      "iteration 29780 || Loss: 5.5011 || 10iter:7.9448 sec.\n",
      "iteration 29790 || Loss: 2.9470 || 10iter:7.9335 sec.\n",
      "iteration 29800 || Loss: 3.5266 || 10iter:7.8799 sec.\n",
      "iteration 29810 || Loss: 5.4550 || 10iter:8.0092 sec.\n",
      "iteration 29820 || Loss: 4.7468 || 10iter:7.7795 sec.\n",
      "iteration 29830 || Loss: 3.4952 || 10iter:7.7960 sec.\n",
      "iteration 29840 || Loss: 4.5169 || 10iter:7.8666 sec.\n",
      "iteration 29850 || Loss: 3.7923 || 10iter:8.0790 sec.\n",
      "iteration 29860 || Loss: 4.0404 || 10iter:7.9175 sec.\n",
      "iteration 29870 || Loss: 2.4733 || 10iter:7.7510 sec.\n",
      "iteration 29880 || Loss: 4.0507 || 10iter:7.8104 sec.\n",
      "iteration 29890 || Loss: 4.3657 || 10iter:7.9274 sec.\n",
      "iteration 29900 || Loss: 4.6753 || 10iter:7.8818 sec.\n",
      "iteration 29910 || Loss: 3.8395 || 10iter:7.8492 sec.\n",
      "iteration 29920 || Loss: 4.6450 || 10iter:7.8518 sec.\n",
      "iteration 29930 || Loss: 4.7148 || 10iter:7.9152 sec.\n",
      "iteration 29940 || Loss: 4.0817 || 10iter:7.8668 sec.\n",
      "iteration 29950 || Loss: 3.2163 || 10iter:7.8222 sec.\n",
      "iteration 29960 || Loss: 2.8086 || 10iter:7.9008 sec.\n",
      "iteration 29970 || Loss: 3.8388 || 10iter:7.9839 sec.\n",
      "iteration 29980 || Loss: 3.5443 || 10iter:7.8656 sec.\n",
      "iteration 29990 || Loss: 3.2970 || 10iter:7.8709 sec.\n",
      "iteration 30000 || Loss: 4.3644 || 10iter:7.9156 sec.\n",
      "iteration 30010 || Loss: 4.0729 || 10iter:7.9124 sec.\n",
      "iteration 30020 || Loss: 4.5119 || 10iter:7.8079 sec.\n",
      "iteration 30030 || Loss: 5.1204 || 10iter:7.8062 sec.\n",
      "-------------------\n",
      "epoch 42 || Epoch_TRAIN_Loss:2857.0802 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 43/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 30040 || Loss: 4.1082 || 10iter:7.9372 sec.\n",
      "iteration 30050 || Loss: 5.4903 || 10iter:7.9574 sec.\n",
      "iteration 30060 || Loss: 4.0090 || 10iter:7.9094 sec.\n",
      "iteration 30070 || Loss: 3.7584 || 10iter:7.9082 sec.\n",
      "iteration 30080 || Loss: 3.7046 || 10iter:7.8663 sec.\n",
      "iteration 30090 || Loss: 3.3879 || 10iter:7.9276 sec.\n",
      "iteration 30100 || Loss: 4.4193 || 10iter:7.8885 sec.\n",
      "iteration 30110 || Loss: 3.9401 || 10iter:7.8262 sec.\n",
      "iteration 30120 || Loss: 3.9555 || 10iter:7.9237 sec.\n",
      "iteration 30130 || Loss: 4.5574 || 10iter:7.8880 sec.\n",
      "iteration 30140 || Loss: 4.0260 || 10iter:7.8952 sec.\n",
      "iteration 30150 || Loss: 4.1816 || 10iter:7.8482 sec.\n",
      "iteration 30160 || Loss: 3.3095 || 10iter:7.7599 sec.\n",
      "iteration 30170 || Loss: 4.3095 || 10iter:7.8399 sec.\n",
      "iteration 30180 || Loss: 3.9697 || 10iter:7.8543 sec.\n",
      "iteration 30190 || Loss: 4.4874 || 10iter:7.8388 sec.\n",
      "iteration 30200 || Loss: 2.7055 || 10iter:7.9136 sec.\n",
      "iteration 30210 || Loss: 3.8579 || 10iter:7.8497 sec.\n",
      "iteration 30220 || Loss: 3.8347 || 10iter:7.8191 sec.\n",
      "iteration 30230 || Loss: 3.4586 || 10iter:7.8852 sec.\n",
      "iteration 30240 || Loss: 3.6111 || 10iter:7.9405 sec.\n",
      "iteration 30250 || Loss: 3.9033 || 10iter:7.7429 sec.\n",
      "iteration 30260 || Loss: 3.5307 || 10iter:7.9802 sec.\n",
      "iteration 30270 || Loss: 4.0047 || 10iter:7.8554 sec.\n",
      "iteration 30280 || Loss: 2.8653 || 10iter:7.9165 sec.\n",
      "iteration 30290 || Loss: 2.9180 || 10iter:7.9048 sec.\n",
      "iteration 30300 || Loss: 4.1671 || 10iter:7.9039 sec.\n",
      "iteration 30310 || Loss: 3.0264 || 10iter:7.9831 sec.\n",
      "iteration 30320 || Loss: 3.9185 || 10iter:7.9890 sec.\n",
      "iteration 30330 || Loss: 4.6020 || 10iter:7.9423 sec.\n",
      "iteration 30340 || Loss: 3.1243 || 10iter:8.0171 sec.\n",
      "iteration 30350 || Loss: 4.7509 || 10iter:7.8431 sec.\n",
      "iteration 30360 || Loss: 3.5917 || 10iter:7.8133 sec.\n",
      "iteration 30370 || Loss: 4.3164 || 10iter:7.8460 sec.\n",
      "iteration 30380 || Loss: 3.5399 || 10iter:7.9853 sec.\n",
      "iteration 30390 || Loss: 3.9511 || 10iter:7.9475 sec.\n",
      "iteration 30400 || Loss: 3.3700 || 10iter:7.9284 sec.\n",
      "iteration 30410 || Loss: 4.0942 || 10iter:7.8947 sec.\n",
      "iteration 30420 || Loss: 4.2382 || 10iter:8.0206 sec.\n",
      "iteration 30430 || Loss: 3.6644 || 10iter:7.9729 sec.\n",
      "iteration 30440 || Loss: 3.9901 || 10iter:8.0451 sec.\n",
      "iteration 30450 || Loss: 4.3458 || 10iter:7.8468 sec.\n",
      "iteration 30460 || Loss: 3.7908 || 10iter:7.9510 sec.\n",
      "iteration 30470 || Loss: 3.4438 || 10iter:7.9201 sec.\n",
      "iteration 30480 || Loss: 4.1761 || 10iter:7.9212 sec.\n",
      "iteration 30490 || Loss: 3.1375 || 10iter:7.9524 sec.\n",
      "iteration 30500 || Loss: 3.3503 || 10iter:7.9142 sec.\n",
      "iteration 30510 || Loss: 4.8241 || 10iter:7.9245 sec.\n",
      "iteration 30520 || Loss: 4.0219 || 10iter:7.9081 sec.\n",
      "iteration 30530 || Loss: 3.8356 || 10iter:7.9965 sec.\n",
      "iteration 30540 || Loss: 4.1542 || 10iter:7.8091 sec.\n",
      "iteration 30550 || Loss: 4.8694 || 10iter:7.8905 sec.\n",
      "iteration 30560 || Loss: 3.6950 || 10iter:7.8904 sec.\n",
      "iteration 30570 || Loss: 3.8774 || 10iter:8.0434 sec.\n",
      "iteration 30580 || Loss: 3.6169 || 10iter:7.8835 sec.\n",
      "iteration 30590 || Loss: 3.6281 || 10iter:8.0930 sec.\n",
      "iteration 30600 || Loss: 4.0051 || 10iter:7.9873 sec.\n",
      "iteration 30610 || Loss: 3.7427 || 10iter:7.9629 sec.\n",
      "iteration 30620 || Loss: 3.8531 || 10iter:7.9224 sec.\n",
      "iteration 30630 || Loss: 4.5659 || 10iter:7.8157 sec.\n",
      "iteration 30640 || Loss: 2.9111 || 10iter:7.7419 sec.\n",
      "iteration 30650 || Loss: 5.0899 || 10iter:7.8865 sec.\n",
      "iteration 30660 || Loss: 3.6907 || 10iter:7.8509 sec.\n",
      "iteration 30670 || Loss: 3.7895 || 10iter:7.8025 sec.\n",
      "iteration 30680 || Loss: 4.7735 || 10iter:7.9151 sec.\n",
      "iteration 30690 || Loss: 4.6523 || 10iter:7.9105 sec.\n",
      "iteration 30700 || Loss: 4.7449 || 10iter:7.8778 sec.\n",
      "iteration 30710 || Loss: 4.5877 || 10iter:7.9136 sec.\n",
      "iteration 30720 || Loss: 3.4815 || 10iter:8.0487 sec.\n",
      "iteration 30730 || Loss: 4.1499 || 10iter:8.0511 sec.\n",
      "iteration 30740 || Loss: 5.4012 || 10iter:7.9467 sec.\n",
      "-------------------\n",
      "epoch 43 || Epoch_TRAIN_Loss:2835.4601 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 44/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 30750 || Loss: 4.1437 || 10iter:3.8166 sec.\n",
      "iteration 30760 || Loss: 3.7747 || 10iter:7.8538 sec.\n",
      "iteration 30770 || Loss: 3.9777 || 10iter:7.7730 sec.\n",
      "iteration 30780 || Loss: 3.7618 || 10iter:8.0118 sec.\n",
      "iteration 30790 || Loss: 3.2259 || 10iter:7.8644 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 30800 || Loss: 4.0112 || 10iter:7.8110 sec.\n",
      "iteration 30810 || Loss: 3.8586 || 10iter:7.9054 sec.\n",
      "iteration 30820 || Loss: 3.2176 || 10iter:7.7174 sec.\n",
      "iteration 30830 || Loss: 3.9157 || 10iter:7.8640 sec.\n",
      "iteration 30840 || Loss: 3.9153 || 10iter:7.8245 sec.\n",
      "iteration 30850 || Loss: 4.1232 || 10iter:7.7755 sec.\n",
      "iteration 30860 || Loss: 4.1392 || 10iter:7.8403 sec.\n",
      "iteration 30870 || Loss: 4.6893 || 10iter:7.9559 sec.\n",
      "iteration 30880 || Loss: 3.9300 || 10iter:7.8897 sec.\n",
      "iteration 30890 || Loss: 2.7949 || 10iter:7.8382 sec.\n",
      "iteration 30900 || Loss: 3.6569 || 10iter:7.9167 sec.\n",
      "iteration 30910 || Loss: 4.1503 || 10iter:7.9000 sec.\n",
      "iteration 30920 || Loss: 4.9536 || 10iter:7.8635 sec.\n",
      "iteration 30930 || Loss: 4.5077 || 10iter:7.8586 sec.\n",
      "iteration 30940 || Loss: 3.6928 || 10iter:7.9844 sec.\n",
      "iteration 30950 || Loss: 3.4461 || 10iter:7.8855 sec.\n",
      "iteration 30960 || Loss: 3.9630 || 10iter:7.9297 sec.\n",
      "iteration 30970 || Loss: 3.7434 || 10iter:7.8932 sec.\n",
      "iteration 30980 || Loss: 2.9694 || 10iter:7.8998 sec.\n",
      "iteration 30990 || Loss: 4.8845 || 10iter:7.9638 sec.\n",
      "iteration 31000 || Loss: 4.4728 || 10iter:7.9343 sec.\n",
      "iteration 31010 || Loss: 4.0026 || 10iter:7.8833 sec.\n",
      "iteration 31020 || Loss: 4.9416 || 10iter:7.9103 sec.\n",
      "iteration 31030 || Loss: 4.4548 || 10iter:7.9985 sec.\n",
      "iteration 31040 || Loss: 4.2788 || 10iter:7.9323 sec.\n",
      "iteration 31050 || Loss: 3.0078 || 10iter:7.9480 sec.\n",
      "iteration 31060 || Loss: 4.5614 || 10iter:7.9493 sec.\n",
      "iteration 31070 || Loss: 3.9258 || 10iter:7.8329 sec.\n",
      "iteration 31080 || Loss: 3.9237 || 10iter:7.9814 sec.\n",
      "iteration 31090 || Loss: 4.1207 || 10iter:7.9922 sec.\n",
      "iteration 31100 || Loss: 4.2777 || 10iter:7.8985 sec.\n",
      "iteration 31110 || Loss: 4.2478 || 10iter:7.8476 sec.\n",
      "iteration 31120 || Loss: 2.8851 || 10iter:7.8207 sec.\n",
      "iteration 31130 || Loss: 3.5811 || 10iter:7.8844 sec.\n",
      "iteration 31140 || Loss: 3.5287 || 10iter:7.8372 sec.\n",
      "iteration 31150 || Loss: 4.6453 || 10iter:7.7957 sec.\n",
      "iteration 31160 || Loss: 4.1057 || 10iter:7.8330 sec.\n",
      "iteration 31170 || Loss: 2.7493 || 10iter:7.9424 sec.\n",
      "iteration 31180 || Loss: 2.8985 || 10iter:7.9030 sec.\n",
      "iteration 31190 || Loss: 3.8372 || 10iter:7.9234 sec.\n",
      "iteration 31200 || Loss: 4.7090 || 10iter:7.9151 sec.\n",
      "iteration 31210 || Loss: 4.7460 || 10iter:7.8749 sec.\n",
      "iteration 31220 || Loss: 3.4308 || 10iter:7.9471 sec.\n",
      "iteration 31230 || Loss: 4.0707 || 10iter:7.8185 sec.\n",
      "iteration 31240 || Loss: 4.3363 || 10iter:7.8305 sec.\n",
      "iteration 31250 || Loss: 3.5148 || 10iter:7.8365 sec.\n",
      "iteration 31260 || Loss: 3.7984 || 10iter:7.8531 sec.\n",
      "iteration 31270 || Loss: 3.0417 || 10iter:7.9401 sec.\n",
      "iteration 31280 || Loss: 4.0306 || 10iter:7.8499 sec.\n",
      "iteration 31290 || Loss: 3.3222 || 10iter:8.0058 sec.\n",
      "iteration 31300 || Loss: 3.4135 || 10iter:7.9207 sec.\n",
      "iteration 31310 || Loss: 5.1348 || 10iter:7.9676 sec.\n",
      "iteration 31320 || Loss: 5.0524 || 10iter:7.8853 sec.\n",
      "iteration 31330 || Loss: 2.9412 || 10iter:7.9728 sec.\n",
      "iteration 31340 || Loss: 4.1454 || 10iter:7.8694 sec.\n",
      "iteration 31350 || Loss: 3.8585 || 10iter:7.9839 sec.\n",
      "iteration 31360 || Loss: 4.3197 || 10iter:7.8591 sec.\n",
      "iteration 31370 || Loss: 4.9370 || 10iter:7.7155 sec.\n",
      "iteration 31380 || Loss: 4.5657 || 10iter:8.0223 sec.\n",
      "iteration 31390 || Loss: 4.7259 || 10iter:7.8775 sec.\n",
      "iteration 31400 || Loss: 4.7460 || 10iter:7.8774 sec.\n",
      "iteration 31410 || Loss: 4.7510 || 10iter:7.8591 sec.\n",
      "iteration 31420 || Loss: 2.8183 || 10iter:8.0198 sec.\n",
      "iteration 31430 || Loss: 3.0948 || 10iter:7.9500 sec.\n",
      "iteration 31440 || Loss: 3.6400 || 10iter:7.8686 sec.\n",
      "iteration 31450 || Loss: 3.3534 || 10iter:7.8291 sec.\n",
      "iteration 31460 || Loss: 3.5991 || 10iter:7.8282 sec.\n",
      "-------------------\n",
      "epoch 44 || Epoch_TRAIN_Loss:2791.9244 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 45/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 31470 || Loss: 3.9136 || 10iter:7.9664 sec.\n",
      "iteration 31480 || Loss: 4.6351 || 10iter:7.9105 sec.\n",
      "iteration 31490 || Loss: 2.5137 || 10iter:7.8033 sec.\n",
      "iteration 31500 || Loss: 4.5329 || 10iter:7.9736 sec.\n",
      "iteration 31510 || Loss: 4.5678 || 10iter:7.9263 sec.\n",
      "iteration 31520 || Loss: 3.4147 || 10iter:7.8583 sec.\n",
      "iteration 31530 || Loss: 3.2606 || 10iter:7.9011 sec.\n",
      "iteration 31540 || Loss: 3.6570 || 10iter:7.8389 sec.\n",
      "iteration 31550 || Loss: 3.8068 || 10iter:7.9049 sec.\n",
      "iteration 31560 || Loss: 4.5131 || 10iter:7.8332 sec.\n",
      "iteration 31570 || Loss: 3.4677 || 10iter:8.1098 sec.\n",
      "iteration 31580 || Loss: 3.7890 || 10iter:7.9452 sec.\n",
      "iteration 31590 || Loss: 2.8952 || 10iter:7.9007 sec.\n",
      "iteration 31600 || Loss: 3.6791 || 10iter:7.8265 sec.\n",
      "iteration 31610 || Loss: 3.5803 || 10iter:8.0490 sec.\n",
      "iteration 31620 || Loss: 4.0921 || 10iter:7.8631 sec.\n",
      "iteration 31630 || Loss: 3.6288 || 10iter:7.8669 sec.\n",
      "iteration 31640 || Loss: 4.1518 || 10iter:7.8690 sec.\n",
      "iteration 31650 || Loss: 4.0564 || 10iter:7.8635 sec.\n",
      "iteration 31660 || Loss: 3.7386 || 10iter:7.9437 sec.\n",
      "iteration 31670 || Loss: 2.5450 || 10iter:7.8663 sec.\n",
      "iteration 31680 || Loss: 3.9741 || 10iter:7.8989 sec.\n",
      "iteration 31690 || Loss: 4.7899 || 10iter:7.8487 sec.\n",
      "iteration 31700 || Loss: 2.8745 || 10iter:7.9998 sec.\n",
      "iteration 31710 || Loss: 4.8049 || 10iter:7.8458 sec.\n",
      "iteration 31720 || Loss: 4.9163 || 10iter:7.8915 sec.\n",
      "iteration 31730 || Loss: 3.8182 || 10iter:7.8496 sec.\n",
      "iteration 31740 || Loss: 3.2793 || 10iter:7.9314 sec.\n",
      "iteration 31750 || Loss: 4.5449 || 10iter:7.8930 sec.\n",
      "iteration 31760 || Loss: 4.2019 || 10iter:7.9845 sec.\n",
      "iteration 31770 || Loss: 3.6319 || 10iter:7.8417 sec.\n",
      "iteration 31780 || Loss: 4.1996 || 10iter:7.9790 sec.\n",
      "iteration 31790 || Loss: 3.7436 || 10iter:7.8536 sec.\n",
      "iteration 31800 || Loss: 4.7449 || 10iter:7.7489 sec.\n",
      "iteration 31810 || Loss: 3.7039 || 10iter:7.9136 sec.\n",
      "iteration 31820 || Loss: 4.6094 || 10iter:7.9100 sec.\n",
      "iteration 31830 || Loss: 4.8434 || 10iter:7.8063 sec.\n",
      "iteration 31840 || Loss: 3.3062 || 10iter:7.8149 sec.\n",
      "iteration 31850 || Loss: 3.8578 || 10iter:7.8828 sec.\n",
      "iteration 31860 || Loss: 4.0783 || 10iter:7.9926 sec.\n",
      "iteration 31870 || Loss: 3.6557 || 10iter:7.8991 sec.\n",
      "iteration 31880 || Loss: 3.8033 || 10iter:7.8627 sec.\n",
      "iteration 31890 || Loss: 4.9062 || 10iter:7.8197 sec.\n",
      "iteration 31900 || Loss: 3.6252 || 10iter:7.9870 sec.\n",
      "iteration 31910 || Loss: 3.4428 || 10iter:7.9517 sec.\n",
      "iteration 31920 || Loss: 4.0418 || 10iter:7.9677 sec.\n",
      "iteration 31930 || Loss: 4.3795 || 10iter:7.9650 sec.\n",
      "iteration 31940 || Loss: 4.3194 || 10iter:7.8991 sec.\n",
      "iteration 31950 || Loss: 4.6406 || 10iter:7.8035 sec.\n",
      "iteration 31960 || Loss: 3.9970 || 10iter:7.8300 sec.\n",
      "iteration 31970 || Loss: 3.8444 || 10iter:7.8958 sec.\n",
      "iteration 31980 || Loss: 4.9040 || 10iter:7.8990 sec.\n",
      "iteration 31990 || Loss: 3.3784 || 10iter:7.9128 sec.\n",
      "iteration 32000 || Loss: 2.8684 || 10iter:7.8831 sec.\n",
      "iteration 32010 || Loss: 3.6476 || 10iter:7.9349 sec.\n",
      "iteration 32020 || Loss: 4.8115 || 10iter:7.9245 sec.\n",
      "iteration 32030 || Loss: 4.5831 || 10iter:7.9505 sec.\n",
      "iteration 32040 || Loss: 4.6006 || 10iter:7.9401 sec.\n",
      "iteration 32050 || Loss: 3.8296 || 10iter:7.9271 sec.\n",
      "iteration 32060 || Loss: 3.0143 || 10iter:7.8666 sec.\n",
      "iteration 32070 || Loss: 4.7745 || 10iter:7.8457 sec.\n",
      "iteration 32080 || Loss: 4.4746 || 10iter:7.8407 sec.\n",
      "iteration 32090 || Loss: 4.1439 || 10iter:7.8622 sec.\n",
      "iteration 32100 || Loss: 2.6550 || 10iter:8.0628 sec.\n",
      "iteration 32110 || Loss: 3.7025 || 10iter:8.0127 sec.\n",
      "iteration 32120 || Loss: 4.2328 || 10iter:7.8403 sec.\n",
      "iteration 32130 || Loss: 3.5024 || 10iter:7.9426 sec.\n",
      "iteration 32140 || Loss: 4.7068 || 10iter:7.9372 sec.\n",
      "iteration 32150 || Loss: 3.0152 || 10iter:7.9369 sec.\n",
      "iteration 32160 || Loss: 4.3984 || 10iter:7.9806 sec.\n",
      "iteration 32170 || Loss: 4.3525 || 10iter:7.7748 sec.\n",
      "-------------------\n",
      "epoch 45 || Epoch_TRAIN_Loss:2789.2938 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 46/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 32180 || Loss: 4.9791 || 10iter:3.7479 sec.\n",
      "iteration 32190 || Loss: 4.3110 || 10iter:8.0052 sec.\n",
      "iteration 32200 || Loss: 3.8249 || 10iter:8.0515 sec.\n",
      "iteration 32210 || Loss: 2.7247 || 10iter:7.9994 sec.\n",
      "iteration 32220 || Loss: 3.6057 || 10iter:7.9227 sec.\n",
      "iteration 32230 || Loss: 2.8851 || 10iter:7.9156 sec.\n",
      "iteration 32240 || Loss: 3.9135 || 10iter:7.9135 sec.\n",
      "iteration 32250 || Loss: 4.5128 || 10iter:7.8913 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 32260 || Loss: 3.3920 || 10iter:7.9343 sec.\n",
      "iteration 32270 || Loss: 4.1701 || 10iter:7.9036 sec.\n",
      "iteration 32280 || Loss: 3.8287 || 10iter:7.8800 sec.\n",
      "iteration 32290 || Loss: 4.3346 || 10iter:7.9383 sec.\n",
      "iteration 32300 || Loss: 3.5365 || 10iter:7.9164 sec.\n",
      "iteration 32310 || Loss: 3.8773 || 10iter:7.9173 sec.\n",
      "iteration 32320 || Loss: 3.5265 || 10iter:7.9178 sec.\n",
      "iteration 32330 || Loss: 3.4438 || 10iter:7.8816 sec.\n",
      "iteration 32340 || Loss: 4.6182 || 10iter:7.9908 sec.\n",
      "iteration 32350 || Loss: 3.3058 || 10iter:7.8972 sec.\n",
      "iteration 32360 || Loss: 3.6754 || 10iter:7.8277 sec.\n",
      "iteration 32370 || Loss: 3.5025 || 10iter:7.9114 sec.\n",
      "iteration 32380 || Loss: 3.4778 || 10iter:8.0007 sec.\n",
      "iteration 32390 || Loss: 3.7568 || 10iter:7.9764 sec.\n",
      "iteration 32400 || Loss: 4.7126 || 10iter:7.9222 sec.\n",
      "iteration 32410 || Loss: 3.4332 || 10iter:7.9051 sec.\n",
      "iteration 32420 || Loss: 3.8258 || 10iter:7.9598 sec.\n",
      "iteration 32430 || Loss: 2.8636 || 10iter:7.9809 sec.\n",
      "iteration 32440 || Loss: 2.4909 || 10iter:7.8912 sec.\n",
      "iteration 32450 || Loss: 3.9905 || 10iter:7.9546 sec.\n",
      "iteration 32460 || Loss: 3.7511 || 10iter:7.9335 sec.\n",
      "iteration 32470 || Loss: 4.8234 || 10iter:7.8045 sec.\n",
      "iteration 32480 || Loss: 3.4315 || 10iter:8.1190 sec.\n",
      "iteration 32490 || Loss: 5.2041 || 10iter:7.8971 sec.\n",
      "iteration 32500 || Loss: 3.6305 || 10iter:7.8897 sec.\n",
      "iteration 32510 || Loss: 3.6610 || 10iter:7.8366 sec.\n",
      "iteration 32520 || Loss: 4.0942 || 10iter:7.8820 sec.\n",
      "iteration 32530 || Loss: 3.5509 || 10iter:7.9312 sec.\n",
      "iteration 32540 || Loss: 4.6533 || 10iter:7.9675 sec.\n",
      "iteration 32550 || Loss: 4.6662 || 10iter:7.9076 sec.\n",
      "iteration 32560 || Loss: 4.0334 || 10iter:7.8903 sec.\n",
      "iteration 32570 || Loss: 3.9878 || 10iter:7.9271 sec.\n",
      "iteration 32580 || Loss: 3.2693 || 10iter:8.0008 sec.\n",
      "iteration 32590 || Loss: 3.5570 || 10iter:7.9552 sec.\n",
      "iteration 32600 || Loss: 3.4336 || 10iter:7.9715 sec.\n",
      "iteration 32610 || Loss: 3.6196 || 10iter:7.9189 sec.\n",
      "iteration 32620 || Loss: 4.6889 || 10iter:8.0146 sec.\n",
      "iteration 32630 || Loss: 3.5917 || 10iter:7.8463 sec.\n",
      "iteration 32640 || Loss: 2.7856 || 10iter:7.9238 sec.\n",
      "iteration 32650 || Loss: 3.8507 || 10iter:7.9690 sec.\n",
      "iteration 32660 || Loss: 4.0164 || 10iter:8.0207 sec.\n",
      "iteration 32670 || Loss: 3.7694 || 10iter:7.7577 sec.\n",
      "iteration 32680 || Loss: 3.9030 || 10iter:8.0113 sec.\n",
      "iteration 32690 || Loss: 3.7248 || 10iter:7.8568 sec.\n",
      "iteration 32700 || Loss: 4.8613 || 10iter:7.9684 sec.\n",
      "iteration 32710 || Loss: 5.0355 || 10iter:7.9981 sec.\n",
      "iteration 32720 || Loss: 3.7714 || 10iter:7.9235 sec.\n",
      "iteration 32730 || Loss: 4.5554 || 10iter:7.9322 sec.\n",
      "iteration 32740 || Loss: 3.4446 || 10iter:7.9736 sec.\n",
      "iteration 32750 || Loss: 3.4859 || 10iter:7.8021 sec.\n",
      "iteration 32760 || Loss: 4.9774 || 10iter:7.9283 sec.\n",
      "iteration 32770 || Loss: 4.4795 || 10iter:7.8665 sec.\n",
      "iteration 32780 || Loss: 4.3201 || 10iter:7.8770 sec.\n",
      "iteration 32790 || Loss: 3.7162 || 10iter:7.8397 sec.\n",
      "iteration 32800 || Loss: 4.1364 || 10iter:7.8169 sec.\n",
      "iteration 32810 || Loss: 3.9068 || 10iter:8.0256 sec.\n",
      "iteration 32820 || Loss: 4.1470 || 10iter:7.8836 sec.\n",
      "iteration 32830 || Loss: 3.7722 || 10iter:7.9899 sec.\n",
      "iteration 32840 || Loss: 4.3246 || 10iter:7.9968 sec.\n",
      "iteration 32850 || Loss: 3.1893 || 10iter:7.8173 sec.\n",
      "iteration 32860 || Loss: 3.6230 || 10iter:7.8837 sec.\n",
      "iteration 32870 || Loss: 2.8875 || 10iter:7.8609 sec.\n",
      "iteration 32880 || Loss: 2.4563 || 10iter:7.8619 sec.\n",
      "iteration 32890 || Loss: 4.1870 || 10iter:7.7652 sec.\n",
      "-------------------\n",
      "epoch 46 || Epoch_TRAIN_Loss:2761.3149 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 47/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 32900 || Loss: 3.6680 || 10iter:7.8731 sec.\n",
      "iteration 32910 || Loss: 3.5264 || 10iter:7.9026 sec.\n",
      "iteration 32920 || Loss: 3.8124 || 10iter:7.9980 sec.\n",
      "iteration 32930 || Loss: 4.9682 || 10iter:7.8823 sec.\n",
      "iteration 32940 || Loss: 3.4560 || 10iter:7.8551 sec.\n",
      "iteration 32950 || Loss: 4.7790 || 10iter:8.0163 sec.\n",
      "iteration 32960 || Loss: 4.3800 || 10iter:7.9522 sec.\n",
      "iteration 32970 || Loss: 3.3633 || 10iter:7.9218 sec.\n",
      "iteration 32980 || Loss: 5.4255 || 10iter:7.9689 sec.\n",
      "iteration 32990 || Loss: 3.9711 || 10iter:7.8995 sec.\n",
      "iteration 33000 || Loss: 2.9060 || 10iter:7.9567 sec.\n",
      "iteration 33010 || Loss: 3.3676 || 10iter:8.0236 sec.\n",
      "iteration 33020 || Loss: 3.8264 || 10iter:7.9927 sec.\n",
      "iteration 33030 || Loss: 4.2545 || 10iter:7.8695 sec.\n",
      "iteration 33040 || Loss: 3.2652 || 10iter:8.0654 sec.\n",
      "iteration 33050 || Loss: 3.5067 || 10iter:7.9268 sec.\n",
      "iteration 33060 || Loss: 2.7009 || 10iter:8.0195 sec.\n",
      "iteration 33070 || Loss: 3.4839 || 10iter:8.0225 sec.\n",
      "iteration 33080 || Loss: 3.7721 || 10iter:7.8991 sec.\n",
      "iteration 33090 || Loss: 4.5545 || 10iter:8.0299 sec.\n",
      "iteration 33100 || Loss: 3.5844 || 10iter:8.0452 sec.\n",
      "iteration 33110 || Loss: 3.3563 || 10iter:7.9247 sec.\n",
      "iteration 33120 || Loss: 4.3618 || 10iter:7.8810 sec.\n",
      "iteration 33130 || Loss: 3.6938 || 10iter:7.9319 sec.\n",
      "iteration 33140 || Loss: 3.8467 || 10iter:7.9082 sec.\n",
      "iteration 33150 || Loss: 3.8495 || 10iter:7.9113 sec.\n",
      "iteration 33160 || Loss: 3.6558 || 10iter:8.1013 sec.\n",
      "iteration 33170 || Loss: 4.4265 || 10iter:7.9582 sec.\n",
      "iteration 33180 || Loss: 3.4623 || 10iter:7.8961 sec.\n",
      "iteration 33190 || Loss: 5.3074 || 10iter:7.9266 sec.\n",
      "iteration 33200 || Loss: 4.0495 || 10iter:8.0119 sec.\n",
      "iteration 33210 || Loss: 4.6532 || 10iter:8.0106 sec.\n",
      "iteration 33220 || Loss: 4.1719 || 10iter:8.0822 sec.\n",
      "iteration 33230 || Loss: 3.8643 || 10iter:8.0572 sec.\n",
      "iteration 33240 || Loss: 3.7839 || 10iter:7.9714 sec.\n",
      "iteration 33250 || Loss: 3.6855 || 10iter:7.9503 sec.\n",
      "iteration 33260 || Loss: 4.7284 || 10iter:7.8682 sec.\n",
      "iteration 33270 || Loss: 3.8346 || 10iter:7.9561 sec.\n",
      "iteration 33280 || Loss: 3.5947 || 10iter:7.9529 sec.\n",
      "iteration 33290 || Loss: 3.8658 || 10iter:7.9223 sec.\n",
      "iteration 33300 || Loss: 3.3118 || 10iter:7.7620 sec.\n",
      "iteration 33310 || Loss: 3.8840 || 10iter:8.0305 sec.\n",
      "iteration 33320 || Loss: 3.7376 || 10iter:8.0384 sec.\n",
      "iteration 33330 || Loss: 3.3083 || 10iter:7.9032 sec.\n",
      "iteration 33340 || Loss: 3.2979 || 10iter:7.9825 sec.\n",
      "iteration 33350 || Loss: 4.6409 || 10iter:8.0048 sec.\n",
      "iteration 33360 || Loss: 3.8734 || 10iter:7.9845 sec.\n",
      "iteration 33370 || Loss: 3.0201 || 10iter:7.8974 sec.\n",
      "iteration 33380 || Loss: 3.2996 || 10iter:7.8473 sec.\n",
      "iteration 33390 || Loss: 4.4102 || 10iter:7.8686 sec.\n",
      "iteration 33400 || Loss: 3.5173 || 10iter:7.8795 sec.\n",
      "iteration 33410 || Loss: 3.2736 || 10iter:7.9065 sec.\n",
      "iteration 33420 || Loss: 3.2807 || 10iter:7.9265 sec.\n",
      "iteration 33430 || Loss: 4.1679 || 10iter:7.8512 sec.\n",
      "iteration 33440 || Loss: 3.2602 || 10iter:7.8453 sec.\n",
      "iteration 33450 || Loss: 3.9845 || 10iter:7.8725 sec.\n",
      "iteration 33460 || Loss: 4.5437 || 10iter:7.8848 sec.\n",
      "iteration 33470 || Loss: 4.1925 || 10iter:7.8870 sec.\n",
      "iteration 33480 || Loss: 4.6240 || 10iter:7.8540 sec.\n",
      "iteration 33490 || Loss: 3.5307 || 10iter:7.8882 sec.\n",
      "iteration 33500 || Loss: 4.8712 || 10iter:7.9737 sec.\n",
      "iteration 33510 || Loss: 3.1321 || 10iter:8.0221 sec.\n",
      "iteration 33520 || Loss: 2.9247 || 10iter:7.8715 sec.\n",
      "iteration 33530 || Loss: 3.2562 || 10iter:7.8817 sec.\n",
      "iteration 33540 || Loss: 2.9209 || 10iter:7.8774 sec.\n",
      "iteration 33550 || Loss: 3.5499 || 10iter:7.9162 sec.\n",
      "iteration 33560 || Loss: 4.3140 || 10iter:7.9247 sec.\n",
      "iteration 33570 || Loss: 3.5901 || 10iter:7.8973 sec.\n",
      "iteration 33580 || Loss: 3.7440 || 10iter:7.9047 sec.\n",
      "iteration 33590 || Loss: 4.2944 || 10iter:7.8871 sec.\n",
      "iteration 33600 || Loss: 4.4114 || 10iter:7.8799 sec.\n",
      "-------------------\n",
      "epoch 47 || Epoch_TRAIN_Loss:2718.1823 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 48/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 33610 || Loss: 3.6721 || 10iter:3.8083 sec.\n",
      "iteration 33620 || Loss: 4.9355 || 10iter:7.8764 sec.\n",
      "iteration 33630 || Loss: 3.7361 || 10iter:7.8671 sec.\n",
      "iteration 33640 || Loss: 4.5937 || 10iter:7.9823 sec.\n",
      "iteration 33650 || Loss: 4.1912 || 10iter:7.8497 sec.\n",
      "iteration 33660 || Loss: 3.4959 || 10iter:7.8409 sec.\n",
      "iteration 33670 || Loss: 3.3526 || 10iter:7.8406 sec.\n",
      "iteration 33680 || Loss: 3.8252 || 10iter:8.0108 sec.\n",
      "iteration 33690 || Loss: 3.5051 || 10iter:7.8942 sec.\n",
      "iteration 33700 || Loss: 3.5293 || 10iter:7.8412 sec.\n",
      "iteration 33710 || Loss: 3.8877 || 10iter:7.8516 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 33720 || Loss: 3.3330 || 10iter:8.0327 sec.\n",
      "iteration 33730 || Loss: 3.4981 || 10iter:7.8459 sec.\n",
      "iteration 33740 || Loss: 2.8920 || 10iter:7.7973 sec.\n",
      "iteration 33750 || Loss: 4.4048 || 10iter:7.9839 sec.\n",
      "iteration 33760 || Loss: 4.3375 || 10iter:7.9566 sec.\n",
      "iteration 33770 || Loss: 3.7253 || 10iter:7.9883 sec.\n",
      "iteration 33780 || Loss: 3.8667 || 10iter:7.8452 sec.\n",
      "iteration 33790 || Loss: 3.3553 || 10iter:7.7922 sec.\n",
      "iteration 33800 || Loss: 4.0714 || 10iter:7.8893 sec.\n",
      "iteration 33810 || Loss: 4.8243 || 10iter:7.8637 sec.\n",
      "iteration 33820 || Loss: 3.8792 || 10iter:7.9015 sec.\n",
      "iteration 33830 || Loss: 4.4430 || 10iter:7.8341 sec.\n",
      "iteration 33840 || Loss: 4.7149 || 10iter:7.9698 sec.\n",
      "iteration 33850 || Loss: 4.6001 || 10iter:7.9268 sec.\n",
      "iteration 33860 || Loss: 3.0143 || 10iter:7.9726 sec.\n",
      "iteration 33870 || Loss: 3.7538 || 10iter:7.9294 sec.\n",
      "iteration 33880 || Loss: 4.0569 || 10iter:7.9495 sec.\n",
      "iteration 33890 || Loss: 3.4847 || 10iter:7.8391 sec.\n",
      "iteration 33900 || Loss: 3.7801 || 10iter:7.9757 sec.\n",
      "iteration 33910 || Loss: 4.8450 || 10iter:7.8943 sec.\n",
      "iteration 33920 || Loss: 4.5071 || 10iter:8.0192 sec.\n",
      "iteration 33930 || Loss: 3.7653 || 10iter:7.9504 sec.\n",
      "iteration 33940 || Loss: 3.1670 || 10iter:8.0004 sec.\n",
      "iteration 33950 || Loss: 3.4151 || 10iter:7.8343 sec.\n",
      "iteration 33960 || Loss: 3.6366 || 10iter:7.9415 sec.\n",
      "iteration 33970 || Loss: 3.7388 || 10iter:7.9458 sec.\n",
      "iteration 33980 || Loss: 2.9243 || 10iter:7.8953 sec.\n",
      "iteration 33990 || Loss: 3.2891 || 10iter:8.0224 sec.\n",
      "iteration 34000 || Loss: 4.1810 || 10iter:7.8031 sec.\n",
      "iteration 34010 || Loss: 4.2427 || 10iter:7.9410 sec.\n",
      "iteration 34020 || Loss: 3.6572 || 10iter:7.9178 sec.\n",
      "iteration 34030 || Loss: 2.5760 || 10iter:7.8530 sec.\n",
      "iteration 34040 || Loss: 3.5130 || 10iter:7.8586 sec.\n",
      "iteration 34050 || Loss: 3.2487 || 10iter:7.9570 sec.\n",
      "iteration 34060 || Loss: 3.7542 || 10iter:7.8292 sec.\n",
      "iteration 34070 || Loss: 3.5613 || 10iter:7.9141 sec.\n",
      "iteration 34080 || Loss: 3.1356 || 10iter:7.9213 sec.\n",
      "iteration 34090 || Loss: 3.1221 || 10iter:7.9571 sec.\n",
      "iteration 34100 || Loss: 3.3508 || 10iter:7.8773 sec.\n",
      "iteration 34110 || Loss: 3.4803 || 10iter:7.8681 sec.\n",
      "iteration 34120 || Loss: 3.4554 || 10iter:7.9176 sec.\n",
      "iteration 34130 || Loss: 4.8360 || 10iter:7.9478 sec.\n",
      "iteration 34140 || Loss: 5.3282 || 10iter:7.9169 sec.\n",
      "iteration 34150 || Loss: 4.8670 || 10iter:7.8917 sec.\n",
      "iteration 34160 || Loss: 2.9113 || 10iter:7.8633 sec.\n",
      "iteration 34170 || Loss: 3.6987 || 10iter:7.9464 sec.\n",
      "iteration 34180 || Loss: 4.6853 || 10iter:7.8174 sec.\n",
      "iteration 34190 || Loss: 3.1434 || 10iter:7.9978 sec.\n",
      "iteration 34200 || Loss: 3.2433 || 10iter:7.8864 sec.\n",
      "iteration 34210 || Loss: 3.7904 || 10iter:7.8469 sec.\n",
      "iteration 34220 || Loss: 2.6608 || 10iter:7.8369 sec.\n",
      "iteration 34230 || Loss: 3.4774 || 10iter:7.8957 sec.\n",
      "iteration 34240 || Loss: 3.6850 || 10iter:7.8753 sec.\n",
      "iteration 34250 || Loss: 4.1737 || 10iter:8.0155 sec.\n",
      "iteration 34260 || Loss: 3.8626 || 10iter:7.9884 sec.\n",
      "iteration 34270 || Loss: 4.4869 || 10iter:7.9190 sec.\n",
      "iteration 34280 || Loss: 3.4296 || 10iter:7.9087 sec.\n",
      "iteration 34290 || Loss: 4.3087 || 10iter:7.8886 sec.\n",
      "iteration 34300 || Loss: 4.3994 || 10iter:8.0666 sec.\n",
      "iteration 34310 || Loss: 4.5631 || 10iter:7.8930 sec.\n",
      "iteration 34320 || Loss: 2.5603 || 10iter:7.7109 sec.\n",
      "-------------------\n",
      "epoch 48 || Epoch_TRAIN_Loss:2714.6632 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 49/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 34330 || Loss: 3.9348 || 10iter:7.9349 sec.\n",
      "iteration 34340 || Loss: 3.4656 || 10iter:7.9104 sec.\n",
      "iteration 34350 || Loss: 4.6359 || 10iter:7.9219 sec.\n",
      "iteration 34360 || Loss: 4.7771 || 10iter:7.9378 sec.\n",
      "iteration 34370 || Loss: 3.4235 || 10iter:7.9662 sec.\n",
      "iteration 34380 || Loss: 3.1099 || 10iter:7.8738 sec.\n",
      "iteration 34390 || Loss: 4.8877 || 10iter:7.7537 sec.\n",
      "iteration 34400 || Loss: 4.0973 || 10iter:7.9226 sec.\n",
      "iteration 34410 || Loss: 2.6779 || 10iter:7.8948 sec.\n",
      "iteration 34420 || Loss: 3.8820 || 10iter:7.8480 sec.\n",
      "iteration 34430 || Loss: 4.4581 || 10iter:7.8904 sec.\n",
      "iteration 34440 || Loss: 3.5971 || 10iter:7.7844 sec.\n",
      "iteration 34450 || Loss: 3.6778 || 10iter:7.9612 sec.\n",
      "iteration 34460 || Loss: 2.8826 || 10iter:7.8128 sec.\n",
      "iteration 34470 || Loss: 2.6905 || 10iter:7.8755 sec.\n",
      "iteration 34480 || Loss: 4.4293 || 10iter:7.8619 sec.\n",
      "iteration 34490 || Loss: 3.1622 || 10iter:7.9410 sec.\n",
      "iteration 34500 || Loss: 4.0057 || 10iter:7.8887 sec.\n",
      "iteration 34510 || Loss: 3.9637 || 10iter:7.9761 sec.\n",
      "iteration 34520 || Loss: 3.8028 || 10iter:7.9145 sec.\n",
      "iteration 34530 || Loss: 3.8133 || 10iter:7.9400 sec.\n",
      "iteration 34540 || Loss: 3.1241 || 10iter:7.8142 sec.\n",
      "iteration 34550 || Loss: 4.6453 || 10iter:7.8712 sec.\n",
      "iteration 34560 || Loss: 3.1530 || 10iter:7.9489 sec.\n",
      "iteration 34570 || Loss: 3.1255 || 10iter:7.9096 sec.\n",
      "iteration 34580 || Loss: 4.0803 || 10iter:7.9487 sec.\n",
      "iteration 34590 || Loss: 2.9885 || 10iter:8.0226 sec.\n",
      "iteration 34600 || Loss: 3.5677 || 10iter:7.9086 sec.\n",
      "iteration 34610 || Loss: 4.3968 || 10iter:7.9150 sec.\n",
      "iteration 34620 || Loss: 3.7173 || 10iter:7.8660 sec.\n",
      "iteration 34630 || Loss: 4.8831 || 10iter:7.8790 sec.\n",
      "iteration 34640 || Loss: 3.1192 || 10iter:7.9959 sec.\n",
      "iteration 34650 || Loss: 4.7949 || 10iter:7.8004 sec.\n",
      "iteration 34660 || Loss: 3.4521 || 10iter:7.9228 sec.\n",
      "iteration 34670 || Loss: 3.8116 || 10iter:7.9803 sec.\n",
      "iteration 34680 || Loss: 3.5245 || 10iter:7.9312 sec.\n",
      "iteration 34690 || Loss: 4.1216 || 10iter:7.8854 sec.\n",
      "iteration 34700 || Loss: 3.9606 || 10iter:7.9650 sec.\n",
      "iteration 34710 || Loss: 3.6894 || 10iter:7.9964 sec.\n",
      "iteration 34720 || Loss: 3.4374 || 10iter:7.8673 sec.\n",
      "iteration 34730 || Loss: 3.9834 || 10iter:7.8370 sec.\n",
      "iteration 34740 || Loss: 3.5717 || 10iter:7.8282 sec.\n",
      "iteration 34750 || Loss: 4.7104 || 10iter:7.8683 sec.\n",
      "iteration 34760 || Loss: 4.1575 || 10iter:7.8854 sec.\n",
      "iteration 34770 || Loss: 4.0382 || 10iter:7.9256 sec.\n",
      "iteration 34780 || Loss: 2.9715 || 10iter:7.9081 sec.\n",
      "iteration 34790 || Loss: 3.9603 || 10iter:7.8983 sec.\n",
      "iteration 34800 || Loss: 4.0874 || 10iter:7.9156 sec.\n",
      "iteration 34810 || Loss: 3.8130 || 10iter:8.0167 sec.\n",
      "iteration 34820 || Loss: 3.6538 || 10iter:7.8783 sec.\n",
      "iteration 34830 || Loss: 4.2449 || 10iter:7.9465 sec.\n",
      "iteration 34840 || Loss: 4.5541 || 10iter:7.8642 sec.\n",
      "iteration 34850 || Loss: 3.9189 || 10iter:7.9024 sec.\n",
      "iteration 34860 || Loss: 4.1130 || 10iter:7.8665 sec.\n",
      "iteration 34870 || Loss: 3.4506 || 10iter:7.8419 sec.\n",
      "iteration 34880 || Loss: 3.6672 || 10iter:7.9823 sec.\n",
      "iteration 34890 || Loss: 3.2180 || 10iter:7.7344 sec.\n",
      "iteration 34900 || Loss: 3.7746 || 10iter:7.9980 sec.\n",
      "iteration 34910 || Loss: 3.6516 || 10iter:7.9624 sec.\n",
      "iteration 34920 || Loss: 3.9816 || 10iter:7.9144 sec.\n",
      "iteration 34930 || Loss: 3.6545 || 10iter:8.0087 sec.\n",
      "iteration 34940 || Loss: 4.1837 || 10iter:7.9646 sec.\n",
      "iteration 34950 || Loss: 2.9390 || 10iter:7.8765 sec.\n",
      "iteration 34960 || Loss: 4.1048 || 10iter:7.8890 sec.\n",
      "iteration 34970 || Loss: 4.0873 || 10iter:7.8435 sec.\n",
      "iteration 34980 || Loss: 3.3452 || 10iter:7.9733 sec.\n",
      "iteration 34990 || Loss: 3.1420 || 10iter:7.9908 sec.\n",
      "iteration 35000 || Loss: 3.8864 || 10iter:7.8865 sec.\n",
      "iteration 35010 || Loss: 3.5750 || 10iter:7.8517 sec.\n",
      "iteration 35020 || Loss: 3.3529 || 10iter:7.9287 sec.\n",
      "iteration 35030 || Loss: 3.9008 || 10iter:7.8545 sec.\n",
      "-------------------\n",
      "epoch 49 || Epoch_TRAIN_Loss:2738.8269 || Epoch_VAL_Loss:0.0000\n",
      "-------------------\n",
      "Epoch 50/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 35040 || Loss: 3.4953 || 10iter:3.8424 sec.\n",
      "iteration 35050 || Loss: 3.2330 || 10iter:7.8123 sec.\n",
      "iteration 35060 || Loss: 3.3991 || 10iter:7.8992 sec.\n",
      "iteration 35070 || Loss: 4.3031 || 10iter:7.9039 sec.\n",
      "iteration 35080 || Loss: 4.6088 || 10iter:8.0762 sec.\n",
      "iteration 35090 || Loss: 4.7741 || 10iter:7.9505 sec.\n",
      "iteration 35100 || Loss: 4.6648 || 10iter:7.8549 sec.\n",
      "iteration 35110 || Loss: 3.8938 || 10iter:7.9086 sec.\n",
      "iteration 35120 || Loss: 2.4938 || 10iter:7.8891 sec.\n",
      "iteration 35130 || Loss: 2.9167 || 10iter:7.9189 sec.\n",
      "iteration 35140 || Loss: 3.7215 || 10iter:7.8921 sec.\n",
      "iteration 35150 || Loss: 4.3736 || 10iter:7.8606 sec.\n",
      "iteration 35160 || Loss: 3.4568 || 10iter:8.0147 sec.\n",
      "iteration 35170 || Loss: 3.2030 || 10iter:7.9233 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 35180 || Loss: 3.4507 || 10iter:7.9795 sec.\n",
      "iteration 35190 || Loss: 4.6380 || 10iter:7.8513 sec.\n",
      "iteration 35200 || Loss: 3.2867 || 10iter:7.9942 sec.\n",
      "iteration 35210 || Loss: 3.3933 || 10iter:8.0478 sec.\n",
      "iteration 35220 || Loss: 3.4519 || 10iter:7.8348 sec.\n",
      "iteration 35230 || Loss: 2.3306 || 10iter:7.9054 sec.\n",
      "iteration 35240 || Loss: 3.7508 || 10iter:8.0127 sec.\n",
      "iteration 35250 || Loss: 3.2243 || 10iter:7.9015 sec.\n",
      "iteration 35260 || Loss: 3.7075 || 10iter:7.8579 sec.\n",
      "iteration 35270 || Loss: 2.9057 || 10iter:7.8094 sec.\n",
      "iteration 35280 || Loss: 3.8047 || 10iter:7.8066 sec.\n",
      "iteration 35290 || Loss: 3.8987 || 10iter:7.9055 sec.\n",
      "iteration 35300 || Loss: 3.5411 || 10iter:8.0266 sec.\n",
      "iteration 35310 || Loss: 3.5757 || 10iter:7.9389 sec.\n",
      "iteration 35320 || Loss: 3.4338 || 10iter:7.9155 sec.\n",
      "iteration 35330 || Loss: 4.3760 || 10iter:7.8957 sec.\n",
      "iteration 35340 || Loss: 4.5024 || 10iter:7.9921 sec.\n",
      "iteration 35350 || Loss: 3.5631 || 10iter:7.8468 sec.\n",
      "iteration 35360 || Loss: 4.3089 || 10iter:7.9671 sec.\n",
      "iteration 35370 || Loss: 4.0109 || 10iter:7.8833 sec.\n",
      "iteration 35380 || Loss: 4.0041 || 10iter:7.9474 sec.\n",
      "iteration 35390 || Loss: 3.1225 || 10iter:7.8565 sec.\n",
      "iteration 35400 || Loss: 3.3435 || 10iter:7.8060 sec.\n",
      "iteration 35410 || Loss: 3.2991 || 10iter:7.9154 sec.\n",
      "iteration 35420 || Loss: 2.8108 || 10iter:7.8989 sec.\n",
      "iteration 35430 || Loss: 3.4135 || 10iter:7.8915 sec.\n",
      "iteration 35440 || Loss: 3.2974 || 10iter:7.7527 sec.\n",
      "iteration 35450 || Loss: 3.9577 || 10iter:7.9200 sec.\n",
      "iteration 35460 || Loss: 2.7496 || 10iter:7.8541 sec.\n",
      "iteration 35470 || Loss: 3.9226 || 10iter:8.0430 sec.\n",
      "iteration 35480 || Loss: 3.3650 || 10iter:7.9235 sec.\n",
      "iteration 35490 || Loss: 2.9245 || 10iter:7.9621 sec.\n",
      "iteration 35500 || Loss: 4.6466 || 10iter:7.9037 sec.\n",
      "iteration 35510 || Loss: 3.9352 || 10iter:7.8270 sec.\n",
      "iteration 35520 || Loss: 3.3844 || 10iter:7.8849 sec.\n",
      "iteration 35530 || Loss: 3.8816 || 10iter:7.8186 sec.\n",
      "iteration 35540 || Loss: 3.9628 || 10iter:8.0244 sec.\n",
      "iteration 35550 || Loss: 3.9294 || 10iter:7.9529 sec.\n",
      "iteration 35560 || Loss: 3.5768 || 10iter:7.8510 sec.\n",
      "iteration 35570 || Loss: 3.6753 || 10iter:8.0183 sec.\n",
      "iteration 35580 || Loss: 3.9592 || 10iter:7.8182 sec.\n",
      "iteration 35590 || Loss: 5.0867 || 10iter:7.9896 sec.\n",
      "iteration 35600 || Loss: 4.0243 || 10iter:7.9562 sec.\n",
      "iteration 35610 || Loss: 3.5862 || 10iter:7.9147 sec.\n",
      "iteration 35620 || Loss: 3.8592 || 10iter:7.9149 sec.\n",
      "iteration 35630 || Loss: 3.9529 || 10iter:7.9100 sec.\n",
      "iteration 35640 || Loss: 3.8071 || 10iter:7.9631 sec.\n",
      "iteration 35650 || Loss: 4.5454 || 10iter:7.9444 sec.\n",
      "iteration 35660 || Loss: 3.3324 || 10iter:7.8338 sec.\n",
      "iteration 35670 || Loss: 3.6834 || 10iter:8.0044 sec.\n",
      "iteration 35680 || Loss: 3.3754 || 10iter:7.8303 sec.\n",
      "iteration 35690 || Loss: 4.5285 || 10iter:8.0156 sec.\n",
      "iteration 35700 || Loss: 4.4975 || 10iter:7.9517 sec.\n",
      "iteration 35710 || Loss: 3.7774 || 10iter:7.8814 sec.\n",
      "iteration 35720 || Loss: 3.7640 || 10iter:7.9231 sec.\n",
      "iteration 35730 || Loss: 3.1663 || 10iter:7.8370 sec.\n",
      "iteration 35740 || Loss: 3.8275 || 10iter:7.9151 sec.\n",
      "iteration 35750 || Loss: 3.5222 || 10iter:7.9598 sec.\n",
      "-------------------\n",
      " (val) \n",
      "-------------------\n",
      "epoch 50 || Epoch_TRAIN_Loss:2678.7963 || Epoch_VAL_Loss:2941.2334\n",
      "-------------------\n",
      "Epoch 51/50\n",
      "-------------------\n",
      "-------------------\n",
      " (train) \n",
      "iteration 35760 || Loss: 4.0339 || 10iter:7.8937 sec.\n",
      "iteration 35770 || Loss: 3.6455 || 10iter:7.8754 sec.\n",
      "iteration 35780 || Loss: 2.8931 || 10iter:7.8755 sec.\n",
      "iteration 35790 || Loss: 3.9111 || 10iter:7.7988 sec.\n",
      "iteration 35800 || Loss: 3.4039 || 10iter:7.8708 sec.\n",
      "iteration 35810 || Loss: 4.1751 || 10iter:7.7976 sec.\n",
      "iteration 35820 || Loss: 3.9499 || 10iter:7.9378 sec.\n",
      "iteration 35830 || Loss: 3.1877 || 10iter:7.8573 sec.\n",
      "iteration 35840 || Loss: 3.5586 || 10iter:7.8062 sec.\n",
      "iteration 35850 || Loss: 4.4195 || 10iter:7.7931 sec.\n",
      "iteration 35860 || Loss: 3.0289 || 10iter:7.8408 sec.\n",
      "iteration 35870 || Loss: 3.5220 || 10iter:7.9182 sec.\n",
      "iteration 35880 || Loss: 4.6666 || 10iter:7.9001 sec.\n",
      "iteration 35890 || Loss: 4.4527 || 10iter:7.9850 sec.\n",
      "iteration 35900 || Loss: 3.0310 || 10iter:7.8051 sec.\n",
      "iteration 35910 || Loss: 4.0263 || 10iter:7.9972 sec.\n",
      "iteration 35920 || Loss: 3.6909 || 10iter:7.8375 sec.\n",
      "iteration 35930 || Loss: 4.7266 || 10iter:7.9369 sec.\n",
      "iteration 35940 || Loss: 3.6805 || 10iter:7.8584 sec.\n",
      "iteration 35950 || Loss: 2.7655 || 10iter:7.8967 sec.\n",
      "iteration 35960 || Loss: 3.4353 || 10iter:7.8972 sec.\n",
      "iteration 35970 || Loss: 2.8861 || 10iter:7.8161 sec.\n",
      "iteration 35980 || Loss: 4.2122 || 10iter:7.8125 sec.\n",
      "iteration 35990 || Loss: 3.7029 || 10iter:7.9114 sec.\n",
      "iteration 36000 || Loss: 4.4842 || 10iter:7.9190 sec.\n",
      "iteration 36010 || Loss: 3.6597 || 10iter:8.0259 sec.\n",
      "iteration 36020 || Loss: 2.8104 || 10iter:7.8756 sec.\n",
      "iteration 36030 || Loss: 4.2711 || 10iter:7.8185 sec.\n",
      "iteration 36040 || Loss: 4.7619 || 10iter:7.9264 sec.\n",
      "iteration 36050 || Loss: 3.6677 || 10iter:8.0848 sec.\n",
      "iteration 36060 || Loss: 4.0491 || 10iter:7.8407 sec.\n",
      "iteration 36070 || Loss: 5.6194 || 10iter:7.9393 sec.\n",
      "iteration 36080 || Loss: 4.1763 || 10iter:7.9313 sec.\n",
      "iteration 36090 || Loss: 3.7994 || 10iter:7.8535 sec.\n",
      "iteration 36100 || Loss: 3.0361 || 10iter:7.9418 sec.\n",
      "iteration 36110 || Loss: 3.3788 || 10iter:7.8861 sec.\n",
      "iteration 36120 || Loss: 3.8692 || 10iter:7.8789 sec.\n",
      "iteration 36130 || Loss: 4.9031 || 10iter:7.8271 sec.\n",
      "iteration 36140 || Loss: 4.3841 || 10iter:7.8770 sec.\n",
      "iteration 36150 || Loss: 3.6971 || 10iter:7.9158 sec.\n",
      "iteration 36160 || Loss: 3.0233 || 10iter:7.8809 sec.\n",
      "iteration 36170 || Loss: 3.8959 || 10iter:7.8322 sec.\n",
      "iteration 36180 || Loss: 4.1324 || 10iter:7.8273 sec.\n",
      "iteration 36190 || Loss: 3.3017 || 10iter:7.9036 sec.\n",
      "iteration 36200 || Loss: 4.2830 || 10iter:8.0807 sec.\n",
      "iteration 36210 || Loss: 4.3748 || 10iter:7.9367 sec.\n",
      "iteration 36220 || Loss: 4.9523 || 10iter:7.8794 sec.\n",
      "iteration 36230 || Loss: 4.0881 || 10iter:7.8938 sec.\n",
      "iteration 36240 || Loss: 3.9102 || 10iter:7.8780 sec.\n",
      "iteration 36250 || Loss: 3.8432 || 10iter:7.8743 sec.\n",
      "iteration 36260 || Loss: 4.3547 || 10iter:7.8356 sec.\n",
      "iteration 36270 || Loss: 5.0935 || 10iter:7.9545 sec.\n",
      "iteration 36280 || Loss: 3.2582 || 10iter:7.8497 sec.\n",
      "iteration 36290 || Loss: 3.9419 || 10iter:7.8839 sec.\n",
      "iteration 36300 || Loss: 3.6829 || 10iter:7.8409 sec.\n",
      "iteration 36310 || Loss: 3.2582 || 10iter:7.9925 sec.\n",
      "iteration 36320 || Loss: 4.0206 || 10iter:8.0037 sec.\n",
      "iteration 36330 || Loss: 3.8238 || 10iter:7.8198 sec.\n",
      "iteration 36340 || Loss: 4.4137 || 10iter:7.9493 sec.\n",
      "iteration 36350 || Loss: 3.4015 || 10iter:7.9517 sec.\n",
      "iteration 36360 || Loss: 3.1527 || 10iter:7.8836 sec.\n",
      "iteration 36370 || Loss: 3.4261 || 10iter:7.9146 sec.\n",
      "iteration 36380 || Loss: 4.3519 || 10iter:7.9168 sec.\n",
      "iteration 36390 || Loss: 4.1115 || 10iter:7.8449 sec.\n",
      "iteration 36400 || Loss: 4.1113 || 10iter:7.7614 sec.\n",
      "iteration 36410 || Loss: 2.4467 || 10iter:8.0476 sec.\n",
      "iteration 36420 || Loss: 4.2333 || 10iter:7.8837 sec.\n",
      "iteration 36430 || Loss: 3.0730 || 10iter:7.8704 sec.\n",
      "iteration 36440 || Loss: 4.4607 || 10iter:7.8727 sec.\n",
      "iteration 36450 || Loss: 4.7842 || 10iter:7.9008 sec.\n",
      "iteration 36460 || Loss: 3.5914 || 10iter:7.8749 sec.\n",
      "-------------------\n",
      "epoch 51 || Epoch_TRAIN_Loss:2667.0617 || Epoch_VAL_Loss:0.0000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
